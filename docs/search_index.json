[["preface.html", "R Programming for Geospatial Sciences 1 Preface 1.1 Course Github 1.2 Syllabus 1.3 Phase 1: Spring GEOG473-673 2-Credit Agenda 1.4 Phase 2: Spring GEOG473-673 1-Credit Agenda", " R Programming for Geospatial Sciences James Simkins 2021-01-25 1 Preface Phase 1: Advanced R for Geospatial Sciences - 02/15/2021 - 04/16/2021 – 2cr Phase 2: Machine Learning with R – 04/19/2021 to 05/18/2021 – 1cr The objective of the spring installment of GEOG 473/673 is to expand on the topics covered in the fall version of GEOG 473/673 – open source environmental computing. The course will be split into 2 phases. The first phase is a 2-credit course focused on using advanced tools within the R programming language. The goal of this phase is for students to gain practical experience with challenging R topics that can be used for generating publication quality material. The second phase is a 1-credit course focused on introducing machine learning practices and implementing these via R. Machine Learning is a growing practice in data science and can be useful for geospatial sciences. Students will apply R programming knowledge and gain confidence in machine learning techniques and application with R. This challenging, fast-paced course is intended for students that already have programming experience with R or Python. 1.1 Course Github https://github.com/jsimkins2/geog473-673 1.2 Syllabus https://github.com/jsimkins2/geog473-673/blob/master/documents/spring_geog473_673_syllabus.pdf 1.3 Phase 1: Spring GEOG473-673 2-Credit Agenda This course is split into 2 phases but this textbook will be the only material for both courses. YouTube recordings will accompany all tutorials. Week Topic 1 Customizing Plots 2 Basic Statistics 3 Plotting with ggplot2 4 Spatial Plots with ggplot2 5 Shapefiles 6 Remote Data Extraction 7 Functions and Code Presentation 8 Project 9 Project 1.4 Phase 2: Spring GEOG473-673 1-Credit Agenda Week Topic 1 Machine Learning Intro 2 Time Series Forecasting 3 Time Series Forecasting 4 Random Forest Modeling 5 Random Forest Modeling "],["getting-started-with-r.html", "2 Getting Started with R 2.1 Why is there such a buzz around R? 2.2 Reasons to love R 2.3 Downloading Guide 2.4 Rstudio 2.5 Download Course Datasets 2.6 Video Examples 2.7 Getting familiar with RStudio 2.8 Open RStudio 2.9 R Studio Layout", " 2 Getting Started with R 2.1 Why is there such a buzz around R? R is an open-source programming language used for data science, statistics, and data visualization. R is currently ranked as the 7th most popular language in the world. Since R is open-source, anyone can contribute to or use R packages that contain pre-built functions/operations. This greatly accelerates our ability to share and collaborate. The first thing you need to do to get started with R is to install it on your computer. R works on pretty much every platform available, including the widely available Windows, Mac OS X, and Linux systems. R takes up very little space on your machine, despite it’s pleathora of data-science capabilities. All of these downloads are 100% free and trusted sources. 2.2 Reasons to love R Attribute Reason Speed R is FAST. It can number crunch magnitudes faster than Microsoft Excel, for example. Capacity R can handle millions of data records. Large datasets that crash in Microsoft Excel or ArcGIS won’t crash in R. Risk Reduction After writing a single R Script, the process of your data science routine is auditable and reproducible within milliseconds. We can write 1 script and make alterations as we see fit. We don’t have to replicate a process of pointing and clicking in Microsoft Excel or ArcGIS. For example, if you are working with a dataset that multiple people are using, you can load this dataset in R and perform the tasks that you need without making any changes to the original dataset. Visualizations R is capable to create high quality visualizations and also has the capacity to create interactive visualizations that can easiliy be shared. Plots or images can easily be exported to PNG, JPEG, or even web-based interactive dashboards that can be hosted on a webpage Collaboration R script sharing is safe and easy. As mentioned above, a team using the same data input file but performing different tasks on it can do so without editing the data input file for everyone else. Data output can also be shared without the concern of a colleague editing the file output. RStudio connect or Github are also popular free track all changes that take place between R script files. Price R is completely free! Yes…every bit of it! 2.3 Downloading Guide Navigate to the R website: https://cloud.r-project.org/ Click Download for your Operating System (Windows, Mac OSx, or Linux - if you don’t have a MacBook, then you’re using Windows most likely) Click the most recent R version to download. Install the downloaded application as you would with any other application on your computer. 2.4 Rstudio While R is the language, RStudio is the application we use to run R. Technically speaking, RStudio is an integrated development environment for R. RStudio makes coding in R easier by highlighting syntax, autocompleting symbols and names, and visualizing our R environment. These aspects are explained in further detail in the R Coding Fundamentals section. For now, let’s download RStudio. Navigate to the RStudio Website: http://www.rstudio.com/download Click Download under RStudio Desktop Free This website detects your operating system, allowing you to just click download again. Note that if it doesn’t automatically detect just select the download next to your operating system below this Note that you may be asked to install command line developer tools if you’re using a Mac - select Yes. Install the downloaded application as you normally would on your computer. 2.5 Download Course Datasets Navigate to the database - https://github.com/jsimkins2/geog473-673/tree/master/datasets Click on Code - then click download as zip 2.6 Video Examples If you want to watch a step-by-step tutorial on how to install R for Mac or Windows, you can watch these videos courtest of Dr. Roger Peng Installing R on Windows Installing R on the Mac Installing RStudio 2.7 Getting familiar with RStudio By now you’ve downloaded R and RStudio and you’re probably wondering, why do I need to download both? R is that programming language that is running on your computer. RStudio is what we call an Indegrated Development Environment (IDE) - this is a technical term for a pretty application that’s all dressed up on the surface but underneath is really crunching some numbers (using R) at serious speeds. RStudio is the application we’ll be using. Let’s open RStudio and get familiar with it. 2.8 Open RStudio Navigate to your applications folder on your computer. Launch RStudio. When you open it for the first time, you should see this. This is RStudio. When you open it for the first time, we’ll need to open a new RScript to begin coding. Open new R Script To open a new R Script, we select the blankpage with green plus icon and select R Script from the menu. This opens up the new R script and we can begin coding in R. Now that we have the R Script open, you’ll notice 4 quadrants. Let’s run through what those quadrants are. 2.9 R Studio Layout Now let’s describe what’s going on here in a little more detail. R Script - This is your canvas. This is where we write and edit our code. A lot of trial and error goes on here. R Console - This is where you run your code. When we talk about running code, we mean we’re telling R to execute the code we’ve written in the R Script. R Console is the place inside RStudio where we are using the R programming language. Variable Environment - This area keeps track of your variables, data tables, filenames, etc. Anything that you run in R that has a name will be stored here. Imagine the Variable Environment to be your closet - every time you make/buy a new sweater, the sweater goes in the closet. We can select data tables to view from this list here. Files/Plots/Help - In this quadrant, we can toggle through files on our computer (we can view where your files are stored), view plots/visualizations that we’re creating in R (whenever you create a plot in R it is output here first), search for help and descriptions of R functions (there’s descriptions on every function you’ll use in R - they can all be loaded here in the help tab), and more. "],["definitions-rules.html", "3 Definitions &amp; Rules 3.1 Important R Programming Definitions 3.2 Rules 3.3 General Recommendations", " 3 Definitions &amp; Rules This is a reference guide to look back on when you’re stuck. These definitions and rules are not expected to be understood right now but it’s important you know you can look back on this as a quick-reference. 3.1 Important R Programming Definitions Read through these now to get familiar and refer back to these whenever you need a refresher. You’re not expected to have these memorized or even understood at this moment. These will make more sense as we progress through the course. Coding Name Example Definition syntax R code the nomenclature and structure of a programming language debugging Failed R run debugging involves fixing R code that is written incorrectly and doesn’t run variable my_var Variables are used to store data, whose value can be changed according to our need. Variables can be declared using &lt;- (tradiational way) or by = (conventional way) package library(ggplot2) A collection of functions prewritten in R function print() A function is a set of statements organized together to perform a specific task. R has a set of preloaded functions that are part of the base package. If a function cannot be found as part of the base package, the function has likely already been built under another package that needs to be loaded in. Functions can be identified due to their enclosing parantheses () arguments read.csv(file = &quot;datasets/tv_shows.csv&quot;, header = FALSE) Components of a function that are separated by commas and declared using the = sign. Arguments in this example are file = and header = index tv_data[3,55] The position of data within a data frame, matrix, list, vector, etc. In R, data is indexed as [row,column] and indexing is done via brackets [] loop for (n in names){print(n)} Repeats a task for a specified number of times. Saves a programmer from repeating codelines with different parameters. logical TRUE, FALSE TRUE and FALSE logical operators are declared using all caps arithmetic operators +,-,*,/,^ Math operators used for addition, subtraction, multiplication, division, exponent, respectively. comparison operators ==, &lt;, &gt;, &lt;=, &gt;=, != Is equal to, less than, greater than, less than or equal to, greater than or equal to, is NOT equal to, respectively and/or operators &amp;, | AND, OR string a_string = &quot;anythign within quotes, single or double&quot; Any value written within a pair of single quote or double quotes in R is treated as a string. numeric 1 Any number - integer, float, etc. vector as.vector(x = c(1,2,3,4)) Vectors are the most basic R data objects and there are six types of atomic vectors. They are logical, integer, double, complex, character and raw. lists list('Peter', 'Sarah', 'Tom', 'Helen') Lists are the R objects which contain elements of different types like − numbers, strings, vectors and another list inside it matrix matrix(c(1:5), nrow = 3, byrow = TRUE) Matrices are the R objects in which the elements are arranged in a two-dimensional rectangular layout. array array(data = c(1,2,3)) Arrays are the R data objects which can store data in more than two dimensions. For example − If we create an array of dimension (1, 2, 3) then it creates 3 rectangular matrices each with 1 rows and 2 columns. Arrays can store only one data type. data frame data.frame(tv_data) R version of Excel Spreadsheet. A data frame is a table or a two-dimensional array-like structure in which each column contains values of one variable and each row contains one set of values from each column. factor factor() Factors are the data objects which are used to categorize the data and store it as levels. They can store both strings and integers. They are useful in the columns which have a limited number of unique values. Like “Male,”Female&quot; and True, False etc. They are useful in data analysis for statistical modeling. help help(read.csv) Default helper function in R. Opens up documentation on a particular function in the lower right quadrant of R. class class(tv_data) Tells us what R is recognizing something as concatenate (c) c(“a”, “b”, “c”) A quick utility for concatenating strings together filepath “/Users/james/Downloads/” The location on your computer where a file is stored. A filepath with a leading slash (akak “/” ) is also referred to as root. Root is the furthest back you can go on your computer. Think of a filepath like this - “/Earth/UnitedStates/Delaware/Newark/” Additional examples can be found here 3.2 Rules Variable names must be assigned. names_list &lt;- list(&#39;Peter&#39;, &#39;Sarah&#39;, &#39;Tom&#39;, &#39;Helen&#39;) # is the Comment Operator - anything on the same line of the # comment operator will not be run by R. # comments can be above names_list &lt;- list(&#39;Peter&#39;, &#39;Sarah&#39;, &#39;Tom&#39;, &#39;Helen&#39;) # comments can be outside ## comments can be anywhere. Parantheses (), Brackets [], Curly brackets {}, Quotations &quot;&quot; must be used in pairs names_list = list(&#39;Peter&#39;, &#39;Sarah&#39;, &#39;Tom&#39;, &#39;Helen&#39;) for (n in names_list){ print(n) } If you don’t have a package, you must install that package. (After you install once, you don’t need to install again.) install.packages(&#39;ggplot2&#39;) Packages MUST be loaded for each R session. library(ggplot2) Variable names should not replicate function/package names. 3.3 General Recommendations Comment, comment, comment. A comment is a brief note on what you were doing when you wrote a line of code. For example, if you write some R code that edits part of a dataframe (R’s version of an Excel Spreadsheet), comment what you were thinking here and why you did it this way. Once you become comfortable coding in R, you’ll be able to churn out new R scripts at a faster rate. It’s very important that you comment on what you’re doing at each step in the script so if you need to look back on something you wrote you can reference what you were doing there. A comment in R is declared using the pound symbol (#). Keep raw data raw. An advantage of R is being able to read in an original spreadsheet and output a new spreadsheet as a separate file. In other words, when you read in a dataset (for example, tv_shows.csv) and make changes to this file, do not save it was tv_shows.csv - thus overwriting the file. Instead, name it something like tv_shows_edited.csv. Also, notice how we use underscores (_) in between words of a filename - this is good practice that should be replicated (spaces are bad, see 4) When in doubt, Google your R question - look for StackOverflow links. StackOverflow is a web-forum where programmers can post questions for help. This is an incredible tool that even advanced programmers and developers use daily. There are other helpful forums out there - StackOverflow is the most popular. Spaces in variable/file names are BAD. A variable is an object or column that you create in R. For example, if you have a list of student names (student_names = list(&quot;John&quot;, &quot;Peter&quot;, &quot;Sebastian&quot;), the variable here would be student_names. Let’s get into the habit of using underscores ’_’ or dashes ‘-’ or periods ‘.’ to separate words instead of spaces. From the computers side of variable name storage, it’s much safer to declare a variable name such as data_file as opposed to data file Keep in mind, these will make more sense after we get more familiar with R - it’s alright if they’re confusing right now! "],["r-coding-fundamentals.html", "4 R Coding Fundamentals 4.1 Entering Input 4.2 Running Code 4.3 Evaluation 4.4 R Objects 4.5 Numbers 4.6 Attributes 4.7 Creating Vectors 4.8 Mixing Objects 4.9 Explicit Coercion 4.10 Matrices 4.11 Lists 4.12 Factors 4.13 Missing Values 4.14 Data Frames 4.15 Names 4.16 Summary", " 4 R Coding Fundamentals Now that we’re comfortable with R Studio and have some definitions under our belt, let’s dive in a little into some R code and discuss it. These fundamentals can always be referred back to when we might be stuck coding later on. 4.1 Entering Input In the R Script area, we write code. Whenever we want to assign a variable, we do so using the assignment operator. The &lt;- symbol is the assignment operator. We can also use = which is a bit more intuitive. It is alright to interchange these when assigning variables. val &lt;- 1 print(val) ## [1] 1 val ## [1] 1 msg &lt;- &quot;hello&quot; val and msg are both variables that we assigned. We use the # character to write comments inside our code. Commented code is NOT executed by R. x &lt;- ## Incomplete expression Anything to the right of the # (including the # itself) is ignored. 4.2 Running Code After placing the above code in your R Script area, we can run the code. Code execution is done in the R Console. We can “send” our code in the R Script to the R Console using the Run Button, ctrl + enter (Windows), or cmd + enter (Mac). We can select specific lines of code to run, larger chunks, or the entire R Script. 4.3 Evaluation When a complete expression is entered at the prompt, it is evaluated and the result of the evaluated expression is returned. The result may be auto-printed. val &lt;- 14 ## nothing printed val ## auto-printing occurs ## [1] 14 print(val) ## explicit printing ## [1] 14 The [1] shown in the output indicates that x is a vector and 14 is its first element. Typically we do not explicitly print variables since auto-printing is easier. When an R vector is printed you will notice that an index for the vector is printed in square brackets [] on the side. For example, see this integer sequence of length 10. my_seq &lt;- 10:20 my_seq ## [1] 10 11 12 13 14 15 16 17 18 19 20 Notice the [1] that preceeds the sequence. The output inside the square bracket is not part of the vector itself, it’s just part of the printed output that has additional information to be more user-friendly. This extra information is not part of the object itself. Also note that we used the : operator to create a sequence of integers from 10 to 20 (10:20). Note that the : operator is used to create integer sequences. 4.4 R Objects R has five basic or “atomic” classes of objects: character numeric (real numbers) integer complex logical (True/False) The most basic type of R object is a vector. Empty vectors can be created with the vector() function. There is really only one rule about vectors in R, which is that A vector can only contain objects of the same class. But of course, like any good rule, there is an exception, which is a list, which we will get to a bit later. A list is represented as a vector but can contain objects of different classes. Indeed, that’s usually why we use them. 4.5 Numbers Numbers in R are generally treated as numeric objects. We can explicitly declare numbers as integers, floats, etc., but I won’t cover that here. There is also a special number Inf which represents infinity. This allows us to represent entities like 1 / 0. This way, Inf can be used in ordinary calculations; e.g. 1 / Inf is 0. The value NaN represents an undefined value (“not a number”); e.g. 0 / 0; NaN can also be thought of as a missing value (more on that later) 4.6 Attributes R objects can have attributes, which are like metadata for the object. These metadata can be very useful in that they help to describe the object. For example, column names on a data frame help to tell us what data are contained in each of the columns. Some examples of R object attributes are names, dimnames dimensions (e.g. matrices, arrays) class (e.g. integer, numeric) length other user-defined attributes/metadata Attributes of an object (if any) can be accessed using the attributes() function. Not all R objects contain attributes, in which case the attributes() function returns NULL. 4.7 Creating Vectors The c() function is referred to as the concatenate function. Using this, we can create vectors of objects by concatenating them together. x &lt;- c(1.25, 2.50) ## numeric x &lt;- c(TRUE, FALSE) ## logical x &lt;- c(T, F) ## logical x &lt;- c(&quot;yes&quot;, &quot;no&quot;, &quot;maybe&quot;) ## character x &lt;- 25:44 ## integer x &lt;- c(1+2i, 3+8i) ## complex Note that in the above example, T and F are short-hand ways to specify TRUE and FALSE. However, in general one should try to use the explicit TRUE and FALSE values when indicating logical values. 4.8 Mixing Objects There are occasions when different classes of R objects get mixed together. Sometimes this happens by accident but it can also happen on purpose. So what happens with the following code? y &lt;- c(1.7, &quot;a&quot;) ## character y &lt;- c(TRUE, 2) ## numeric y &lt;- c(&quot;a&quot;, TRUE) ## character In each case above, we are mixing objects of two different classes in a vector. But remember that the only rule about vectors says this is not allowed. When different objects are mixed in a vector, coercion occurs so that every element in the vector is of the same class. In the example above, we see the effect of implicit coercion. What R tries to do is find a way to represent all of the objects in the vector in a reasonable fashion. Sometimes this does exactly what you want and…sometimes not. For example, combining a numeric object with a character object will create a character vector, because numbers can usually be easily represented as strings. 4.9 Explicit Coercion Objects can be explicitly coerced from one class to another using the as.* functions, if available. x &lt;- 0:10 class(x) ## [1] &quot;integer&quot; as.numeric(x) ## [1] 0 1 2 3 4 5 6 7 8 9 10 as.logical(x) ## [1] FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE as.character(x) ## [1] &quot;0&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; &quot;10&quot; Sometimes, R can’t figure out how to coerce an object and this can result in NAs being produced. x &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) as.numeric(x) ## Warning: NAs introduced by coercion ## [1] NA NA NA as.logical(x) ## [1] NA NA NA as.complex(x) ## Warning: NAs introduced by coercion ## [1] NA NA NA When nonsensical coercion takes place, you will usually get a warning from R. 4.10 Matrices Matrices are vectors with a dimension attribute. The dimension attribute is itself an integer vector of length 2 (number of rows, number of columns) m &lt;- matrix(nrow = 2, ncol = 3) m ## [,1] [,2] [,3] ## [1,] NA NA NA ## [2,] NA NA NA dim(m) ## [1] 2 3 attributes(m) ## $dim ## [1] 2 3 Matrices are constructed column-wise, so entries can be thought of starting in the “upper left” corner and running down the columns. m &lt;- matrix(1:6, nrow = 2, ncol = 3) m ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 Matrices can also be created directly from vectors by adding a dimension attribute. m &lt;- 1:10 m ## [1] 1 2 3 4 5 6 7 8 9 10 dim(m) &lt;- c(2, 5) m ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 3 5 7 9 ## [2,] 2 4 6 8 10 Matrices can be created by column-binding or row-binding with the cbind() and rbind() functions. x &lt;- 1:3 y &lt;- 10:12 cbind(x, y) ## x y ## [1,] 1 10 ## [2,] 2 11 ## [3,] 3 12 rbind(x, y) ## [,1] [,2] [,3] ## x 1 2 3 ## y 10 11 12 4.11 Lists Lists are a special type of vector that can contain elements of different classes. Lists are a very important data type in R and you should get to know them well. Lists, in combination with the various “apply” functions discussed later, make for a powerful combination. Lists can be explicitly created using the list() function, which takes an arbitrary number of arguments. x &lt;- list(1, &quot;a&quot;, TRUE) x ## [[1]] ## [1] 1 ## ## [[2]] ## [1] &quot;a&quot; ## ## [[3]] ## [1] TRUE We can also create an empty list of a prespecified length with the vector() function x &lt;- vector(&quot;list&quot;, length = 5) x ## [[1]] ## NULL ## ## [[2]] ## NULL ## ## [[3]] ## NULL ## ## [[4]] ## NULL ## ## [[5]] ## NULL 4.12 Factors Factors are used to represent categorical data and can be unordered or ordered. One can think of a factor as an integer vector where each integer has a label. Factors are important in statistical modeling and are treated specially by modelling functions like lm() and glm(). Using factors with labels is better than using integers because factors are self-describing. Having a variable that has values “Male” and “Female” is better than a variable that has values 1 and 2. Factor objects can be created with the factor() function. x &lt;- factor(c(&quot;yes&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;)) x ## [1] yes yes no yes no ## Levels: no yes table(x) ## x ## no yes ## 2 3 ## See the underlying representation of factor unclass(x) ## [1] 2 2 1 2 1 ## attr(,&quot;levels&quot;) ## [1] &quot;no&quot; &quot;yes&quot; Often factors will be automatically created for you when you read a dataset in using a function like read.table(). Those functions often default to creating factors when they encounter data that look like characters or strings. The order of the levels of a factor can be set using the levels argument to factor(). This can be important in linear modelling because the first level is used as the baseline level. x &lt;- factor(c(&quot;yes&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;)) x ## Levels are put in alphabetical order ## [1] yes yes no yes no ## Levels: no yes x &lt;- factor(c(&quot;yes&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;), levels = c(&quot;yes&quot;, &quot;no&quot;)) x ## [1] yes yes no yes no ## Levels: yes no 4.13 Missing Values Missing values are denoted by NA or NaN for q undefined mathematical operations. is.na() is used to test objects if they are NA is.nan() is used to test for NaN NA values have a class also, so there are integer NA, character NA, etc. A NaN value is also NA but the converse is not true ## Create a vector with NAs in it x &lt;- c(1, 2, NA, 10, 3) ## Return a logical vector indicating which elements are NA is.na(x) ## [1] FALSE FALSE TRUE FALSE FALSE ## Return a logical vector indicating which elements are NaN is.nan(x) ## [1] FALSE FALSE FALSE FALSE FALSE ## Now create a vector with both NA and NaN values x &lt;- c(1, 2, NaN, NA, 4) is.na(x) ## [1] FALSE FALSE TRUE TRUE FALSE is.nan(x) ## [1] FALSE FALSE TRUE FALSE FALSE 4.14 Data Frames Data frames are used to store tabular data in R. They are an important type of object in R and are used in a variety of statistical modeling applications. We’ll be working with many dataframes throughout these tutorials. Data frames are represented as a special type of list where every element of the list has to have the same length. Each element of the list can be thought of as a column and the length of each element of the list is the number of rows. Unlike matrices, data frames can store different classes of objects in each column. Matrices must have every element be the same class (e.g. all integers or all numeric). In addition to column names, indicating the names of the variables or predictors, data frames have a special attribute called row.names which indicate information about each row of the data frame. Data frames are usually created by reading in a dataset using the read.table() or read.csv(). However, data frames can also be created explicitly with the data.frame() function or they can be coerced from other types of objects like lists. Data frames can be converted to a matrix by calling data.matrix(). While it might seem that the as.matrix() function should be used to coerce a data frame to a matrix, almost always, what you want is the result of data.matrix(). x &lt;- data.frame(foo = 1:4, bar = c(T, T, F, F)) x ## foo bar ## 1 1 TRUE ## 2 2 TRUE ## 3 3 FALSE ## 4 4 FALSE nrow(x) ## [1] 4 ncol(x) ## [1] 2 4.15 Names R objects can have names, which is very useful for writing readable code and self-describing objects. Here is an example of assigning names to an integer vector. x &lt;- 1:3 names(x) ## NULL names(x) &lt;- c(&quot;New York&quot;, &quot;Seattle&quot;, &quot;Los Angeles&quot;) x ## New York Seattle Los Angeles ## 1 2 3 names(x) ## [1] &quot;New York&quot; &quot;Seattle&quot; &quot;Los Angeles&quot; Lists can also have names, which is often very useful. x &lt;- list(&quot;Los Angeles&quot; = 1, Boston = 2, London = 3) x ## $`Los Angeles` ## [1] 1 ## ## $Boston ## [1] 2 ## ## $London ## [1] 3 names(x) ## [1] &quot;Los Angeles&quot; &quot;Boston&quot; &quot;London&quot; Matrices can have both column and row names. m &lt;- matrix(1:4, nrow = 2, ncol = 2) dimnames(m) &lt;- list(c(&quot;a&quot;, &quot;b&quot;), c(&quot;c&quot;, &quot;d&quot;)) m ## c d ## a 1 3 ## b 2 4 Column names and row names can be set separately using the colnames() and rownames() functions. colnames(m) &lt;- c(&quot;h&quot;, &quot;f&quot;) rownames(m) &lt;- c(&quot;x&quot;, &quot;z&quot;) m ## h f ## x 1 3 ## z 2 4 Note that for data frames, there is a separate function for setting the row names, the row.names() function. Also, data frames do not have column names, they just have names (like lists). So to set the column names of a data frame just use the names() function. Yes, I know its confusing. Here’s a quick summary: Object Set column names Set row names data frame names() row.names() matrix colnames() rownames() 4.16 Summary There are a variety of different builtin-data types in R. In this chapter we have reviewed the following atomic classes: numeric, logical, character, integer, complex vectors, lists factors missing values data frames and matrices All R objects can have attributes that help to describe what is in the object. Perhaps the most useful attribute is names, such as column and row names in a data frame, or simply names in a vector or list. Attributes like dimensions are also important as they can modify the behavior of objects, like turning a vector into a matrix. The content in this section was adapted from Dr. Roger Peng "],["basic-r-scripts.html", "5 Basic R Scripts 5.1 Global Carbon-Dioxide Concentrations 5.2 Cars - Motor Trends Magazine Data", " 5 Basic R Scripts In this example, we’ll begin with data science question and answer it in R. We’ll go through these scripts line by line to show how we can use R. To follow along, copy each of these lines and paste them in your R Script (top left quadrant). Once it’s pasted there, we can run each line and view the output in the R Console (bottom left quadrant). 5.1 Global Carbon-Dioxide Concentrations CO2 is a greenhouse gas responsible for trapping heat. Human’s have released more CO2 into the atmosphere through industrialism. How have global carbon-dioxide (CO2) concentrations changed over time? First, we load the dataset. In our case, the dataset we’ll be using is pre-built into R as co2, meaning we don’t need to download this dataset, we just need to call it. The co2 dataset contains atmospheric concentrations of CO2 are expressed in parts per million (ppm). This data is a time-series of monthly CO2 concentrations recorded between 1959 and 1997. First, let’s make sure the pre-built data is installed correcly. co2 ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct ## 1959 315.42 316.31 316.50 317.56 318.13 318.00 316.39 314.65 313.68 313.18 ## 1960 316.27 316.81 317.42 318.87 319.87 319.43 318.01 315.74 314.00 313.68 ## 1961 316.73 317.54 318.38 319.31 320.42 319.61 318.42 316.63 314.83 315.16 ## 1962 317.78 318.40 319.53 320.42 320.85 320.45 319.45 317.25 316.11 315.27 ## 1963 318.58 318.92 319.70 321.22 322.08 321.31 319.58 317.61 316.05 315.83 ## 1964 319.41 320.07 320.74 321.40 322.06 321.73 320.27 318.54 316.54 316.71 ## 1965 319.27 320.28 320.73 321.97 322.00 321.71 321.05 318.71 317.66 317.14 ## 1966 320.46 321.43 322.23 323.54 323.91 323.59 322.24 320.20 318.48 317.94 ## 1967 322.17 322.34 322.88 324.25 324.83 323.93 322.38 320.76 319.10 319.24 ## 1968 322.40 322.99 323.73 324.86 325.40 325.20 323.98 321.95 320.18 320.09 ## 1969 323.83 324.26 325.47 326.50 327.21 326.54 325.72 323.50 322.22 321.62 ## 1970 324.89 325.82 326.77 327.97 327.91 327.50 326.18 324.53 322.93 322.90 ## 1971 326.01 326.51 327.01 327.62 328.76 328.40 327.20 325.27 323.20 323.40 ## 1972 326.60 327.47 327.58 329.56 329.90 328.92 327.88 326.16 324.68 325.04 ## 1973 328.37 329.40 330.14 331.33 332.31 331.90 330.70 329.15 327.35 327.02 ## 1974 329.18 330.55 331.32 332.48 332.92 332.08 331.01 329.23 327.27 327.21 ## 1975 330.23 331.25 331.87 333.14 333.80 333.43 331.73 329.90 328.40 328.17 ## 1976 331.58 332.39 333.33 334.41 334.71 334.17 332.89 330.77 329.14 328.78 ## 1977 332.75 333.24 334.53 335.90 336.57 336.10 334.76 332.59 331.42 330.98 ## 1978 334.80 335.22 336.47 337.59 337.84 337.72 336.37 334.51 332.60 332.38 ## 1979 336.05 336.59 337.79 338.71 339.30 339.12 337.56 335.92 333.75 333.70 ## 1980 337.84 338.19 339.91 340.60 341.29 341.00 339.39 337.43 335.72 335.84 ## 1981 339.06 340.30 341.21 342.33 342.74 342.08 340.32 338.26 336.52 336.68 ## 1982 340.57 341.44 342.53 343.39 343.96 343.18 341.88 339.65 337.81 337.69 ## 1983 341.20 342.35 342.93 344.77 345.58 345.14 343.81 342.21 339.69 339.82 ## 1984 343.52 344.33 345.11 346.88 347.25 346.62 345.22 343.11 340.90 341.18 ## 1985 344.79 345.82 347.25 348.17 348.74 348.07 346.38 344.51 342.92 342.62 ## 1986 346.11 346.78 347.68 349.37 350.03 349.37 347.76 345.73 344.68 343.99 ## 1987 347.84 348.29 349.23 350.80 351.66 351.07 349.33 347.92 346.27 346.18 ## 1988 350.25 351.54 352.05 353.41 354.04 353.62 352.22 350.27 348.55 348.72 ## 1989 352.60 352.92 353.53 355.26 355.52 354.97 353.75 351.52 349.64 349.83 ## 1990 353.50 354.55 355.23 356.04 357.00 356.07 354.67 352.76 350.82 351.04 ## 1991 354.59 355.63 357.03 358.48 359.22 358.12 356.06 353.92 352.05 352.11 ## 1992 355.88 356.63 357.72 359.07 359.58 359.17 356.94 354.92 352.94 353.23 ## 1993 356.63 357.10 358.32 359.41 360.23 359.55 357.53 355.48 353.67 353.95 ## 1994 358.34 358.89 359.95 361.25 361.67 360.94 359.55 357.49 355.84 356.00 ## 1995 359.98 361.03 361.66 363.48 363.82 363.30 361.94 359.50 358.11 357.80 ## 1996 362.09 363.29 364.06 364.76 365.45 365.01 363.70 361.54 359.51 359.65 ## 1997 363.23 364.06 364.61 366.40 366.84 365.68 364.52 362.57 360.24 360.83 ## Nov Dec ## 1959 314.66 315.43 ## 1960 314.84 316.03 ## 1961 315.94 316.85 ## 1962 316.53 317.53 ## 1963 316.91 318.20 ## 1964 317.53 318.55 ## 1965 318.70 319.25 ## 1966 319.63 320.87 ## 1967 320.56 321.80 ## 1968 321.16 322.74 ## 1969 322.69 323.95 ## 1970 323.85 324.96 ## 1971 324.63 325.85 ## 1972 326.34 327.39 ## 1973 327.99 328.48 ## 1974 328.29 329.41 ## 1975 329.32 330.59 ## 1976 330.14 331.52 ## 1977 332.24 333.68 ## 1978 333.75 334.78 ## 1979 335.12 336.56 ## 1980 336.93 338.04 ## 1981 338.19 339.44 ## 1982 339.09 340.32 ## 1983 340.98 342.82 ## 1984 342.80 344.04 ## 1985 344.06 345.38 ## 1986 345.48 346.72 ## 1987 347.64 348.78 ## 1988 349.91 351.18 ## 1989 351.14 352.37 ## 1990 352.69 354.07 ## 1991 353.64 354.89 ## 1992 354.09 355.33 ## 1993 355.30 356.78 ## 1994 357.59 359.05 ## 1995 359.61 360.74 ## 1996 360.80 362.38 ## 1997 362.49 364.34 This dataset is stored in R as co2. We can create a new variable called co2_data that replicates this co2 dataset. co2_data &lt;- co2 co2_data ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct ## 1959 315.42 316.31 316.50 317.56 318.13 318.00 316.39 314.65 313.68 313.18 ## 1960 316.27 316.81 317.42 318.87 319.87 319.43 318.01 315.74 314.00 313.68 ## 1961 316.73 317.54 318.38 319.31 320.42 319.61 318.42 316.63 314.83 315.16 ## 1962 317.78 318.40 319.53 320.42 320.85 320.45 319.45 317.25 316.11 315.27 ## 1963 318.58 318.92 319.70 321.22 322.08 321.31 319.58 317.61 316.05 315.83 ## 1964 319.41 320.07 320.74 321.40 322.06 321.73 320.27 318.54 316.54 316.71 ## 1965 319.27 320.28 320.73 321.97 322.00 321.71 321.05 318.71 317.66 317.14 ## 1966 320.46 321.43 322.23 323.54 323.91 323.59 322.24 320.20 318.48 317.94 ## 1967 322.17 322.34 322.88 324.25 324.83 323.93 322.38 320.76 319.10 319.24 ## 1968 322.40 322.99 323.73 324.86 325.40 325.20 323.98 321.95 320.18 320.09 ## 1969 323.83 324.26 325.47 326.50 327.21 326.54 325.72 323.50 322.22 321.62 ## 1970 324.89 325.82 326.77 327.97 327.91 327.50 326.18 324.53 322.93 322.90 ## 1971 326.01 326.51 327.01 327.62 328.76 328.40 327.20 325.27 323.20 323.40 ## 1972 326.60 327.47 327.58 329.56 329.90 328.92 327.88 326.16 324.68 325.04 ## 1973 328.37 329.40 330.14 331.33 332.31 331.90 330.70 329.15 327.35 327.02 ## 1974 329.18 330.55 331.32 332.48 332.92 332.08 331.01 329.23 327.27 327.21 ## 1975 330.23 331.25 331.87 333.14 333.80 333.43 331.73 329.90 328.40 328.17 ## 1976 331.58 332.39 333.33 334.41 334.71 334.17 332.89 330.77 329.14 328.78 ## 1977 332.75 333.24 334.53 335.90 336.57 336.10 334.76 332.59 331.42 330.98 ## 1978 334.80 335.22 336.47 337.59 337.84 337.72 336.37 334.51 332.60 332.38 ## 1979 336.05 336.59 337.79 338.71 339.30 339.12 337.56 335.92 333.75 333.70 ## 1980 337.84 338.19 339.91 340.60 341.29 341.00 339.39 337.43 335.72 335.84 ## 1981 339.06 340.30 341.21 342.33 342.74 342.08 340.32 338.26 336.52 336.68 ## 1982 340.57 341.44 342.53 343.39 343.96 343.18 341.88 339.65 337.81 337.69 ## 1983 341.20 342.35 342.93 344.77 345.58 345.14 343.81 342.21 339.69 339.82 ## 1984 343.52 344.33 345.11 346.88 347.25 346.62 345.22 343.11 340.90 341.18 ## 1985 344.79 345.82 347.25 348.17 348.74 348.07 346.38 344.51 342.92 342.62 ## 1986 346.11 346.78 347.68 349.37 350.03 349.37 347.76 345.73 344.68 343.99 ## 1987 347.84 348.29 349.23 350.80 351.66 351.07 349.33 347.92 346.27 346.18 ## 1988 350.25 351.54 352.05 353.41 354.04 353.62 352.22 350.27 348.55 348.72 ## 1989 352.60 352.92 353.53 355.26 355.52 354.97 353.75 351.52 349.64 349.83 ## 1990 353.50 354.55 355.23 356.04 357.00 356.07 354.67 352.76 350.82 351.04 ## 1991 354.59 355.63 357.03 358.48 359.22 358.12 356.06 353.92 352.05 352.11 ## 1992 355.88 356.63 357.72 359.07 359.58 359.17 356.94 354.92 352.94 353.23 ## 1993 356.63 357.10 358.32 359.41 360.23 359.55 357.53 355.48 353.67 353.95 ## 1994 358.34 358.89 359.95 361.25 361.67 360.94 359.55 357.49 355.84 356.00 ## 1995 359.98 361.03 361.66 363.48 363.82 363.30 361.94 359.50 358.11 357.80 ## 1996 362.09 363.29 364.06 364.76 365.45 365.01 363.70 361.54 359.51 359.65 ## 1997 363.23 364.06 364.61 366.40 366.84 365.68 364.52 362.57 360.24 360.83 ## Nov Dec ## 1959 314.66 315.43 ## 1960 314.84 316.03 ## 1961 315.94 316.85 ## 1962 316.53 317.53 ## 1963 316.91 318.20 ## 1964 317.53 318.55 ## 1965 318.70 319.25 ## 1966 319.63 320.87 ## 1967 320.56 321.80 ## 1968 321.16 322.74 ## 1969 322.69 323.95 ## 1970 323.85 324.96 ## 1971 324.63 325.85 ## 1972 326.34 327.39 ## 1973 327.99 328.48 ## 1974 328.29 329.41 ## 1975 329.32 330.59 ## 1976 330.14 331.52 ## 1977 332.24 333.68 ## 1978 333.75 334.78 ## 1979 335.12 336.56 ## 1980 336.93 338.04 ## 1981 338.19 339.44 ## 1982 339.09 340.32 ## 1983 340.98 342.82 ## 1984 342.80 344.04 ## 1985 344.06 345.38 ## 1986 345.48 346.72 ## 1987 347.64 348.78 ## 1988 349.91 351.18 ## 1989 351.14 352.37 ## 1990 352.69 354.07 ## 1991 353.64 354.89 ## 1992 354.09 355.33 ## 1993 355.30 356.78 ## 1994 357.59 359.05 ## 1995 359.61 360.74 ## 1996 360.80 362.38 ## 1997 362.49 364.34 We replicate this dataset and rename it as co2_data so that it’s a variable that is shown in our global environment (top right quadrant). What class is this data? class(co2_data) ## [1] &quot;ts&quot; A ts class is a time-series class. We can print out a summary of the co2 dataset like so. summary(co2_data) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 313.2 323.5 335.2 337.1 350.3 366.8 The summary() function is a base function R offers for quick statistics. How long is this dataset? length(co2_data) ## [1] 468 468 months worth of CO2 observations. Can we plot this dataset? plot(x = co2, ylab = &quot;Atmospheric concentration of CO2 (ppm)&quot;, main = &quot;CO2 Dataset&quot;) Altogether, this script looks like this… # make sure the dataset is loaded - it&#39;s a prebuilt dataset automatically loaded into R co2 # rename the dataset as co2_data so it is visible in our global environment co2_data &lt;- co2 co2_data # identify the class of the co2_data object class(co2_data) # print the summary of the co2_data dataset summary(co2_data) # find the length of the co2_data dataset using the length() function length(co2_data) # plot the dataset using the default plot function plot(x = co2, ylab = &quot;Atmospheric concentration of CO2 (ppm)&quot;, main = &quot;CO2 Dataset&quot;) Above is our script. We run this script in the console. We can save this script and re-run this at any time. An example of this would be saving this file as co2_script.R. Once this is saved, you can close R, re-open it, and re-run your co2_script.R without re-writing any code. 5.1.1 Saving your plot We can save any plot from the R plot window. Simply navigate to the Plots tab and select Export then Save As Image. Figure 5.1: Save As Image 5.2 Cars - Motor Trends Magazine Data The data was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models). Load the dataset - again this is a pre-loaded dataset, but let’s call on it so we can bring it into our global environment. data(&#39;mtcars&#39;) head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 The head() function is a quick function that prints out the first few rows of a dataset. What’s exactly is mtcars? class(mtcars) ## [1] &quot;data.frame&quot; It’s a data.frame. Data Frames have a different storage than time series. You can also view this dataframe by clicking on the mtcars dataframe in your global environment. What are the dimensions of this dataframe? How many rows and columns does it have? dim(mtcars) ## [1] 32 11 nrow(mtcars) ## [1] 32 ncol(mtcars) ## [1] 11 We have 32 rows and 11 columns within this dataframe. What are our column names? colnames(mtcars) ## [1] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; &quot;drat&quot; &quot;wt&quot; &quot;qsec&quot; &quot;vs&quot; &quot;am&quot; &quot;gear&quot; ## [11] &quot;carb&quot; What are our row names (aka the make of the car)? rownames(mtcars) ## [1] &quot;Mazda RX4&quot; &quot;Mazda RX4 Wag&quot; &quot;Datsun 710&quot; ## [4] &quot;Hornet 4 Drive&quot; &quot;Hornet Sportabout&quot; &quot;Valiant&quot; ## [7] &quot;Duster 360&quot; &quot;Merc 240D&quot; &quot;Merc 230&quot; ## [10] &quot;Merc 280&quot; &quot;Merc 280C&quot; &quot;Merc 450SE&quot; ## [13] &quot;Merc 450SL&quot; &quot;Merc 450SLC&quot; &quot;Cadillac Fleetwood&quot; ## [16] &quot;Lincoln Continental&quot; &quot;Chrysler Imperial&quot; &quot;Fiat 128&quot; ## [19] &quot;Honda Civic&quot; &quot;Toyota Corolla&quot; &quot;Toyota Corona&quot; ## [22] &quot;Dodge Challenger&quot; &quot;AMC Javelin&quot; &quot;Camaro Z28&quot; ## [25] &quot;Pontiac Firebird&quot; &quot;Fiat X1-9&quot; &quot;Porsche 914-2&quot; ## [28] &quot;Lotus Europa&quot; &quot;Ford Pantera L&quot; &quot;Ferrari Dino&quot; ## [31] &quot;Maserati Bora&quot; &quot;Volvo 142E&quot; How do we extract individual columns/variables from this dataframe? mtcars[&quot;mpg&quot;] ## mpg ## Mazda RX4 21.0 ## Mazda RX4 Wag 21.0 ## Datsun 710 22.8 ## Hornet 4 Drive 21.4 ## Hornet Sportabout 18.7 ## Valiant 18.1 ## Duster 360 14.3 ## Merc 240D 24.4 ## Merc 230 22.8 ## Merc 280 19.2 ## Merc 280C 17.8 ## Merc 450SE 16.4 ## Merc 450SL 17.3 ## Merc 450SLC 15.2 ## Cadillac Fleetwood 10.4 ## Lincoln Continental 10.4 ## Chrysler Imperial 14.7 ## Fiat 128 32.4 ## Honda Civic 30.4 ## Toyota Corolla 33.9 ## Toyota Corona 21.5 ## Dodge Challenger 15.5 ## AMC Javelin 15.2 ## Camaro Z28 13.3 ## Pontiac Firebird 19.2 ## Fiat X1-9 27.3 ## Porsche 914-2 26.0 ## Lotus Europa 30.4 ## Ford Pantera L 15.8 ## Ferrari Dino 19.7 ## Maserati Bora 15.0 ## Volvo 142E 21.4 We can also extract the vector of data using the $ operator. mtcars$mpg ## [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4 ## [16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7 ## [31] 15.0 21.4 What are the statistics like for each variable? summary(mtcars) ## mpg cyl disp hp ## Min. :10.40 Min. :4.000 Min. : 71.1 Min. : 52.0 ## 1st Qu.:15.43 1st Qu.:4.000 1st Qu.:120.8 1st Qu.: 96.5 ## Median :19.20 Median :6.000 Median :196.3 Median :123.0 ## Mean :20.09 Mean :6.188 Mean :230.7 Mean :146.7 ## 3rd Qu.:22.80 3rd Qu.:8.000 3rd Qu.:326.0 3rd Qu.:180.0 ## Max. :33.90 Max. :8.000 Max. :472.0 Max. :335.0 ## drat wt qsec vs ## Min. :2.760 Min. :1.513 Min. :14.50 Min. :0.0000 ## 1st Qu.:3.080 1st Qu.:2.581 1st Qu.:16.89 1st Qu.:0.0000 ## Median :3.695 Median :3.325 Median :17.71 Median :0.0000 ## Mean :3.597 Mean :3.217 Mean :17.85 Mean :0.4375 ## 3rd Qu.:3.920 3rd Qu.:3.610 3rd Qu.:18.90 3rd Qu.:1.0000 ## Max. :4.930 Max. :5.424 Max. :22.90 Max. :1.0000 ## am gear carb ## Min. :0.0000 Min. :3.000 Min. :1.000 ## 1st Qu.:0.0000 1st Qu.:3.000 1st Qu.:2.000 ## Median :0.0000 Median :4.000 Median :2.000 ## Mean :0.4062 Mean :3.688 Mean :2.812 ## 3rd Qu.:1.0000 3rd Qu.:4.000 3rd Qu.:4.000 ## Max. :1.0000 Max. :5.000 Max. :8.000 Notice that now the summary() function is printing out the summary statistics for each column (aka variable) within our dataframe (mtcars) What if we just wanted to focus on the first 5 cars in the dataset? We need to index. mtcars[1:5,] ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Remember, data is stored as Row, Column. Above, we’re indexing the first 5 rows and then including all columns. What if we just wanted to focus on the first column? mtcars[1:5,1] ## [1] 21.0 21.0 22.8 21.4 18.7 That’s the mpg column of the first 5 cars. Which car has the best miles per gallon (mpg)? barplot(height = mtcars$mpg[1:5], names.arg = rownames(mtcars)[1:5]) The Datsun 710 has the highest MPG rating of the first 5 cars (rows) in the dataset. The entire script looks like this: # load the data - the mtcars dataset is pre-built data(&#39;mtcars&#39;) # print out the first few rows of the dataset using the head() function head(mtcars) # print the class of the mtcars dataset class(mtcars) # dimensions of the mtcars dataframe dim(mtcars) # number of rows nrow(mtcars) # number of columns ncol(mtcars) # column names colnames(mtcars) # row names rownames(mtcars) # selecting the miles per gallon column mtcars[&quot;mpg&quot;] # selecting the vector of the mpg column mtcars$mpg # printing a summary of the dataframe summary(mtcars) # indexing the first 5 rows, including all of the columns mtcars[1:5,] # indexing the first 5 rows and the 1st column mtcars[1:5,1] # creating a barplot of mpg for the first 5 cars barplot(height = mtcars$mpg[1:5], names.arg = rownames(mtcars)[1:5]) "],["scientific-r-scripts.html", "6 Scientific R Scripts 6.1 Script Breakdown 6.2 In Class Exercise", " 6 Scientific R Scripts So far, the examples of R scripts we’ve shown have been general purpose scripts. For this week, we’re going to cover an example where we want to plot sea surface temperature from a given netCDF file. A netCDF file (network Common Data Format) is an advanced filetype that is incredibly efficient at storing data and heavily used in physical sciences. Within each file, metadata (time, latitude info, longitude info, projection, etc.), and variables (sea surface temperature, latitude points, longitude points, chlorophyll, etc.) can be found. The netCDF file we’ll be looking at in this example is OR_ABI-L2-SSTF-M3_G16_s20192081300453_e20192081400161_c20192081406297.nc. This is a data file from the GOES-16 (Geostationary Operational Environmental Satellite) satellite that shows a snapshot of the Northwest Atlantic Ocean and Eastern Seaboard. So we have the data, but what does it look like? A scientific R Script might look like this: # James Simkins # Load libraries library(ncdf4) library(fields) ncFile &lt;- ncdf4::nc_open(&quot;~/Documents/Github/geog473-673/datasets/OR_ABI-L2-SSTF-M3_G16_s20192081300453_e20192081400161_c20192081406297.nc&quot;) sstK &lt;- ncdf4::ncvar_get(nc=ncFile, varid=&quot;SST&quot;) lat &lt;- ncdf4::ncvar_get(nc=ncFile, varid=&quot;latitude&quot;) lon &lt;- ncdf4::ncvar_get(nc=ncFile, varid=&quot;longitude&quot;) # convert sst from Kelvin to Celsius sstC &lt;- sstK - 273.15 # remove values below 0C sstC[sstC &lt; 0] = NA # Plot the matrix fields::image.plot(x=lon, y=lat, z=sstC, legend.lab=&quot;Celsius&quot;) title(&quot;GOES-R SST Product 7/25/2019 19:00 UTC&quot;) 6.1 Script Breakdown # Load libraries library(ncdf4) library(fields) In R, we need to call on packages/libraries that we want to load in. As a reminder, packages are the toolbox we want to bring into R and the functions inside each package are the tools within that specific toolbox. The library() base function loads packages. After we load a package, R will know what we mean when we call on a function from that package. # Load libraries library(ncdf4) library(fields) ####################################### ncFile &lt;- ncdf4::nc_open(filename=&quot;~/Documents/Github/geog473-673/datasets/OR_ABI-L2-SSTF-M3_G16_s20192081300453_e20192081400161_c20192081406297.nc&quot;) sstK &lt;- ncdf4::ncvar_get(nc=ncFile, varid=&quot;SST&quot;) lat &lt;- ncdf4::ncvar_get(nc=ncFile, varid=&quot;latitude&quot;) lon &lt;- ncdf4::ncvar_get(nc=ncFile, varid=&quot;longitude&quot;) Our object name is ncFile and it holds an opened NetCDF file. We open this file via the nc_open() function that’s within the ncdf4 library. Note the ncdf4::nc_open() syntax. Using this syntax, we explicitly tell R we wish to use the nc_open() function within (::) the ncdf4 package. This is not necessary after loading the ncdf4 package above but is still important ot know. Notice we can use either &lt;- or = for objects (like ncFile or sstK) but I MUST use = within the function ‘walls’ (the parantheses). We open the netcdf file and then extract what we want out of it using ncvar_get(), which is short for “netcdf variable get”. Confused about how to use ncvar_get()? Try running help(ncvar_get) in your console. # Load libraries library(ncdf4) library(fields) ncFile &lt;- ncdf4::nc_open(filename=&quot;~/Documents/Github/geog473-673/datasets/OR_ABI-L2-SSTF-M3_G16_s20192081300453_e20192081400161_c20192081406297.nc&quot;) sstK &lt;- ncdf4::ncvar_get(nc=ncFile, varid=&quot;SST&quot;) ####################################### # convert sst from Kelvin to Celsius sstC &lt;- sstK - 273.15 The SST variable from the netCDF file was in Kelvin and we want to convert it to Celsius. Right now, sstK is a matrix. How do I know this? Look at your environment, or simply type into your console: # Load libraries library(ncdf4) library(fields) ncFile &lt;- ncdf4::nc_open(filename=&quot;~/Documents/Github/geog473-673/datasets/OR_ABI-L2-SSTF-M3_G16_s20192081300453_e20192081400161_c20192081406297.nc&quot;) sstK &lt;- ncdf4::ncvar_get(nc=ncFile, varid=&quot;SST&quot;) ####################################### class(sstK) ## [1] &quot;matrix&quot; &quot;array&quot; Class is a useful function that is loaded with the base library everytime you fire up R. It tells us what type of object we have. Now that we know this is a matrix, we can subtract 0 Celsius, or 273.15 Kelvin. When we have a matrix in R and perform any math on it, it does that math on each and every matrix value. #Quick and Dirty Quality Control There are bad values that crept into the dataset and we need to convert all of them to NaN (aka Not A Number…also known as NA (Not Available) in R). How do we know there are bad values in this dataset? # Load libraries library(ncdf4) library(fields) ncFile &lt;- ncdf4::nc_open(filename=&quot;~/Documents/Github/geog473-673/datasets/OR_ABI-L2-SSTF-M3_G16_s20192081300453_e20192081400161_c20192081406297.nc&quot;) sstK &lt;- ncdf4::ncvar_get(nc=ncFile, varid=&quot;SST&quot;) ####################################### summary(as.vector(sstK)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## -999.0 286.9 300.0 278.2 302.0 310.0 2175822 summary() is another great base function. In order to use it on a matrix, we need to convert it to a vector - summary can’t do 2 dimensional objects like matrices, it needs a one dimensional vector of numbers. Notice the Min in the summary output. -999? No way is that a valid Kelvin value, especially since we subtract another 273.15 to this number to make the the Celsius matrix. So clearly we have some bad data that we need to convert to NA’s. We do this by… library(ncdf4) library(fields) ncFile &lt;- ncdf4::nc_open(&quot;~/Documents/Github/geog473-673/datasets/OR_ABI-L2-SSTF-M3_G16_s20192081300453_e20192081400161_c20192081406297.nc&quot;) sstK &lt;- ncdf4::ncvar_get(nc=ncFile, varid=&quot;SST&quot;) lat &lt;- ncdf4::ncvar_get(nc=ncFile, varid=&quot;latitude&quot;) lon &lt;- ncdf4::ncvar_get(nc=ncFile, varid=&quot;longitude&quot;) # convert sst from Kelvin to Celsius sstC &lt;- sstK - 273.15 ####################################### # remove values below 0C sstC[sstC &lt; 0] = NA This line reads as: sstC where sstC is less than 0 equals NA. The brackets here can be thought of as the ‘condition’, that is what you’re looking to change. This is called a vector operation, which we will get more into later but these are important becuase it’s far faster to do this than a for loop. library(ncdf4) library(fields) ncFile &lt;- ncdf4::nc_open(&quot;~/Documents/Github/geog473-673/datasets/OR_ABI-L2-SSTF-M3_G16_s20192081300453_e20192081400161_c20192081406297.nc&quot;) sstK &lt;- ncdf4::ncvar_get(nc=ncFile, varid=&quot;SST&quot;) lat &lt;- ncdf4::ncvar_get(nc=ncFile, varid=&quot;latitude&quot;) lon &lt;- ncdf4::ncvar_get(nc=ncFile, varid=&quot;longitude&quot;) # convert sst from Kelvin to Celsius sstC &lt;- sstK - 273.15 # remove values below 0C ####################################### sstC[sstC &lt; 0] = NA summary(as.vector(sstC)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.0 19.8 27.6 24.1 29.0 36.8 2679819 Now we see that our summary looks good (even though we’re looking at a matrix in celsius now). So all that’s left to do is plot this up… # James Simkins # Load libraries library(ncdf4) library(fields) ncFile &lt;- ncdf4::nc_open(&quot;~/Documents/Github/geog473-673/datasets/OR_ABI-L2-SSTF-M3_G16_s20192081300453_e20192081400161_c20192081406297.nc&quot;) sstK &lt;- ncdf4::ncvar_get(nc=ncFile, varid=&quot;SST&quot;) lat &lt;- ncdf4::ncvar_get(nc=ncFile, varid=&quot;latitude&quot;) lon &lt;- ncdf4::ncvar_get(nc=ncFile, varid=&quot;longitude&quot;) # convert sst from Kelvin to Celsius sstC &lt;- sstK - 273.15 # remove values below 0C sstC[sstC &lt; 0] = NA ####################################### # Plot the matrix fields::image.plot(x=lon, y=lat, z=sstC, legend.lab=&quot;Celsius&quot;) title(&quot;GOES-R SST Product 7/25/2019 19:00 UTC&quot;) Now we plot this up using the image.plot() function from the fields library. image.plot() takes the arguments x, y, and z. This is a 2 dimensional dataset of Sea Surface Temperature where the x is longitude, the y is latitude, and what we plot within the box (z) is sstC. Notice that for this kind of plot, the ‘title()’ function is a separate function rather than an argument that goes inside of image.plot(). Remember, if you’re ever confused about something like this just tell R you need ‘help()’. 6.2 In Class Exercise Go to https://github.com/jsimkins2/geog473-673/tree/master/datasets Download ‘OR_ABI-L2-SSTF-M3_G16_s20192081300453_e20192081400161_c20192081406297.nc’ Make simple plot of the ‘DQF’ (Data Quality Flag) variable -Hint: You may have to use ‘install.packages’ in your console first Aim for something like this: 6.2.1 Further Explanation of DataTypes If you look into your environment from the in class exercise, you’ll notice under the ‘Data’ tab you have a large matrix of the DQF values. R stores these matrices without the column/row identifier (aka lat &amp; lon). We provide the image.plot() function with the lon &amp; lat arrays becuase it doesn’t know what the x &amp; y coordinates are of the matrix. Notice that the environment tells you the dimensions - the matrix is 2778 x 1989. R is indexed from 1 to the length of the dimension. Here is what I mean: dim(sstC) is 2778 1989 dim(lat) is 1989 lat[0] is numeric(0) lat[1] is 16.00283 lat[1989] is 51.98563 lat[1990] is NA The above tests are referred to as indexing. The 1st point of the lat array is 16.00283. In R, we index using brackets []. If you want to find more values other than just a single point, the procedure is referred to as indexing/slicing/subsetting. lat[1:10] is 16.00283 16.02093 16.03903 16.05713 16.07523 16.09333 16.11143 16.12953 16.14763 16.16573 The lat object we’ve been exploring here is an ‘array’. An array is a vector with one or more dimensions. So, an array with one dimension is (almost) the same as a vector. An array with two dimensions is (almost) the same as a matrix. An array with three or more dimensions is an n-dimensional array. A vector is what is called an array in all other programming languages except R — a collection of cells with a fixed size where all cells hold the same type (integers or characters or reals or whatever). "],["time-series-data.html", "7 Time Series Data 7.1 For Loops 7.2 DEOS Data 7.3 Assignment", " 7 Time Series Data This week we’ll look at some Time-Series data from DEOS. Delaware has the highest concentration of environmental monitoring stations in the country thanks to DEOS (Delaware Environmental Observing System) which is controlled by University of Delaware’s CEMA (Center for Environmental Monitoring and Analysis). The data collected using this dense network is useful for a variety of purposes. Before we dive into the data, let’s cover an important skill - the for loop. 7.1 For Loops For loops &amp; conditional statements are a key skill in programming. They allow you to process through large datasets or multiple datasets thus minimizing the amount of manual work you need to do. The basic for loop looks like this… # Generate sequence of numbers from 1 to 10 using the seq() function (seq for sequence) numbersList = seq(from=1,to=10,by=1) # Multiply each number in the numbersList by 8 and print the result for (i in numbersList){ temNumber = i * 8 print(temNumber) } ## [1] 8 ## [1] 16 ## [1] 24 ## [1] 32 ## [1] 40 ## [1] 48 ## [1] 56 ## [1] 64 ## [1] 72 ## [1] 80 Notice the general structure of R for loops. for signals to R you’re beginning a for loop, which requires the general structure to look like: for (something in something inside these parentheses){ do something within these curly brackets } Yes, you must have these parentheses and curly brackets present and surrounding the appropriate code. If you forget a parentheses or curly bracket you’ll have errors pop up…this happens to advanced programmers all the time, so don’t be discouraged if it takes time getting used to this. The general structure is always: for(condition){do something}. If statements are set up the same way # Generate sequence of numbers from 1 to 10 using the seq() function (seq for sequence) numbersList = seq(from=1,to=10,by=1) # Multiply each number in the numbersList by 8 and print the result for (i in numbersList){ if (i==4){ temNumber = i * 8 print(temNumber) } } ## [1] 32 This is referred to as a nested loop, because there is a conditional statement within another one. Key takeaway here: in programming languages, = is an assignment (i.e. x = 4), whereas == is an equality test (i == 4). To put this loop in layman’s terms: for i in numbersList, if i is equal to 4, multiply i by 8 and then print temNumber. We can also have nested for loops. # Generate sequence of numbers from 1 to 3 this time using the seq() function (seq for sequence) numbersList = seq(from=1,to=3,by=1) lettersList = list(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) for (num in numbersList){ for (let in lettersList){ print(c(num,let)) } } ## [1] &quot;1&quot; &quot;A&quot; ## [1] &quot;1&quot; &quot;B&quot; ## [1] &quot;1&quot; &quot;C&quot; ## [1] &quot;2&quot; &quot;A&quot; ## [1] &quot;2&quot; &quot;B&quot; ## [1] &quot;2&quot; &quot;C&quot; ## [1] &quot;3&quot; &quot;A&quot; ## [1] &quot;3&quot; &quot;B&quot; ## [1] &quot;3&quot; &quot;C&quot; You can name the object within the list whatever you want (i, j, num, let, etc.). Also, c() is the concatenate function that combines values into a vector or list. The order doesn’t matter in this for loop… # Generate sequence of numbers from 1 to 3 this time using the seq() function (seq for sequence) numbersList = seq(from=1,to=3,by=1) lettersList = list(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) for (let in lettersList){ for (num in numbersList){ print(c(num,let)) } } ## [1] &quot;1&quot; &quot;A&quot; ## [1] &quot;2&quot; &quot;A&quot; ## [1] &quot;3&quot; &quot;A&quot; ## [1] &quot;1&quot; &quot;B&quot; ## [1] &quot;2&quot; &quot;B&quot; ## [1] &quot;3&quot; &quot;B&quot; ## [1] &quot;1&quot; &quot;C&quot; ## [1] &quot;2&quot; &quot;C&quot; ## [1] &quot;3&quot; &quot;C&quot; But it does in this one… # Generate sequence of numbers from 1 to 10 using the seq() function (seq for sequence) numbersList = seq(from=1,to=10,by=1) # Multiply each number in the numbersList by 8 and print the result if (i==4){ for (i in numbersList){ temNumber = i * 8 print(temNumber) } } Here’s one more example for multi conditional statement with an else… # Generate sequence of numbers from 1 to 3 this time using the seq() function (seq for sequence) numbersList = seq(from=1,to=3,by=1) lettersList = list(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) for (num in numbersList){ for (let in lettersList){ if (num == 3 &amp;&amp; let == &quot;B&quot;){ print(c(num,let)) } else{ print(&quot;Not what we want&quot;) } } } ## [1] &quot;Not what we want&quot; ## [1] &quot;Not what we want&quot; ## [1] &quot;Not what we want&quot; ## [1] &quot;Not what we want&quot; ## [1] &quot;Not what we want&quot; ## [1] &quot;Not what we want&quot; ## [1] &quot;Not what we want&quot; ## [1] &quot;3&quot; &quot;B&quot; ## [1] &quot;Not what we want&quot; &amp;&amp; means “and” … || means “or”…these are useful in multi conditional statements. The else statement is an appendage of the if statement. It basically means if num == 3 and let == B is false, print &quot;not what we want&quot;. Notice that the ‘else’ statement is outside of the if statement but immediately after it. In Class Exercise: debug this for loop soiltype &lt;- list(&quot;sand&quot;, &quot;silt&quot;, &quot;saline&quot;, &quot;clay&quot;, &quot;peat&quot;) permeability &lt;- c(0.09, 0.05, 0.03, 0.01, 0.005) for (s in seq(length(soiltype)){ print(a) for (p in permeability) if (p == 0.05 | p == 0.005){ print(permeability[s]) } } } And get this output… ## [1] 1 ## [1] 0.09 ## [1] 0.09 ## [1] 2 ## [1] 0.05 ## [1] 0.05 ## [1] 3 ## [1] 0.03 ## [1] 0.03 ## [1] 4 ## [1] 0.01 ## [1] 0.01 ## [1] 5 ## [1] 0.005 ## [1] 0.005 7.2 DEOS Data The dataset we’ll be using is named - DEOS.csv . It contains a variety of meteorological variables collected at 5 minute intervals. Let’s dive into the coded example. Note: When we load the file, your path might look something like this: Downloads/geog473-673/datasets/DEOS.csv Remember, we need to tell R where we stored the file we want to open. We can use getwd() to see where R is looking right now. From there, we can tell it to look in Downloads, or Documents, or Desktop, etc. # load the data using read.csv() deos_data &lt;- read.csv(&quot;../datasets/DEOS.csv&quot;, header = TRUE, skip=0, stringsAsFactors = FALSE) # view the top few rows of data using the head() function head(deos_data) ## Timestamp..UTC. Air.Temperature.deg..C. Dew.Point.Temperature.deg..C. ## 1 2014-04-04 04:05 7.1 4.3 ## 2 2014-04-04 04:10 6.9 4.3 ## 3 2014-04-04 04:15 6.8 4.3 ## 4 2014-04-04 04:20 6.7 4.4 ## 5 2014-04-04 04:25 6.6 4.4 ## 6 2014-04-04 04:30 6.6 4.4 ## Wind.Speed.m.sec. Wind.Direction.deg.. Barometric.Pressure.. ## 1 3.7 351.7 NA ## 2 3.5 352.9 NA ## 3 3.9 357.5 NA ## 4 3.9 356.8 NA ## 5 3.3 5.5 NA ## 6 2.7 18.4 NA ## Solar.Radiation.UNKNOWN. Wind.Gust.Speed..5..m.sec. ## 1 0 5.1 ## 2 0 5.8 ## 3 0 6.2 ## 4 0 5.9 ## 5 0 4.9 ## 6 0 3.5 ## Gage.Precipitation..5..mm. ## 1 0.00 ## 2 0.00 ## 3 0.25 ## 4 0.00 ## 5 0.00 ## 6 0.00 # view summary of the data summary(deos_data) ## Timestamp..UTC. Air.Temperature.deg..C. Dew.Point.Temperature.deg..C. ## Length:182645 Min. :-51.0 Min. :-53.300 ## Class :character 1st Qu.: 7.6 1st Qu.: 2.200 ## Mode :character Median : 15.2 Median : 10.200 ## Mean : 14.1 Mean : 8.505 ## 3rd Qu.: 21.2 3rd Qu.: 16.200 ## Max. : 37.0 Max. : 25.500 ## Wind.Speed.m.sec. Wind.Direction.deg.. Barometric.Pressure.. ## Min. : 0.100 Min. : 0.0 Mode:logical ## 1st Qu.: 1.800 1st Qu.: 99.3 NA&#39;s:182645 ## Median : 2.600 Median :202.2 ## Mean : 2.997 Mean :187.0 ## 3rd Qu.: 3.700 3rd Qu.:258.9 ## Max. :16.700 Max. :360.0 ## Solar.Radiation.UNKNOWN. Wind.Gust.Speed..5..m.sec. Gage.Precipitation..5..mm. ## Min. : 0.0 Min. : 0.100 Min. :0.00000 ## 1st Qu.: 0.0 1st Qu.: 2.900 1st Qu.:0.00000 ## Median : 4.0 Median : 4.400 Median :0.00000 ## Mean : 192.7 Mean : 4.907 Mean :0.01354 ## 3rd Qu.: 313.0 3rd Qu.: 6.400 3rd Qu.:0.00000 ## Max. :1335.0 Max. :24.400 Max. :9.65000 # view variable names colnames(deos_data) ## [1] &quot;Timestamp..UTC.&quot; &quot;Air.Temperature.deg..C.&quot; ## [3] &quot;Dew.Point.Temperature.deg..C.&quot; &quot;Wind.Speed.m.sec.&quot; ## [5] &quot;Wind.Direction.deg..&quot; &quot;Barometric.Pressure..&quot; ## [7] &quot;Solar.Radiation.UNKNOWN.&quot; &quot;Wind.Gust.Speed..5..m.sec.&quot; ## [9] &quot;Gage.Precipitation..5..mm.&quot; #the variable names look wonky because they have spaces in them - spaces in variable names is a big no-no #change the names to something more readable names(deos_data) = c(&quot;datetime&quot;, &quot;air_temperature&quot;, &quot;dewpoint&quot;, &quot;windspeed&quot;, &quot;winddirection&quot;, &quot;pressure&quot;, &quot;solar_radiation&quot;, &quot;wind_gust&quot;, &quot;precipitation&quot;) # print the first datetime value deos_data$datetime[1] ## [1] &quot;2014-04-04 04:05&quot; # view the class of the first date index class(deos_data$datetime) ## [1] &quot;character&quot; # give it a datetime class, notice the format deos_data$datetime = as.POSIXct(deos_data$datetime, format = &#39;%Y-%m-%d %H:%M&#39;) # subset the data using the which.min() and which.max() functions to find our indices low_ind = which.min(deos_data$datetime &lt; &quot;2014-04-04 00:00:00&quot;) upper_ind = which.max(deos_data$datetime &gt; &quot;2015-04-04 23:59&quot;) # plot the dataset by indexing it between teh low_ind and upper_ind plot(deos_data$datetime[low_ind:upper_ind], deos_data$air_temperature[low_ind:upper_ind], type = &quot;l&quot;, col = &#39;red&#39;, xlab = paste0(deos_data$datetime[low_ind], &quot; to &quot;, deos_data$datetime[upper_ind]), ylab = &quot;Celsius&quot;) # add a title to the plot title(&quot;1 Year Air Temperature at Station&quot;) #subset the data using the subset function this time subDeos = subset(deos_data, deos_data$datetime &gt;= &quot;2014-07-04 00:00:00&quot; &amp; deos_data$datetime &lt;= &quot;2014-07-11 23:59:00&quot;) # Now add dewpoint to that plot plot(x = subDeos$datetime, y = subDeos$air_temperature, type = &quot;l&quot;, col = &#39;red&#39;, xlab = paste0(subDeos$datetime[1], &quot; to &quot;, subDeos$datetime[length(subDeos$datetime)]), ylab = &quot;Celsius&quot;, ylim = c(5,40)) # add &quot;lines&quot; to the plot, in this case we want our line to be the dewpoint lines(x = subDeos$datetime, y = subDeos$dewpoint, col = &#39;yellow&#39;) legend(&#39;bottomright&#39;, legend=c(&#39;Air Temp&#39;, &#39;Dew Pt&#39;), col = c(&#39;red&#39;, &#39;yellow&#39;), lty = c(1, 1)) title(&quot;Air Temperature &amp; Dew Point Temperature&quot;) 7.3 Assignment Subset dataset to January 2015 only Convert Wind Speed &amp; Wind Gust data from m/s to mph Plot wind speed and wind gust on same plot - wind gust as points first, then wind speed as a line second Hint: you’ll have to use an argument of type = &quot;p&quot; for wind gust. Add legend Compute correlation coefficient between wind gust and wind speed using cor() function - add to title of plot Deliver Resulting Plot to Canvas assignment week3 Aim for something like this (does not have to be exact)… "],["raster-visualization.html", "8 Raster Visualization 8.1 Paste0() 8.2 Rasters 8.3 Assignment", " 8 Raster Visualization This week we’ll use a different dataset from the GOES-16 satellite to explore new coding practices. We’ll take what we have learned from the example and apply it to a global tree cover dataset. First, let’s cover an important skill - understanding the paste0() function. 8.1 Paste0() Last week we covered the for loop which is an essential programming skill. We also touched on the paste0() function, but this is incredibly useful and should be expanded upon. paste0() is the sister function of paste(), who’s cousin is sprintf() . It depends on preference, but most coders I know gravitate towards paste0(). Let’s check out all 3. # Paste Example 1 - default sep (aka separation) is space paste(&quot;file&quot;, &quot;number&quot;, &quot;32&quot;) ## [1] &quot;file number 32&quot; # Paste Example 2 - set sep to &quot;_&quot; paste(&quot;file&quot;, &quot;number&quot;, &quot;32&quot;, sep = &quot;_&quot;) ## [1] &quot;file_number_32&quot; # Paste0 Example 1 - 0 for 0 separating characters paste0(&quot;file&quot;, &quot;number&quot;, &quot;32&quot;) ## [1] &quot;filenumber32&quot; # sprintf example 1 sprintf(&quot;%s %s %s&quot;, &quot;file&quot;, &quot;number&quot;, &quot;32&quot;) ## [1] &quot;file number 32&quot; # sprintf example 2 person &lt;-&quot;Grover&quot; action &lt;-&quot;flying&quot; message(sprintf(&quot;On %s I realized %s was...\\n%s by the street&quot;, Sys.Date(), person, action)) ## On 2021-01-25 I realized Grover was... ## flying by the street # Notice that paste() is limiting because the separating character is not always present between # each string you&#39;re concatenating # Let&#39;s use paste0 here fileList &lt;- c(&#39;filename1&#39;, &#39;filename2&#39;, &#39;filename3&#39;, &#39;filename4&#39;) dateFolder &lt;- c(&#39;0813&#39;, &#39;0814&#39;, &#39;0815&#39;, &#39;0816&#39;) homeDir &lt;- &quot;~/Documents/&quot; pathList &lt;- list() for (i in 1:length(fileList)){ print(i) tempString &lt;- paste0(homeDir, dateFolder[i], &#39;/&#39;, fileList[i]) pathList[i] &lt;- tempString } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 pathList ## [[1]] ## [1] &quot;~/Documents/0813/filename1&quot; ## ## [[2]] ## [1] &quot;~/Documents/0814/filename2&quot; ## ## [[3]] ## [1] &quot;~/Documents/0815/filename3&quot; ## ## [[4]] ## [1] &quot;~/Documents/0816/filename4&quot; pathList[[1]] ## [1] &quot;~/Documents/0813/filename1&quot; # Now let&#39;s rewrite using sprintf fileList &lt;- c(&#39;filename1&#39;, &#39;filename2&#39;, &#39;filename3&#39;, &#39;filename4&#39;) dateFolder &lt;- c(&#39;0813&#39;, &#39;0814&#39;, &#39;0815&#39;, &#39;0816&#39;) homeDir &lt;- &quot;~/Documents/&quot; pathList &lt;- list() pathList &lt;- sprintf(&quot;%s%s/%s&quot;, homeDir, dateFolder, fileList) pathList ## [1] &quot;~/Documents/0813/filename1&quot; &quot;~/Documents/0814/filename2&quot; ## [3] &quot;~/Documents/0815/filename3&quot; &quot;~/Documents/0816/filename4&quot; pathList[[1]] ## [1] &quot;~/Documents/0813/filename1&quot; #sprintf is a very useful and quick function for combining various strings into longer strings #but paste0 allows you a little more freedom and is slightly more intuitive Debug and fix the following code: # Let&#39;s use paste0 here fileList &lt;- c(&#39;sstData20190912&#39;, &#39;sstData20190913&#39;, &#39;sstData20190914&#39;, &#39;sstData20190915&#39;) dateFolder &lt;- seq(0912, 0915, 1) homeDir &lt;- &quot;~/Documents/&quot; pathList &lt;- list() for (i in fileList){ print(i) tempString &lt;- paste0(homeDir, dateFolder[i], &#39;/&#39;, i) pathList[i] &lt;- tempString } pathList pathList[[1]] The final pathList[[1]] should look like this… &quot;~/Documents/0912/sstData20190912&quot; 8.2 Rasters Rasters offer us a 2 dimensional dataset (matrix) of data with geospatial coordinates. Rasters can come in a variety of filetypes including netCDF, GeoTIFF, CSV (if they come in CSV, they’ll be massive - this is an inefficient datatype), etc. We’ll be looking at the GOES-16 Rolling 1 Day average datafile - GOES_R_ROLLING_1DAY_20190814.nc. The GOES-16 Satellite records sea surface temperatures every hour. A rolling 1 day is an average of the previous 24 hours of record of sea surface temperature (and this dataset is created by CEMA here at UD). library(maptools) library(sp) library(ncdf4) library(raster) library(rasterVis) library(RColorBrewer) ncFile &lt;- nc_open(&quot;../datasets/GOES_R_ROLLING_1DAY_20190814.nc&quot;) sstGoes &lt;- ncvar_get(ncFile, varid = &quot;sst&quot;) sstRast &lt;- raster(sstGoes) # plot using quick &#39;image&#39; function from raster image(sstRast) # netcdf stores data backwards, so to convert it to raster we need to transpose it sstRast &lt;- t(sstRast) # plot using quick &#39;image&#39; function from raster image(sstRast) # now we see that the data is just upside down, so we use the flip() function sstRast &lt;- flip(sstRast, 2) # plot using quick &#39;image&#39; function from raster image(sstRast) sstRast ## class : RasterLayer ## dimensions : 1989, 2778, 5525442 (nrow, ncol, ncell) ## resolution : 0.0003599712, 0.0005027652 (x, y) ## extent : 0, 1, 0, 1 (xmin, xmax, ymin, ymax) ## crs : NA ## source : memory ## names : layer ## values : 5.344696, 34.9397 (min, max) # notice that the extent and crs information are wrong/not included so let&#39;s fix this lat = ncvar_get(ncFile, &quot;latitude&quot;) lon = ncvar_get(ncFile, &quot;longitude&quot;) # define new extents for the raster extent(sstRast) = c(min(lon), max(lon), min(lat), max(lat)) # define the proj4 projection string crs(sstRast) = &quot;+proj=longlat +datum=WGS84 +no_defs &quot; sstRast ## class : RasterLayer ## dimensions : 1989, 2778, 5525442 (nrow, ncol, ncell) ## resolution : 0.01799352, 0.0180909 (x, y) ## extent : -99.99015, -50.00415, 16.00283, 51.98563 (xmin, xmax, ymin, ymax) ## crs : +proj=longlat +datum=WGS84 +no_defs ## source : memory ## names : layer ## values : 5.344696, 34.9397 (min, max) image(sstRast) # levelplot the sstRast - levelplot is from rasterVis package levelplot(sstRast) # USA shapefiles via the getData function usa &lt;- getData(&#39;GADM&#39;, country = &#39;USA&#39;, level = 1) # Throw together the usa spatial polygons data frame plt &lt;- levelplot(sstRast, margin=F, par.settings=BuRdTheme, main=&quot;GOES-R Rolling SST 08/14&quot;) plt + layer(sp.polygons(usa, col=&#39;black&#39;,fill=&#39;grey&#39;, lwd=0.4)) # Now let&#39;s do the same with geotiff tifFile = &quot;../datasets/goesSST.tif&quot; sstRast = raster(tifFile) # Quick image using raster&#39;s image plot generator function image(sstRast) # Now let&#39;s load a base dataset of world maps that R already has stored data(wrld_simpl) plt &lt;- levelplot(sstRast, margin=F, par.settings=BuRdTheme, main=&quot;GOES-R Rolling SST 08/14&quot;) plt + layer(sp.lines(wrld_simpl, col=&#39;black&#39;, lwd=0.4)) # Let&#39;s do the same plot but with a custom shapefile # note that even though we only point the function to the &#39;.shp&#39; file, the &#39;.shx&#39; and &#39;.dbf&#39; need to be in the same folder for this to work. world.shp &lt;- rgdal::readOGR(&quot;../datasets/world_shpfiles/world.shp&quot;) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/Users/james/Documents/Github/geog473-673/datasets/world_shpfiles/world.shp&quot;, layer: &quot;world&quot; ## with 252 features ## It has 2 fields # add custom color theme using brewer.pal from the package RColorBrewer and setting the rasterTheme mapTheme &lt;- rasterTheme(region=brewer.pal(8,&quot;Reds&quot;)) plt &lt;- levelplot(sstRast, margin=F, par.settings=mapTheme, main=&quot;GOES-R Rolling SST 08/14&quot;) plt + layer(sp.lines(world.shp, col=&#39;gray&#39;, lwd=0.4)) 8.3 Assignment Download treecov.nc from the datasets folder Open tree cover % as a variable, remove bad values (i.e. percentages outside of 0 and 100). Plot tree cover variable using a green theme. Be sure to add coastlines via your choice of underlying dataset. Ensure correct latitude/longitudes are displayed. Add title. Submit resulting image to Canvas assignment 4 ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/Users/james/Documents/Github/geog473-673/datasets/world_shpfiles/world.shp&quot;, layer: &quot;world&quot; ## with 252 features ## It has 2 fields "],["reprojecting-writing-rasters.html", "9 Reprojecting &amp; Writing Rasters 9.1 Indexing Data 9.2 Resampling and Reprojecting 9.3 PNGs 9.4 Writing Rasters 9.5 Assignment", " 9 Reprojecting &amp; Writing Rasters This week work on handling raster datasets that have undesirable projections. We’ll reproject these datasets and then write them to a new datafile that we can use in the future. 9.1 Indexing Data matA=matrix(1:16,4,4) matA ## [,1] [,2] [,3] [,4] ## [1,] 1 5 9 13 ## [2,] 2 6 10 14 ## [3,] 3 7 11 15 ## [4,] 4 8 12 16 matA[2,3] ## [1] 10 matA[c(1,3),c(2,4)] ## [,1] [,2] ## [1,] 5 13 ## [2,] 7 15 matA[1:3,2:4] ## [,1] [,2] [,3] ## [1,] 5 9 13 ## [2,] 6 10 14 ## [3,] 7 11 15 matA[1:2,] ## [,1] [,2] [,3] [,4] ## [1,] 1 5 9 13 ## [2,] 2 6 10 14 matA[,1:2] ## [,1] [,2] ## [1,] 1 5 ## [2,] 2 6 ## [3,] 3 7 ## [4,] 4 8 matA[1,] ## [1] 1 5 9 13 dim(matA) ## [1] 4 4 ##In Class Exercise: Starting with this code… matA=matrix(1:16,4,4) Make this matrix…. ## [,1] [,2] [,3] [,4] ## [1,] 1 10 18 26 ## [2,] 47 47 47 47 ## [3,] 6 14 22 39 ## [4,] 8 16 24 32 9.2 Resampling and Reprojecting This week we’ll be working an example with globalTemClim1961-1990.nc. This is Global Temperature climatology from 1961 to 1990. We’ll look at resampling this raster to a different size (resolution). Next, we’ll reproject this dataset. Reprojection and resampling are a frequent task for spatial datasets because the Earth isn’t flat (despite what your distant relative on Facebook might think). Earth’s shape (oblate spheroid) presents challenging projection issues. # load in the packages library(raster) library(rasterVis) library(maptools) # also loads sp package # load in dataset directly via raster package, specify varname which is &#39;tem&#39; for &#39;temperature&#39; temClim = raster(&quot;../datasets/globalTemClim1961-1990.nc&quot;, varname = &#39;tem&#39;, band=1) temClim ## class : RasterLayer ## band : 1 (of 12 bands) ## dimensions : 36, 72, 2592 (nrow, ncol, ncell) ## resolution : 5, 5 (x, y) ## extent : -180, 180, -90, 90 (xmin, xmax, ymin, ymax) ## crs : +proj=longlat +datum=WGS84 +no_defs ## source : /Users/james/Documents/Github/geog473-673/datasets/globalTemClim1961-1990.nc ## names : CRU_Global_1961.1990_Mean_Monthly_Surface_Temperature_Climatology ## z-value : 1 ## zvar : tem # Create a new, blank raster that has a totally different sizing newRaster = raster(nrow = 180, ncol = 360) newRaster ## class : RasterLayer ## dimensions : 180, 360, 64800 (nrow, ncol, ncell) ## resolution : 1, 1 (x, y) ## extent : -180, 180, -90, 90 (xmin, xmax, ymin, ymax) ## crs : +proj=longlat +datum=WGS84 +no_defs #resample the temClim raster to the resizedRaster resTemClim = resample(x=temClim, y=newRaster, method=&#39;bilinear&#39;) # can be set to nearest neighbor using &#39;ngb&#39; method resTemClim ## class : RasterLayer ## dimensions : 180, 360, 64800 (nrow, ncol, ncell) ## resolution : 1, 1 (x, y) ## extent : -180, 180, -90, 90 (xmin, xmax, ymin, ymax) ## crs : +proj=longlat +datum=WGS84 +no_defs ## source : memory ## names : CRU_Global_1961.1990_Mean_Monthly_Surface_Temperature_Climatology ## values : -48.8, 32 (min, max) #define new projection as robinson via a proj4 string. Note that this can also be achieved # using EPSG codes with the following - &quot;+init=epsg:4326&quot; for longlat newproj &lt;- CRS(&quot;+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs&quot; ) newproj ## CRS arguments: ## +proj=robin +lon_0=0 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs # reproject the raster to the new projection projTemClim = projectRaster(resTemClim,crs=newproj) projTemClim ## class : RasterLayer ## dimensions : 171, 372, 63612 (nrow, ncol, ncell) ## resolution : 94500, 107000 (x, y) ## extent : -17570274, 17583726, -9136845, 9160155 (xmin, xmax, ymin, ymax) ## crs : +proj=robin +lon_0=0 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs ## source : memory ## names : CRU_Global_1961.1990_Mean_Monthly_Surface_Temperature_Climatology ## values : -48.15074, 31.77626 (min, max) data(wrld_simpl) plt &lt;- levelplot(resTemClim, margin=F, par.settings=BuRdTheme, main=&quot;January Global Average Temp 1961-1990&quot;) plt + layer(sp.lines(wrld_simpl, col=&#39;black&#39;, lwd=0.4)) # convert the wrld_simpl land polygons to the robinson projection wrld_simpl = spTransform(wrld_simpl, CRS(&quot;+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs&quot; )) plt &lt;- levelplot(projTemClim, margin=F, par.settings=BuRdTheme, main=&quot;January Global Average Temp 1961-1990&quot;) plt + layer(sp.lines(wrld_simpl, col=&#39;black&#39;, lwd=0.4)) 9.3 PNGs The png() function is a function that saves a plot to png. After we invoke the function and fill out the arguments, we need to execute the plot code between the png() function and dev.off(). dev.off() tells R that you’re done adding things to the plot and that it can be done plotting. png(filename = &quot;~/Downloads/myPNG.png&quot;, width = 10, height = 6, units = &#39;in&#39;,res=100) plt &lt;- levelplot(projTemClim, margin=F, par.settings=BuRdTheme, main=&quot;January Global Average Temp 1961-1990&quot;) plt + layer(sp.lines(wrld_simpl, col=&#39;black&#39;, lwd=0.4)) dev.off() 9.4 Writing Rasters writeRaster(x=projTemClim, filename=&quot;~/Downloads/projectedTemClim1961-1990.tif&quot;, format=&#39;GTiff&#39;, varname=&quot;Temperature&quot;, longname=&quot;Global Average Temperature January 1960-1990&quot;, xname=&quot;lon&quot;, yname=&quot;lat&quot;) You can save these rasters in a variety of formats. If you’re interested in looking them up, run help(writeRaster) and read about the format argument. 9.5 Assignment Load in globalTemClim1961-1990.nc Extract data for January and July Find difference between two months globally Enhance resolution 2x using nearest neighbor method (hint: run help(resample) if you get stuck) Plot in mollwide projection (“+proj=moll +lon_0=0 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs”) Write raster to NetCDF Upload PNG and netCDF file to Canvas under week 5 assignment "],["plot-customization.html", "10 Plot Customization 10.1 Expanding on Basic Plots in R 10.2 Barplots, Boxplots, Pie Charts 10.3 Tree Data 10.4 Assignment:", " 10 Plot Customization R has a plethora of plotting packages, tools, and techniques. Generally speaking, base R graphs aren’t as popular amongst advanced R users as others (e.g. ggplot). That being said, R base graphing offers a variety of styling techniques and unique customization. The base R plotting methods will offer a solid foundation before we learn more about the aforementioned ggplot. Boxplots, histograms, pie charts, bar charts, and scatter plots are readily available and customizable in the base R plotting package. We can customize and style colors, text, line types, plotting symbols, line thicknesses, symbol size, and much more. 10.1 Expanding on Basic Plots in R Variable Assignment &amp; Operations x = 7 y = 9 z &lt;- y * (x/2 + 7) + sqrt(y) print(x) ## [1] 7 y ## [1] 9 z ## [1] 97.5 X &lt;- 1:20 Y &lt;- (1 + 1/X) + 2 X ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Y ## [1] 4.000000 3.500000 3.333333 3.250000 3.200000 3.166667 3.142857 3.125000 ## [9] 3.111111 3.100000 3.090909 3.083333 3.076923 3.071429 3.066667 3.062500 ## [17] 3.058824 3.055556 3.052632 3.050000 plot(X,Y) Now let’s add in some extra arguments to the plot function to make it prettier plot(x=X,y=Y, type = &quot;b&quot;, pch=c(24), col = &#39;blue&#39;, bg=&#39;yellow&#39;, cex = 1.6, xlab = &quot;X&quot;, ylab = &quot;Y&quot;, col.lab = &quot;orange&quot;, lwd=1.4, lty=2, main = &quot;X vs Y&quot;) plot() Argument Definition type type of plot - p stands for points pch plotting ‘character’, i.e., symbol to use col color bg background color (only applicable for pch’s between 21:25) cex size of the points xlab x label ylab y label col.lab x and y label color lwd line width lty line type main title of the plot For more argument definitions, use the help() function or ? function like so - help(plot) or ?plot. Multiple options may pop up. This occurs when you have two functions from different packages with the same name. In this case, the plot() function we are using is from the base package. 10.1.1 Adding to a Plot We can add additional items to a particular plot. Some of these items might be… Function Definition legend adds a legend to a plot lines connects points sequentially with lines (added to a plot) points plots points (adds to a plot) segments add lines to a plot (between pairs of points) text add text to a plot legend add a legend to a plot abline add a line to a plot by specifying its slope and intercept title title can also be added outside of the function you use. This is necessary when we want an overarching title for multiple plots in one figure. Let’s use our previous example but go about plotting it a different way to show how it works. # plot our X and Y as a line plot plot(x=X,y=Y, type = &quot;l&quot;, col=&#39;blue&#39;, xlab = &quot;X&quot;, ylab = &quot;Y&quot;,col.lab = &quot;orange&quot;, lwd=1.4, lty=2, main = &quot;X vs Y&quot;) # add points to the line via the points function points(x=X, y=Y, pch=c(24), col = &#39;blue&#39;, bg=&#39;yellow&#39;, cex = 1.6) # add a vertical line at 3.4 abline(v=10) # add a legend legend(x = &quot;topright&quot;, legend=c(&quot;my points&quot;), pch=c(24), col = &#39;blue&#39;, pt.bg=&#39;yellow&#39;) Let’s add in another plot(1:25, 1:25, xlab=&quot;&quot;,ylab=&quot;&quot;,pch=1:25,col=1:25,cex=2) grid(lty=1, col=&quot;gray90&quot;) points(1:25, 1:25, xlab=&quot;&quot;,ylab=&quot;&quot;,pch=1:25,col=1:25,cex=2) title(&quot;Plotting symbol, line type, &amp; color codes&quot;) legend(&quot;topleft&quot;, legend=1:6, lty=1:6, lwd=1.5, ncol=2, bg=&quot;gray95&quot;) legend(&quot;bottomright&quot;, legend=1:8, col=1:8, ncol=3, pch=19, bg=&quot;gray95&quot;) 10.2 Barplots, Boxplots, Pie Charts We’re going to create a fake dataset of those who are infected by the coronavirus. Let’s do so using the sample() function. # Create a random sample of the 3 types - Susceptible, Infected and Recovered InfStatus &lt;- factor(sample(c(&quot;Susceptible&quot;, &quot;Infected&quot;, &quot;Recovered&quot;),size = 50, replace = TRUE)) I &lt;- table(InfStatus) I ## InfStatus ## Infected Recovered Susceptible ## 12 19 19 # Now let&#39;s make a random sample of 3 genotypes (RR, Rr, and rr) Genotype &lt;- factor(sample(c(&quot;RR&quot;, &quot;Rr&quot;, &quot;rr&quot;), size = 50, replace = TRUE)) G &lt;- table(Genotype) G ## Genotype ## rr Rr RR ## 18 11 21 #show genotype and infected status as a table table(Genotype, InfStatus) ## InfStatus ## Genotype Infected Recovered Susceptible ## rr 4 10 4 ## Rr 2 3 6 ## RR 6 6 9 Note - We turned the sample data info a factor to add levels (i.e. categories) of our test data. # WITHOUT the factor test &lt;- sample(c(&quot;Susceptible&quot;, &quot;Infected&quot;, &quot;Recovered&quot;),size = 50, replace = TRUE) test ## [1] &quot;Recovered&quot; &quot;Recovered&quot; &quot;Infected&quot; &quot;Recovered&quot; &quot;Susceptible&quot; ## [6] &quot;Infected&quot; &quot;Infected&quot; &quot;Infected&quot; &quot;Susceptible&quot; &quot;Recovered&quot; ## [11] &quot;Susceptible&quot; &quot;Infected&quot; &quot;Recovered&quot; &quot;Recovered&quot; &quot;Infected&quot; ## [16] &quot;Infected&quot; &quot;Infected&quot; &quot;Susceptible&quot; &quot;Recovered&quot; &quot;Infected&quot; ## [21] &quot;Infected&quot; &quot;Infected&quot; &quot;Recovered&quot; &quot;Infected&quot; &quot;Recovered&quot; ## [26] &quot;Susceptible&quot; &quot;Susceptible&quot; &quot;Recovered&quot; &quot;Susceptible&quot; &quot;Recovered&quot; ## [31] &quot;Recovered&quot; &quot;Recovered&quot; &quot;Recovered&quot; &quot;Recovered&quot; &quot;Recovered&quot; ## [36] &quot;Recovered&quot; &quot;Recovered&quot; &quot;Infected&quot; &quot;Recovered&quot; &quot;Recovered&quot; ## [41] &quot;Infected&quot; &quot;Infected&quot; &quot;Susceptible&quot; &quot;Susceptible&quot; &quot;Susceptible&quot; ## [46] &quot;Susceptible&quot; &quot;Susceptible&quot; &quot;Recovered&quot; &quot;Recovered&quot; &quot;Infected&quot; class(test) ## [1] &quot;character&quot; # Now as a factor test2 &lt;- factor(sample(c(&quot;Susceptible&quot;, &quot;Infected&quot;, &quot;Recovered&quot;),size = 50, replace = TRUE)) test2 ## [1] Infected Infected Recovered Susceptible Susceptible Infected ## [7] Susceptible Infected Susceptible Susceptible Infected Infected ## [13] Recovered Susceptible Recovered Recovered Infected Infected ## [19] Recovered Recovered Susceptible Susceptible Susceptible Susceptible ## [25] Recovered Infected Infected Infected Infected Recovered ## [31] Susceptible Susceptible Infected Susceptible Recovered Susceptible ## [37] Recovered Susceptible Infected Infected Recovered Susceptible ## [43] Recovered Susceptible Susceptible Infected Infected Recovered ## [49] Recovered Susceptible ## Levels: Infected Recovered Susceptible class(test2) ## [1] &quot;factor&quot; 10.2.1 Combining Plots R makes it easy to combine multiple plots into one overall graph, using either the par() function. With the par() function, you can include the option mfrow=c(nrows, ncols) to create a matrix of nrows x ncols plots that are filled in by row. The Additionally, we can control text size with cex - a number indicating the amount by which plotting text and symbols should be scaled relative to the default. 1=default, 1.5 is 50% larger, 0.5 is 50% smaller, etc. Here’s some more descriptions. option description mfrow mfrow=c(nrows, ncols) - nrows x ncols plots that are filled in by row mar A numerical vector of the form c(bottom, left, top, right) which gives the number of lines of margin to be specified on the four sides of the plot oma A vector of the form c(bottom, left, top, right) giving the size of the outer margins in lines of text. bg background color cex magnification of text and symbols relative to default. cex.axis magnification of axis annotation relative to cex cex.lab magnification of x and y labels relative to cex cex.main magnification of titles relative to cex cex.sub magnification of subtitles relative to cex We can always list the par settings by entering par() in the console. Let’s use these par settings with our test data created above. par(mfrow=c(2, 2), mar=c(3, 2, 2, 1), oma=c(0, 0, 3, 0), bg = &quot;white&quot;) ## create plot array of 2 row x 2 columns plot(InfStatus, ylim = c(0, 27)) # basic plot with y limit set as a range box() # adds a box around the plot coded before this line barplot(table(Genotype, InfStatus), ylim = c(0, 13), beside = TRUE, col = c(&quot;seagreen4&quot;, &quot;coral&quot;, &quot;dodgerblue2&quot;)) # barplot box() # adds a box around the plot coded before this line legend(&quot;topright&quot;, c(&quot;RR&quot;, &quot;Rr&quot;, &quot;rr&quot;), fill = c(&quot;seagreen4&quot;, &quot;coral&quot;, &quot;dodgerblue2&quot;), ncol = 1, cex = 0.75) # legend for the previous plot, which in this case is the barplot boxplot(rnorm(50, mean = 15, sd = 3) ~ Genotype, col = c(&quot;seagreen4&quot;, &quot;coral&quot;, &quot;dodgerblue2&quot;)) # boxplot pie(G, col = c(&quot;seagreen4&quot;, &quot;coral&quot;, &quot;dodgerblue2&quot;)) # pie plot mtext(&quot;Basic R Plots with Test Genotype Data&quot;, outer = TRUE, cex = 1.5, font = 2) # main title 10.3 Tree Data We’re going to download the TreeData.csv file from the course github using a direct method. We can use the download.file() function to do this. download.file(url = &quot;https://github.com/jsimkins2/geog473-673/tree/master/datasets/TreeData.csv&quot;, destfile = &quot;/Users/james/Downloads/TreeData.csv&quot; , mode='wb') The url argument is the direct url of the file we wish to download. destfile is the destination file path + name - note that this is my relative path and that yours will look different. mode describes the method with whcih to write the file (wb can be used in most cases). 10.3.1 Loading the Dataset # Our file location is our destfile argument above. For me, it&#39;s /Users/james/Downloads/TreeData.csv treedat &lt;- read.csv(&quot;/Users/james/Documents/Github/geog473-673/datasets/TreeData.csv&quot;) treedat ## tree spp season Infected dbh SapDepth BarkThick NobarkArea Heartwood ## 1 1 PICO Summer Yes 42.2 8.4 0.3 1359.2 483.1 ## 2 2 ABLA Fall Yes 13.4 2.0 0.4 124.7 58.1 ## 3 3 ABLA Summer No 13.1 1.5 0.3 122.7 70.9 ## 4 4 PICO Spring No 15.0 4.1 0.2 167.4 32.2 ## 5 5 POTR Winter Yes 14.2 3.6 0.0 158.4 38.5 ## 6 6 POTR Winter Yes 20.0 5.9 0.0 314.2 52.8 ## 7 7 ABLA Summer Yes 9.1 2.1 0.3 56.7 14.5 ## 8 8 ABLA Spring No 9.2 2.5 0.5 52.8 8.0 ## 9 9 ABLA Fall Yes 27.3 3.0 0.6 535.0 317.3 ## 10 10 PICO Fall No 11.2 3.4 0.3 88.2 11.3 ## 11 11 PICO Spring Yes 18.0 5.9 0.3 237.8 24.6 ## 12 12 POTR Summer No 9.0 3.0 0.0 63.6 7.1 ## 13 13 POTR Spring No 15.4 4.1 0.0 186.3 40.7 ## 14 14 POTR Winter No 24.1 6.2 0.0 456.2 107.5 ## 15 15 PICO Winter Yes 24.2 7.0 0.5 422.7 66.5 ## 16 16 PIFL Fall No 14.4 4.2 0.5 141.0 19.6 ## 17 17 PIFL Winter No 13.1 2.2 0.6 111.2 44.2 ## 18 18 PIFL Summer Yes 21.5 5.1 0.6 323.7 80.1 ## 19 19 PIFL Spring No 13.4 2.2 0.5 120.8 50.3 ## 20 20 PIFL Fall Yes 16.2 3.5 0.5 181.5 52.8 ## SapArea ## 1 876.1 ## 2 66.6 ## 3 51.8 ## 4 135.2 ## 5 119.9 ## 6 261.3 ## 7 42.2 ## 8 44.8 ## 9 217.7 ## 10 76.9 ## 11 213.2 ## 12 56.5 ## 13 145.5 ## 14 348.7 ## 15 356.3 ## 16 121.4 ## 17 67.0 ## 18 243.5 ## 19 70.5 ## 20 128.6 treedat is a data frame. As a reminder, a data frame is essentially a 2-dimensional array that contains a combination of vectors (columns of data) that are of the class; integer, numeric, character. This is different from a matrix which can only contain 1 type of data. In this case, we have some tree data that includes species of tree, season the data was collected, diameter of the tree, bark thickness, area of no bark, heartwood diameter, and sapwood diameter. Let’s manipulate this dataframe to make it easier to work wtih. # let&#39;s set the rownames equal to the tree column treedat &lt;- read.csv(&quot;/Users/james/Documents/Github/geog473-673/datasets/TreeData.csv&quot;,row.names=&#39;tree&#39;) # print treedat treedat ## spp season Infected dbh SapDepth BarkThick NobarkArea Heartwood SapArea ## 1 PICO Summer Yes 42.2 8.4 0.3 1359.2 483.1 876.1 ## 2 ABLA Fall Yes 13.4 2.0 0.4 124.7 58.1 66.6 ## 3 ABLA Summer No 13.1 1.5 0.3 122.7 70.9 51.8 ## 4 PICO Spring No 15.0 4.1 0.2 167.4 32.2 135.2 ## 5 POTR Winter Yes 14.2 3.6 0.0 158.4 38.5 119.9 ## 6 POTR Winter Yes 20.0 5.9 0.0 314.2 52.8 261.3 ## 7 ABLA Summer Yes 9.1 2.1 0.3 56.7 14.5 42.2 ## 8 ABLA Spring No 9.2 2.5 0.5 52.8 8.0 44.8 ## 9 ABLA Fall Yes 27.3 3.0 0.6 535.0 317.3 217.7 ## 10 PICO Fall No 11.2 3.4 0.3 88.2 11.3 76.9 ## 11 PICO Spring Yes 18.0 5.9 0.3 237.8 24.6 213.2 ## 12 POTR Summer No 9.0 3.0 0.0 63.6 7.1 56.5 ## 13 POTR Spring No 15.4 4.1 0.0 186.3 40.7 145.5 ## 14 POTR Winter No 24.1 6.2 0.0 456.2 107.5 348.7 ## 15 PICO Winter Yes 24.2 7.0 0.5 422.7 66.5 356.3 ## 16 PIFL Fall No 14.4 4.2 0.5 141.0 19.6 121.4 ## 17 PIFL Winter No 13.1 2.2 0.6 111.2 44.2 67.0 ## 18 PIFL Summer Yes 21.5 5.1 0.6 323.7 80.1 243.5 ## 19 PIFL Spring No 13.4 2.2 0.5 120.8 50.3 70.5 ## 20 PIFL Fall Yes 16.2 3.5 0.5 181.5 52.8 128.6 # look at the data frame variable names names(treedat) ## [1] &quot;spp&quot; &quot;season&quot; &quot;Infected&quot; &quot;dbh&quot; &quot;SapDepth&quot; ## [6] &quot;BarkThick&quot; &quot;NobarkArea&quot; &quot;Heartwood&quot; &quot;SapArea&quot; # print the dbh variable treedat$dbh ## [1] 42.2 13.4 13.1 15.0 14.2 20.0 9.1 9.2 27.3 11.2 18.0 9.0 15.4 24.1 24.2 ## [16] 14.4 13.1 21.5 13.4 16.2 # rename the dbh variable, but first let&#39;s be sure our index of the dbh variable is correct colnames(treedat)[4] ## [1] &quot;dbh&quot; # yep, dbh is the index number 4 colnames(treedat)[4] = &quot;tree.diameter&quot; treedat ## spp season Infected tree.diameter SapDepth BarkThick NobarkArea Heartwood ## 1 PICO Summer Yes 42.2 8.4 0.3 1359.2 483.1 ## 2 ABLA Fall Yes 13.4 2.0 0.4 124.7 58.1 ## 3 ABLA Summer No 13.1 1.5 0.3 122.7 70.9 ## 4 PICO Spring No 15.0 4.1 0.2 167.4 32.2 ## 5 POTR Winter Yes 14.2 3.6 0.0 158.4 38.5 ## 6 POTR Winter Yes 20.0 5.9 0.0 314.2 52.8 ## 7 ABLA Summer Yes 9.1 2.1 0.3 56.7 14.5 ## 8 ABLA Spring No 9.2 2.5 0.5 52.8 8.0 ## 9 ABLA Fall Yes 27.3 3.0 0.6 535.0 317.3 ## 10 PICO Fall No 11.2 3.4 0.3 88.2 11.3 ## 11 PICO Spring Yes 18.0 5.9 0.3 237.8 24.6 ## 12 POTR Summer No 9.0 3.0 0.0 63.6 7.1 ## 13 POTR Spring No 15.4 4.1 0.0 186.3 40.7 ## 14 POTR Winter No 24.1 6.2 0.0 456.2 107.5 ## 15 PICO Winter Yes 24.2 7.0 0.5 422.7 66.5 ## 16 PIFL Fall No 14.4 4.2 0.5 141.0 19.6 ## 17 PIFL Winter No 13.1 2.2 0.6 111.2 44.2 ## 18 PIFL Summer Yes 21.5 5.1 0.6 323.7 80.1 ## 19 PIFL Spring No 13.4 2.2 0.5 120.8 50.3 ## 20 PIFL Fall Yes 16.2 3.5 0.5 181.5 52.8 ## SapArea ## 1 876.1 ## 2 66.6 ## 3 51.8 ## 4 135.2 ## 5 119.9 ## 6 261.3 ## 7 42.2 ## 8 44.8 ## 9 217.7 ## 10 76.9 ## 11 213.2 ## 12 56.5 ## 13 145.5 ## 14 348.7 ## 15 356.3 ## 16 121.4 ## 17 67.0 ## 18 243.5 ## 19 70.5 ## 20 128.6 Now that our data is curated, let’s create a figure with 2 plots in the window; plot 1 will be a histogram of bark thickness and plot 2 will be a boxplot of sapdepth by species. # Now let&#39;s do some plotting par(mfrow=c(1,2)) ## create plot array of 1 row x 2 columns par(cex.axis=0.8) ## shrinks the name size of the x axes. If we don&#39;t do this, not all the names in the boxplot show up par(cex.main=0.7) ## shrinks the name size of the titles. If we don&#39;t do this, the titles don&#39;t fit in the window # Use the his() function to plot a histogram hist(treedat$BarkThick, xlab= &quot;Bark Thickness (cm)&quot;, main= &quot;Histogram: Bark Thickness&quot;, col= &quot;darkgreen&quot;) boxplot(SapDepth ~ spp, data= treedat, ylab= &quot;SapDepth&quot;, col= &quot;darkslateblue&quot;, main= &quot;Boxplot: Sapwood Depth by Species&quot;) First we notice that with the par() function, we can declare arguments in different lines so long as we call the par() function again. hist() is the histogram function; boxplot() is the boxplot function. The boxplot() function using a slightly different syntax for plotting in the form of y ~ x, or y versus x. hist(), on the other hand, can only plot numerical values. For categorical data, such as our tree species column, must be plotted with barplot() and the table() function. # print the table function output of treedat$spp - notice the categorical help from the table function table(treedat$spp) ## ## ABLA PICO PIFL POTR ## 5 5 5 5 # plot this categorical data using barplot() function barplot(table(treedat$spp)) # customize the barplot function barplot(table(treedat$spp), main=&quot;SPP Count Barplot&quot;, xlab=&quot;SPP&quot;, ylab=&quot;Count&quot;, border=&quot;red&quot;, col=&quot;blue&quot;, density=10) 10.4 Assignment: Create the plot below using a sequence of X values where Y is the log of the X values. For the line type, use type = &quot;b&quot; to obtain lines and points. Use a pch of 21, cex of 1.5, and colors of your choosing. Using the TreeData.csv from the course dataset folder, complete the following: Rename spp variable to species Make a 3 column plot consisting of a Sapwood Depth histogram, a boxplot of Bark Thickness by species, and a seasonal count barplot. Use your own colors, borders, line types, etc. Your plot does not have to perfectly match the example below, but try and get close to it. Submit the plot to UD Canvas. Your final plot should look like this "],["basic-statistics.html", "11 Basic Statistics 11.1 Statistical Function Dictionary 11.2 Basic Statistics in R 11.3 Assignment", " 11 Basic Statistics This week we’ll be diving into some basic statistical procedures. R started out as THE statistical programming language. It spread like wildfire for it’s performance and efficiency at crunching numbers. It’s early success as a statistical programming langugae attracted the early developers who really made R into the do-it-all language we’re using today. First, I’ll throw in the useful definitions list at the top of this tutorial just for ease of access. Second, we’ll dive into a recap of for loops and how they’re structured. Then we’ll show off some of R’s great statistics functions and how to add them to a plot. Finally, we’ll have an assignment where we cover some of 11.1 Statistical Function Dictionary Function Description length returns length of a vector sum returns the sum mean returns the mean median returns the median sd returns the standard deviation (n − 1 in denominator) min returns minimum max returns maximum sort sort a vector (rearranges the vector in order) order returns indices of vectors that will order them rank returns rank of each element in vector lm multiple linear regression glm generalized linear regression anova analysis of variance chisq.test Pearson’s Chi-squared test for count data summary shows results of various model fitting functions predict predicted results from model passing an lm object will result in adding the predicted line to the plot sample produces a random sample of the specified values set.seed sets seed for next random sample (repeat random sample) rnorm produces a random sample from a normal distribution qnorm quantiles (percentiles) of normal distribution pnorm CDF of normal distribution dnorm PDF of normal distribution rbinom produces a random sample from a binomial distribution fitdistrplus package that helps find the fit of univariat eparametric distributions plotdist empirical distribution plotting function descdist Computes descriptive parameters of an empirical distribution and provides a skewness-kurtosis plot fitdist Fits various distributions to data denscomp plots the histogram against fitted density functions cdfcomp lots the empirical cumulative distribution against fitted distribution functions qqcomp plots theoretical quantiles against empirical ones ppcomp plots theoretical probabilities against empirical ones 11.2 Basic Statistics in R R is famous for it’s easy-to-use statistics features and once you get the hang of it you’ll never want to touch Microsoft Excel again. Despite the fact that these stats functions seem pretty fancy to me, I call them basic functions because R has so much to offer to statisticians. In this tutorial, we examine meteorological observations that were recorded in Willow Creek Wisconsin, USA. There are 3 years of Willow Creek data - WCr_1hr.2010.nc, WCr_1hr.2011.nc, WCr_1hr.2012.nc. Let’s start with the 2010 dataset. 11.2.1 Load Willow Creek data library(ncdf4) # open the netcdf file of Willow Creek, Wisconsin meteorology data nc_file = nc_open(&quot;/Users/james/Documents/Github/geog473-673/datasets/WCr_1hr.2010.nc&quot;) # what does the nc file look like nc_file ## File /Users/james/Documents/Github/geog473-673/datasets/WCr_1hr.2010.nc (NC_FORMAT_CLASSIC): ## ## 10 variables (excluding dimension variables): ## float air_temperature[latitude,longitude,time] ## units: Kelvin ## _FillValue: -999 ## float air_temperature_max[latitude,longitude,time] ## units: Kelvin ## _FillValue: -999 ## float air_temperature_min[latitude,longitude,time] ## units: Kelvin ## _FillValue: -999 ## float surface_downwelling_longwave_flux_in_air[latitude,longitude,time] ## units: W/m2 ## _FillValue: -999 ## float air_pressure[latitude,longitude,time] ## units: Pascal ## _FillValue: -999 ## float surface_downwelling_shortwave_flux_in_air[latitude,longitude,time] ## units: W/m2 ## _FillValue: -999 ## float eastward_wind[latitude,longitude,time] ## units: m/s ## _FillValue: -999 ## float northward_wind[latitude,longitude,time] ## units: m/s ## _FillValue: -999 ## float specific_humidity[latitude,longitude,time] ## units: g/g ## _FillValue: -999 ## float precipitation_flux[latitude,longitude,time] ## units: kg/m2/s ## _FillValue: -999 ## ## 3 dimensions: ## latitude Size:1 ## units: degree_north ## long_name: latitude ## longitude Size:1 ## units: degree_east ## long_name: longitude ## time Size:8760 *** is unlimited *** ## units: sec ## long_name: time # ok, still a lot of info...let&#39;s list the names of the variables names(nc_file$var) ## [1] &quot;air_temperature&quot; ## [2] &quot;air_temperature_max&quot; ## [3] &quot;air_temperature_min&quot; ## [4] &quot;surface_downwelling_longwave_flux_in_air&quot; ## [5] &quot;air_pressure&quot; ## [6] &quot;surface_downwelling_shortwave_flux_in_air&quot; ## [7] &quot;eastward_wind&quot; ## [8] &quot;northward_wind&quot; ## [9] &quot;specific_humidity&quot; ## [10] &quot;precipitation_flux&quot; # alright, now we have some names, so let&#39;s put the variables into a new dataframe separate from the nc_file var_names = names(nc_file$var) willow_creek_2010 = list() dim &lt;- nc_file$dim for (v in seq_along(var_names)){ willow_creek_2010[[v]] = ncvar_get(nc_file, varid = var_names[v]) } # convert the list into a dataframe wcreek_df = data.frame(willow_creek_2010) # tell the dataframe what the column names are colnames(wcreek_df) = var_names # let&#39;s rename the variables to make them shorter - note that these short names MUST be in the same order as the longer names short_names = c(&quot;tair&quot;, &quot;tmax&quot;, &quot;tmin&quot;, &quot;lwave&quot;, &quot;pres&quot;, &quot;swave&quot;, &quot;ewind&quot;, &quot;nwind&quot;, &quot;shum&quot;, &quot;prec&quot;) # rename the column names to our new short name vector colnames(wcreek_df) = short_names # print the first few lines head(wcreek_df) ## tair tmax tmin lwave pres swave ewind nwind ## 1 260.1135 260.137 260.090 237.6100 95704.5 0 1.522976 -1.462524 ## 2 260.0315 260.044 260.019 241.6900 95735.5 0 1.624952 -1.563508 ## 3 259.9805 259.994 259.967 240.3500 95765.5 0 1.696362 -1.553177 ## 4 259.9005 259.941 259.860 240.9325 95795.5 0 1.493329 -1.702995 ## 5 259.7585 259.780 259.737 247.3875 95824.5 0 1.921457 -1.460475 ## 6 259.6825 259.693 259.672 246.5075 95853.0 0 1.963118 -1.667144 ## shum prec ## 1 0.001176792 0 ## 2 0.001170627 0 ## 3 0.001167514 0 ## 4 0.001160727 0 ## 5 0.001142952 0 ## 6 0.001133232 0 Our data has been read in and the variables have been renamed for our convenience. A major note here is that the replacement names must be in the same order as the variable names. Now, we turn our attnetion to the time component of this data. This is a time series dataset of weather variables so we’ll need a time variable to keep us organized. Here’s how we can create one… ### TIME ### # what are the units of time dim$time$units ## [1] &quot;sec&quot; # how are the time values spaced dim$time$vals[1:10] ## [1] 3600 7200 10800 14400 18000 21600 25200 28800 32400 36000 # we can back out that this data is hourly data just from knowing the units are seconds and time between # each recorded value is 3600 (3600 seconds == 1 hour) #### Add a datetime column #### date.seq = seq(as.POSIXct(&quot;2010-01-01 00:00:00&quot;), as.POSIXct(&quot;2010-12-31 23:00:00&quot;), by=&quot;hour&quot;) wcreek_df[&#39;datetime&#39;] = date.seq wcreek_df$datetime[1:10] ## [1] &quot;2010-01-01 00:00:00 EST&quot; &quot;2010-01-01 01:00:00 EST&quot; ## [3] &quot;2010-01-01 02:00:00 EST&quot; &quot;2010-01-01 03:00:00 EST&quot; ## [5] &quot;2010-01-01 04:00:00 EST&quot; &quot;2010-01-01 05:00:00 EST&quot; ## [7] &quot;2010-01-01 06:00:00 EST&quot; &quot;2010-01-01 07:00:00 EST&quot; ## [9] &quot;2010-01-01 08:00:00 EST&quot; &quot;2010-01-01 09:00:00 EST&quot; The data is loaded, organized, and ready for statistical analysis. 11.2.2 Subsetting Data In certain cases, our dataset may be too large. For example, our dataset contains 1 year of data. How can we subset our dataset from January 1 to June 30? We can use the which function to find out which row a particular datetime is located. In other words, the which function returns the dates index position. In order for this to work, the datetime format we use must match the datetime format of the dataset. In our case, the datetime is YYYY-mm-dd, so we need to search with that format. After we have our indices, we can subset the wcreek_df dataset using wcreek_df[rows,columns] subsetting rules. # use YYYY-mm-dd to return which datettime index has a value of 2010-01-01 start_ind = which(wcreek_df$datetime == &quot;2010-01-01&quot;) # use YYYY-mm-dd to return which datettime index has a value of 2010-06-30 end_ind = which(wcreek_df$datetime == &quot;2010-06-30&quot;) # Index our wcree_df dataframe using the indices gathered above. We want all columns so column section stays blank jan2jun = wcreek_df[start_ind:end_ind,] Note that there is also a subset function which is handy, but unreliable. Sometimes this function produces unintended consequences so the above method is the preferred for subsetting data. 11.2.3 Trends and Distributions Our dataset has been subset from January 1 to June 30. In the Northern Hemisphere, we would expect air temperature to increase from January to June. Let’s examine this and fit a trendline to the data. # air temperature ~ datetime - remember, the tilde (~) here can be thought of as &#39;versus&#39; or &#39;against&#39;. # So this plot is tair versus datetime plot(tair ~ datetime, data = jan2jun, pch = 20, col=&quot;blue&quot;) # use the lm function to fit a trendline to the data fit &lt;- lm(tair ~ datetime, data = jan2jun) summary(fit) ## ## Call: ## lm(formula = tair ~ datetime, data = jan2jun) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.7070 -3.6385 -0.2478 3.7455 19.0279 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.501e+03 2.446e+01 -102.3 &lt;2e-16 *** ## datetime 2.188e-06 1.926e-08 113.6 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.683 on 4318 degrees of freedom ## Multiple R-squared: 0.7493, Adjusted R-squared: 0.7493 ## F-statistic: 1.291e+04 on 1 and 4318 DF, p-value: &lt; 2.2e-16 coef(fit) ## (Intercept) datetime ## -2.501181e+03 2.187997e-06 plot(tair ~ datetime, data = jan2jun, main= &quot;tair vs. datetime&quot;, ylab= &quot;Air Temperature ( Kelvin )&quot;, xlab= &quot;Datetime&quot;, pch= 19, col= &#39;blue&#39;) grid(NA,NULL, lty= 4) # NA first for no y axis grid lines, null second to ignore the default x axis linetype abline(fit, col= &#39;black&#39;, lty= 4, lwd= 2) legend(&quot;topleft&quot;, legend=c(&quot;lm(fit)&quot;), col= 2, lty= 4, bg= &quot;gray85&quot;, box.lty=0) Just as we suspected, the temperature increases from January to June. So, we have an idea of the trend now, but what about the frequency of recorded temperatures? Are some temperatures more likely to occur than others in this dataset? Histograms are a good way to check the distribution and answer these questions. Then, we’ll overlay the expected probability distribution function given the standard deviation and mean values of the jan2jun dataset using the dnorm function. Let’s add this to a histogram of the data. # plot a histogram of jan2jun air temperature and add a normal distribution over it. hist(jan2jun$tair, freq = FALSE, main= &quot;Willow Creek Air Temp - Jan 1 to June 30, 2010&quot;, xlab = &quot;Temperature (Kelvin)&quot;) # create a sequence covering the x axis of our histogram temp_bins &lt;- seq(240, 310, length.out=100) # calculate the normal PDF of tair y &lt;- dnorm(x=temp_bins, mean=mean(jan2jun$tair), sd=sd(jan2jun$tair)) # add lines of the normal dist to the histogram lines(temp_bins, y, col = &quot;red&quot;, lwd=2, lty=2) Generally speaking, a normal distribution does a reasonable job capturing the actual data points. We see that there is a positive trend in the dataset and that the normal PDF underestimates warmer temperatures. Density plots are another plotting method that can show the distribution of data. Density plots can be thought of as plots of smoothed histograms. The smoothness is controlled by a bandwidth parameter that is analogous to the histogram binwidth. Let’s create a density plot the jan2jun air temperature. # calculate the density d = density(jan2jun$tair, bw = 0.5) # plot the density of tair plot(d, xlab = &quot;Air Temperature (Kelvin)&quot;, ylab = &quot;Density&quot;, main=&quot;Air Temperature Density&quot;, col=&quot;black&quot;, lwd=2) # fill in the space below the density distribution instead of leaving it white polygon(d, col=&quot;coral&quot;, border=&quot;blue&quot;) 11.2.4 Finding a Best-Fit Distribution The fitdistrplus package is excellent for performing statistical analysis tests and fitting distributions. We can use this package to tell us which distribution fits a particular set of data best. One useful function to perform quick histograms with empirical fits and cumulative distributions is the plotdist function. library(fitdistrplus) ## Loading required package: MASS ## ## Attaching package: &#39;MASS&#39; ## The following objects are masked from &#39;package:raster&#39;: ## ## area, select ## Loading required package: survival plotdist(jan2jun$tair, histo = TRUE, demp = TRUE) In the previous section, we fit our histogram of Air Temperature with a normal distribution. Was this a good selection? We can use the descdist function to plot a Cullen and Frey Graph. This graph provides analysis for which distribution (normal, uniform, exponential, etc.) best fits our data. Basically, we just look to see which theoretical distribution is closest to our observation point. descdist(jan2jun$tair) ## summary statistics ## ------ ## min: 248.4165 max: 304.0025 ## median: 278.452 ## mean: 277.785 ## estimated sd: 11.34891 ## estimated skewness: -0.176518 ## estimated kurtosis: 2.190581 Uniform, normal, and gamma distributions are the 3 closest to our air temperature data. Let’s fit each of these distributions to the empirical data and create a density plot (denscomp), a cumulative density function plot (cdfcomp), a Q-Q plot (qqcomp), and a P-P plot (ppcomp) for these distributions. fit_u &lt;- fitdist(jan2jun$tair, &quot;unif&quot;) fit_n &lt;- fitdist(jan2jun$tair, &quot;norm&quot;) fit_g &lt;- fitdist(jan2jun$tair, &quot;gamma&quot;) summary(fit_g) ## Fitting of the distribution &#39; gamma &#39; by maximum likelihood ## Parameters : ## estimate Std. Error ## shape 595.374330 12.80305602 ## rate 2.143291 0.04610913 ## Loglikelihood: -16634.45 AIC: 33272.91 BIC: 33285.65 ## Correlation matrix: ## shape rate ## shape 1.0000000 0.9995799 ## rate 0.9995799 1.0000000 par(mfrow=c(2,2)) plot.legend &lt;- c(&quot;uniform&quot;, &quot;normal&quot;, &quot;gamma&quot;) denscomp(list(fit_u, fit_n, fit_g), legendtext = plot.legend) cdfcomp (list(fit_u, fit_n, fit_g), legendtext = plot.legend) qqcomp (list(fit_u, fit_n, fit_g), legendtext = plot.legend) ppcomp (list(fit_u, fit_n, fit_g), legendtext = plot.legend) Some of the statistics shown in this section are beyond the scope of this course but this package is important to know of. 11.2.5 Correlation Plots Correlation plots show correlation coefficients across variables. For example, we expect shortwave radiation and temperature to have high a high correlation coefficient because generally speaking when the a lot of sunlight is received at the surface, temperature increases. There’s a handy package called corrplot that caluclates correlation coefficients quickly and intuitively. If you don’t have the package installed, you’ll need to use - install.packages(&quot;corrplot&quot;). For the correlation plot, we’re going to use the full year dataset, wcreek_df, rather than the jan2jun dataset. When we calculate correlations, the more data we have the better. We’re also going to calculate residual values - that is the distance between actual data and the trendline. It’s another way to express error. When we have a large dataset like this with many scatter points, it’s difficult to nail down a specific trendline that may capture the dataset the best. Different trendlines may represent the data in similar ways. Residuals measure the diffence between a particular trendline and the data point. library(corrplot) ## corrplot 0.84 loaded # create a duplicate of the wcreek_df - remove the datetime variable from this wcreek_nodate = wcreek_df # we need to remove the datetime so our dataset is full of numeric values only - no datetime values or characters wcreek_nodate$datetime = NULL # awesome, now let&#39;s calculate the correlation coefficients cor_wcreek = cor(wcreek_nodate) head(cor_wcreek) ## tair tmax tmin lwave pres swave ## tair 1.0000000 0.9998053 0.9998039 0.8200776 -0.16229303 0.35668470 ## tmax 0.9998053 1.0000000 0.9992183 0.8177929 -0.16085314 0.35552272 ## tmin 0.9998039 0.9992183 1.0000000 0.8220495 -0.16367456 0.35771119 ## lwave 0.8200776 0.8177929 0.8220495 1.0000000 -0.29557674 0.27059006 ## pres -0.1622930 -0.1608531 -0.1636746 -0.2955767 1.00000000 0.02962979 ## swave 0.3566847 0.3555227 0.3577112 0.2705901 0.02962979 1.00000000 ## ewind nwind shum prec ## tair -0.35196115 0.07174053 0.9064799 0.08391199 ## tmax -0.35195936 0.07146779 0.9056241 0.08342905 ## tmin -0.35182514 0.07198617 0.9069838 0.08436382 ## lwave -0.29876208 0.09322482 0.8416941 0.14016558 ## pres 0.22972978 0.03904779 -0.1628097 -0.09972888 ## swave -0.07181433 -0.11448984 0.2681873 -0.02659980 # now let&#39;s calculate the residuals of the correlations with a 95% confidence interval residuals_1 &lt;- cor.mtest(wcreek_nodate, conf.level = .95) # now let&#39;s plot this up. corrplot(cor_wcreek, p.mat = residuals_1$p, method = &#39;color&#39;, number.cex = .7, type = &#39;lower&#39;, addCoef.col = &quot;black&quot;, # Add coefficient of correlation tl.col = &quot;black&quot;, tl.srt = 90, # Text label color and rotation # Combine with significance sig.level = 0.05, insig = &quot;blank&quot;) Do these correlations make sense? Let’s take two variables that are highly correlated - temperature and shortwave radiation (sunlight). Our correalation is .36 for these two variables - this is a slight correlation. Why isn’t the correlation higher? Advection - that is air being transported from other locations via wind. 11.3 Assignment Using the WCr_1hr.2012.nc dataset found in the datasets folder, create a document (word, pdf, notepad, etc.) answering these questions with accompanied figures (no code). Subset the data between July 1 and Dec 31. Create a scatterplot with a trendline of Shortwave Radiation similar to the air temperature example above. Does this trendline make sense? Create a filled density plot of Shortwave Radiation similar to the air temperature example above. Note, you will need to change your binwidth to fit the shortwave dataset. Does this density plot make sense? Why or why not? Create a correlation plot of your willow creek 2012 dataset. How does it compare to the 2010 dataset in the tutorial? Save your plots created in #2, #3, #4, attach them to a document and answer the questions. Submit this document to Canvas. 11.3.1 Extra Credit - 2 points Using the WCr_1hr.2010.nc, WCr_1hr.2011.nc and WCr_1hr.2012.nc found in the datasets folder, complete the following Fuse together the 3 datasets into one continuous dataframe. Resample the data from an hourly to a daily resolution Plot Air temperature for your new combined data frame and add a trendline to it. Submit to assignment above labeled ‘extra_credit.png’ "],["plotting-with-ggplot2.html", "12 Plotting with ggplot2 12.1 ggplot2 with Willow Creek Meteorological Data 12.2 Assignment: 12.3 Extra Credit - 2 Points", " 12 Plotting with ggplot2 So far, we’ve only used base graphics plotting routines. While there are endless customizations and schemes that can be used to generate publiation quality material, many advanced R programmers use ggplot2 to create plots. ggplot2 is a go-to plotting package and can do all that base graphics can do with different styling. ggplot2 is designed to work with dataframes, rather than individual vectors. ggplot2 also has more aesthetic options that allow for more visually unique &amp; pleasing plots. The first thing you’ll notice about ggplot2 is that the syntax is different. Instead of coding all customizations through arguments or additional functions with the add=TRUE option keyed in, ggplot2 uses + to add in extra layers to a plot. This may seem weird at first, but the longer you spend with it the more regular it will seem. Below is a brief dicitonary of important ggplot functions. Function Description ggplot() Creates a new ggplot aes() Construct aesthetic mappings + add components to a plot geom_line() add line geometry to a ggplot geom_point() add point geometry to a ggplot geom_smooth() adds a trendline to a ggplot based on provided aesthetic data geom_density() adds a density plot to a ggplot scale_color_manual Adds color to ggplot layers xlim() x limits ylim() y limits xlab() x label ylab() y label ggtitle() adds title to a ggplot labs() ggplot function that takes a number of arguments and adds them to a ggplot (title, x label, y label, etc.) library(ggplot2) ## ## Attaching package: &#39;ggplot2&#39; ## The following object is masked from &#39;package:latticeExtra&#39;: ## ## layer # load in a dataset - https://github.com/jsimkins2/geog473-673/blob/master/datasets/acadia.csv acadia = read.csv(&quot;/Users/james/Documents/Github/geog473-673/datasets/acadia.csv&quot;) # list the top rows of acadia head(acadia) ## X region state code park_name type visitors year ## 1 1 NE ME ACAD Acadia National Park National Park 64000 1919 ## 2 2 NE ME ACAD Acadia National Park National Park 66500 1920 ## 3 3 NE ME ACAD Acadia National Park National Park 69836 1921 ## 4 4 NE ME ACAD Acadia National Park National Park 73779 1922 ## 5 5 NE ME ACAD Acadia National Park National Park 64200 1923 ## 6 6 NE ME ACAD Acadia National Park National Park 71758 1924 # remove the X column, it&#39;s just a duplicate index acadia$X = NULL # Initialize Ggplot ggplot(acadia, aes(x=year, y=visitors)) # year and visitors are columns in acadia dataframe, aes() stands for aesthetics # simple scatter plot ggplot(acadia, aes(x=year, y=visitors)) + geom_point() # simple line plot ggplot(acadia, aes(x=year, y=visitors)) + geom_line() # remember the trendline? here is how easy it is to add with ggplot2 ggplot(acadia, aes(x=year, y=visitors)) + geom_point() + geom_smooth(method=&quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Notice above how ggplot() creates a ggplot plot, but the geom_point() or geom_line() add the data to the plot. This is a key difference between base graphics and ggplot2. Also notice how you add what you want to the plot via the + symbol. Since the aes() (aesthetics) axes are already defined and since the acadia dataframe has been set, the functions geom_point() and geom_line() already know what data to add to the plots based on the dataset, x and y axes. Another cool thing about ggplot2() is that we can save a plot as an object and accumuatively add things to it g = ggplot(acadia, aes(x=year, y=visitors)) + geom_point() + geom_smooth(method=&quot;lm&quot;) plot(g) ## `geom_smooth()` using formula &#39;y ~ x&#39; # technically, we don&#39;t even have to say plot(g)...we can just print it and it will plot! g = ggplot(acadia, aes(x=year, y=visitors)) g = g + geom_point() g = g + geom_line() g = g + geom_smooth(method=&quot;lm&quot;) g ## `geom_smooth()` using formula &#39;y ~ x&#39; If you place aes in the initial ggplot argument, that sets the aesthetics, and the dataframe, for the entire plot regardless of the layers you add to it. However, we can also place an aes argument for each ggplot function we use (geom_line, geom_point, geom_smooth, etc.). This is useful if we are working with multiple datasets. Let’s create a dummy dataset with half the visitors of the acadia dataset and place the aesthetic function (aes) in each geom_point function. # create dummy data where we divide the actual number of visitors by 2 dummy_data = acadia dummy_data$visitors = dummy_data$visitors/2 # plot a ggplot instance with multiple lines g = ggplot(acadia) + geom_point(aes(x=year, y=visitors), color=&quot;red&quot;) + geom_smooth(data = acadia, aes(x=year, y=visitors), method=&quot;lm&quot;, color=&quot;red&quot;) + geom_point(data = dummy_data,aes(x=year, y=visitors), color=&quot;blue&quot;) + geom_smooth(data = dummy_data, aes(x=year, y=visitors), method=&quot;lm&quot;) plot(g) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `geom_smooth()` using formula &#39;y ~ x&#39; We can “crop” out areas of a plot using xlim and ylim. This method does delete points, however. g = ggplot(acadia, aes(x=year, y=visitors)) + geom_point() + geom_line() + geom_smooth(method=&quot;lm&quot;) g + xlim(c(2000,2017)) + ylim(c(2000000, 3500000)) # x axis years 2000 to 2017, ylimit 2000000 to 3500000 ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 81 rows containing non-finite values (stat_smooth). ## Warning: Removed 81 rows containing missing values (geom_point). ## Warning: Removed 81 row(s) containing missing values (geom_path). Instead of cropping out, we can also just zoom in with the coord_cartesian function (we’re using cartesian coordinates here). This method does not delete points. g = ggplot(acadia, aes(x=year, y=visitors)) + geom_point() + geom_line() + geom_smooth(method=&quot;lm&quot;) g + coord_cartesian(xlim=c(2000,2017), ylim = c(2000000, 3500000)) # x axis years 2000 to 2017, ylimit 2000000 to 3500000 ## `geom_smooth()` using formula &#39;y ~ x&#39; Notice how different the trendline is above. When we just zoom in, we preserve all of the data going into the plot which helps us keep the trendline of the entire dataset. Now let’s check out adjusting the x/y labels and title. There are 2 ways. # Add Title and Labels g1 = ggplot(acadia, aes(x=year, y=visitors)) + geom_point() + geom_line() + geom_smooth(method=&quot;lm&quot;) g1 + labs(title=&quot;Acadia National Park Attendance&quot;, subtitle=&quot;Total Visitors per year&quot;, y=&quot;Visitors&quot;, x=&quot;Year&quot;, caption=&quot;National Park Database&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; # or g2 = ggplot(acadia, aes(x=year, y=visitors)) + geom_point() + geom_line() + geom_smooth(method=&quot;lm&quot;) g2 = g2 + ggtitle(&quot;Acadia National Park Attendance&quot;, subtitle=&quot;Total Visitors per year&quot;) g2 + xlab(&quot;Year&quot;) + ylab(&quot;Visitors&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; What about changing up the colors, size, etc.? g = ggplot(acadia, aes(x=year, y=visitors)) + geom_line(col=&quot;yellow&quot;, size=5) + geom_point(col=&quot;steelblue&quot;, size=3) + geom_smooth(method=&quot;lm&quot;, col=&quot;firebrick&quot;) + labs(title=&quot;Acadia National Park Attendance&quot;, subtitle=&quot;Total Visitors per year&quot;, y=&quot;Visitors&quot;, x=&quot;Year&quot;, caption=&quot;National Park Database&quot;) g ## `geom_smooth()` using formula &#39;y ~ x&#39; 12.1 ggplot2 with Willow Creek Meteorological Data Let’s use ggplot2 to with the Willow Creek 2010 dataset. library(ncdf4) y = 2010 nc_file = nc_open(paste0(&quot;/Users/james/Documents/Github/geog473-673/datasets/WCr_1hr.&quot;, y,&quot;.nc&quot;)) # what does the nc file look like var_names = names(nc_file$var) wcr_data = list() dim &lt;- nc_file$dim for (v in seq_along(var_names)){ wcr_data[[v]] = ncvar_get(nc_file, varid = var_names[v]) } wcreek_df = data.frame(wcr_data) colnames(wcreek_df) = var_names # create a new short_names vector that matches the order of the actual names short_names = c(&quot;tair&quot;, &quot;tmax&quot;, &quot;tmin&quot;, &quot;lwave&quot;, &quot;pres&quot;, &quot;swave&quot;, &quot;ewind&quot;, &quot;nwind&quot;, &quot;shum&quot;, &quot;prec&quot;) # rename the column names to our new short name vector colnames(wcreek_df) = short_names date.seq = seq(as.POSIXct(paste0(y,&quot;-01-01 00:00:00&quot;)), as.POSIXct(paste0(y,&quot;-12-31 23:00:00&quot;)), by=&quot;hour&quot;) wcreek_df[&#39;datetime&#39;] = date.seq # Plot a ggplot point plot of willow creek 2010 shortwave radiation ggplot(wcreek_df) + geom_point(data = wcreek_df, aes(y=swave, x=datetime, color = &#39;Shortwave Radiation&#39;), size=0.5, alpha = 0.5) + xlab(&quot;Date&quot;) + ylab(&quot;Shortwave Radiaiton (W/m2&quot;) + ggtitle(&quot;Willow Creek Shortwave Radiation 2010&quot;) + scale_color_manual(values = c(&#39;Shortwave Radiation&#39; = &#39;firebrick&#39;)) # plot a ggplot density plot of the specific humidity ggplot(wcreek_df) + geom_density(data = wcreek_df, aes(x=shum, y=..density.., color = &#39;Specific Humidity&#39;), size=3, adjust = 1, fill=&quot;lightblue&quot;, alpha = 0.5) + xlab(&quot;Specific Humidity&quot;) + ggtitle(&quot;Gaussian Specific Humidity Densities 2010&quot;) + scale_color_manual(values = c(&#39;Specific Humidity&#39; = &#39;firebrick&#39;)) Let’s break down the geom_density plot above. First, we create a ggplot window of wcreek_df dataframe. Then, we call the the geom_density function and declared data argument as wcreek_df. Then, we specified the aes (aesthetics) function within the geom_density function. In the aes, we declared the x axis - specific humidity - and the y axis - ..density.. - which is necessary for the plot to know that’s the axis to place the density on. The final piece to the aes() argument is the color. Notice that we also set the color here to “Specific Humidity”. This is not a color in R. Rather, it’s a reference to the color we declare down below with scale_color_manual. In other words, we state below that the color of “Specific Humidity” is firebrick within the scale_color_manual function. Still within the geom_density function, we set the size of the density line, set the adjust which is ggplots term for bandwidth, declared a fill color to fill in the density area and gave that fill color an alpha of 0.5 to make it semi-transparent. Xlabel and title we’ve already covered. Finally I set a manual color label and shape of the label for specific humidity. 12.2 Assignment: Using the WCr_1hr.2012.nc found in the datasets folder, complete the following: Using ggplot2, create an geom_line plot for air_temperature. Add a trendline to this plot. Include appropriate titles, axes, labels, colors, etc. Using ggplot2, create a density plot of air temperature with appropriate titles, axes, legend etc. Submit the plots to UD canvas. Your final plots should look something like these… ## `geom_smooth()` using formula &#39;y ~ x&#39; 12.3 Extra Credit - 2 Points Using the WCr_1hr.2010.nc, WCr_1hr.2011.nc and WCr_1hr.2012.nc found in the datasets folder, complete the following Create a density plot using ggplot2 of “Air temperature” with each year being a component of the density plot (adding each year to the density plot as you loop through the data). Adjust alpha settings, colors, linetypes, etc. to make each line extinct. Submit to assignment above labeled ‘extra_credit.png’ - also submit your code. "],["spatial-plots-with-ggplot2.html", "13 Spatial Plots with ggplot2 13.1 Simple Features 13.2 Sea Surface Temperature ggplot 13.3 R Color Brewer Palettes 13.4 Assignment", " 13 Spatial Plots with ggplot2 In Geospatial Sciences we’re constantly working with spatial datasets that come in many different projections. We’ve previously shown how R can be used to read in spatial data, reproject spatial data, and resample spatial datasets. Once a spatial dataset can be stored in R as a data frame, we can use ggplot to plot it. Function Description geom_sf() Adds geometry stored in a sf object to a ggplot coord_sf() Provides coordinate specifications for a ggplot geom_raster() Plots a data frame as a raster on a ggplot st_crs() Obtains projection information from an EPSG code geom_sf_label() adds label to an sf geometry scale_fill_gradientn() adds a density plot to a ggplot borders() Adds country borders to a ggplot coord_quickmap() Approximates projected lines for faster image creation colorRampPalette() Create custom color ramp palette brewer.pal() Grabs premade color palette from RColorBrewer grid.arrange() Function that allows you to organize multiple ggplot objects into the same window 13.1 Simple Features One package useful for spatial data in R is the sf package (short for simple features). It contains functions that perform equations that place uneven projections on a 2 dimensional computer screen. It can handle many projections and is built to work seamlessly with ggplot. For sample data, we’re also going to load ozmaps which contains maps of Australia. library(ozmaps) library(sf) ## Linking to GEOS 3.8.1, GDAL 3.1.4, PROJ 6.3.1 oz_states &lt;- ozmaps::ozmap_states oz_states ## Simple feature collection with 9 features and 1 field ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 105.5507 ymin: -43.63203 xmax: 167.9969 ymax: -9.229287 ## geographic CRS: GDA94 ## # A tibble: 9 x 2 ## NAME geometry ## &lt;chr&gt; &lt;MULTIPOLYGON [°]&gt; ## 1 New South Wales (((150.7016 -35.12286, 150.6611 -35.11782, 150.6373 -35.… ## 2 Victoria (((146.6196 -38.70196, 146.6721 -38.70259, 146.6744 -38.… ## 3 Queensland (((148.8473 -20.3457, 148.8722 -20.37575, 148.8515 -20.3… ## 4 South Australia (((137.3481 -34.48242, 137.3749 -34.46885, 137.3805 -34.… ## 5 Western Australia (((126.3868 -14.01168, 126.3625 -13.98264, 126.3765 -13.… ## 6 Tasmania (((147.8397 -40.29844, 147.8902 -40.30258, 147.8812 -40.… ## 7 Northern Territory (((136.3669 -13.84237, 136.3339 -13.83922, 136.3532 -13.… ## 8 Australian Capital … (((149.2317 -35.222, 149.2346 -35.24047, 149.2716 -35.27… ## 9 Other Territories (((167.9333 -29.05421, 167.9188 -29.0344, 167.9313 -29.0… oz_states is a simple feature which stores data like data frames but has a projection (proj4string), spatial extents, and polygon geometry. In other words, we have shapes on a map that are georeferenced and stored within a data frame that ggplot is great at working with. There are two columns - NAME and geometry. The NAME column is the name of the Australian State. The geometry specifies where the lines of the polygons are drawn (the state boundaries). So, let’s plot these polygons! library(ggplot2) ggplot(data = oz_states) + geom_sf() + coord_sf() # if we declare mapping aesthetics, we can tell ggplot to fill the spatial features (australian states) based on the NAME column / variable - ggplot automatically chooses coloring for us if we don&#39;t specify ggplot() + geom_sf(data = oz_states, mapping = aes(fill = NAME)) + coord_sf() The function coord_sf allows to deal with the coordinate system, which includes both projection and extent of the map. By default, the map will use the coordinate system of the first layer that defines one (i.e. scanned in the order provided), or if none, fall back on WGS84 (latitude/longitude, the reference system used in GPS). Remember, our oz_states is a simple feature data frame that contains projection information. Thus, coord_sf is assuming we want the projection of oz_states - +proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs. We can specify our own crs argument if we want to override the assumed projection. If we set the crs argument to a valid PROJ4 string, we can accoimplish this. Let’s project our oz_states onto a Mollewide graph. ggplot() + geom_sf(data = oz_states, mapping = aes(fill = NAME)) + coord_sf(crs = &quot;+proj=moll +lon_0=0 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs&quot;) + ggtitle(&quot;oz_states projected onto Mollewide ggplot&quot;) We did not overwrite the data to this new projection. We simply performed a transformation on the fly to fit the data to a new grid. Instead of a PROJ4 string, we could use an EPSG code with the st_crs() function. Let’s not project our data onto a grid with EPSG code 3112. ggplot() + geom_sf(data = oz_states, mapping = aes(fill = NAME)) + coord_sf(crs = st_crs(3112)) + ggtitle(&quot;oz_states projected onto EPSG 3112&quot;) st_crs() is that simple, just plug in the code and the function will grab the PROJ4 string for you. coord_sf has other options that are useful too such as x and y limits. Let’s go back to our regular projection of this data and show the limits in action. ggplot() + geom_sf(data = oz_states, mapping = aes(fill = NAME), show.legend = FALSE) + coord_sf(xlim = c(140, 150), ylim=c(-44, -39.5)) + geom_sf_label( data = oz_states, aes(label = NAME)) We also added geom_sf_label which adds a label to an sf geometry. In this case, we added the Tasmania name tag to the geometry. 13.2 Sea Surface Temperature ggplot When it comes to spatial data, the raster package is our go-to library. After the data is translated to a dataframe and ready to plot, we’ll need the help of a package called mapproj which makes sense of some external maps that ggplot2 uses. library(ggplot2) library(mapproj) ## Loading required package: maps library(maptools) # also loads sp package library(raster) library(rasterVis) library(RColorBrewer) sstRast &lt;- raster(&quot;/Users/james/Documents/Github/geog473-673/datasets/GOES_R_ROLLING_1DAY_20190814.nc&quot;) # crop the raster so this runs faster sstRast &lt;- crop(sstRast, extent(-100,-80,16,30)) sstRast ## class : RasterLayer ## dimensions : 774, 1111, 859914 (nrow, ncol, ncell) ## resolution : 0.018, 0.0181 (x, y) ## extent : -99.99915, -80.00115, 15.99378, 30.00318 (xmin, xmax, ymin, ymax) ## crs : +proj=longlat +datum=WGS84 +no_defs ## source : memory ## names : Sea.Surface.Temperature ## values : 25.86023, 34.9397 (min, max) ## time : 2019-08-14 16:30:43 # convert to dataframe and df &lt;- as.data.frame(sstRast, xy = TRUE) head(df) ## x y Sea.Surface.Temperature ## 1 -99.99015 29.99413 NaN ## 2 -99.97215 29.99413 NaN ## 3 -99.95415 29.99413 NaN ## 4 -99.93615 29.99413 NaN ## 5 -99.91815 29.99413 NaN ## 6 -99.90015 29.99413 NaN # plot the raster ggplot() + geom_raster(data = df , aes(x = x, y = y, fill = Sea.Surface.Temperature)) + coord_sf() In this case, df is just a data frame - not an sf data frame. We need to use geom_raster in this case to plot a regular data frame as a raster. As for the projeciton, ggplot automatically guessed lat/long which in this case is correct. A better practice is to use the crs from the sstRast which contains all of the projection information. # print the crs of the sstRast crs(sstRast) ## CRS arguments: +proj=longlat +datum=WGS84 +no_defs # plug in crs into the `coord_sf` function ggplot() + geom_raster(data = df , aes(x = x, y = y, fill = Sea.Surface.Temperature)) + coord_sf(crs=crs(sstRast)) The georeferenced data is properly placed. Let’s now turn our attention to the coloring of the data. We can use the RColorBrewer package to create our own color palettes. Also, becuase we’re in lat/long projection, we’re going to use coord_quickmap instead of coord_sf. This function approximates geolocated lines for faster plotting. This can be used in lat/long projected data close to the equator. # now let&#39;s use a better colorscheme jet.colors &lt;- colorRampPalette(c(&quot;#00007F&quot;, &quot;blue&quot;, &quot;#007FFF&quot;, &quot;cyan&quot;, &quot;#7FFF7F&quot;, &quot;yellow&quot;, &quot;#FF7F00&quot;, &quot;red&quot;, &quot;#7F0000&quot;)) ggplot() + geom_raster(data = df , aes(x = x, y = y, fill = Sea.Surface.Temperature)) + scale_fill_gradientn(colors = jet.colors(7), limits = c(28, 33)) + coord_quickmap() # now let&#39;s add borders using the borders function ggplot() + geom_raster(data = df , aes(x = x, y = y, fill = Sea.Surface.Temperature)) + scale_fill_gradientn(colors = jet.colors(7), limits = c(28, 33)) + borders(fill=&quot;white&quot;, xlim = c(-100,-80), ylim=c(16,30),alpha = 0.5) + coord_quickmap(xlim = c(-100,-80), ylim=c(16,30)) Let’s get rid of the expanded area beyond the raster domain using the expand=FALSE argument in coord_quickmap() ggplot() + geom_raster(data = df , aes(x = x, y = y, fill = Sea.Surface.Temperature)) + scale_fill_gradientn(colors = jet.colors(7), limits = c(28, 33)) + borders(fill=&quot;white&quot;, xlim = c(-100,-80), ylim=c(16,30),alpha = 0.5) + coord_quickmap(xlim = c(-100,-80), ylim=c(16,30),expand = FALSE) 13.3 R Color Brewer Palettes There are a number of pre-made color palettes from RColorBrewer. Here is a list. library(RColorBrewer) display.brewer.all() Let’s plot using a Yellow-Orange-Red palette and a white NA value cols &lt;- brewer.pal(9, &quot;YlOrRd&quot;) # maximum number of colors in palette YlOrRd is 9 pal &lt;- colorRampPalette(cols) ggplot() + geom_raster(data = df , aes(x = x, y = y, fill = Sea.Surface.Temperature)) + scale_fill_gradientn(colors = pal(20), limits = c(25, 35),na.value = &quot;white&quot;) + borders(fill=&quot;white&quot;, xlim = c(-100,-80), ylim=c(16,30),alpha = 0.5) + coord_quickmap(xlim = c(-100,-80), ylim=c(16,30),expand = FALSE) 13.3.1 Multi Plot ggplot Multiple Plots in one window is possible with ggplot2 but is done with a different method. Remember how we explicitly named ggplot2 instances in the previous tutorial? We must do that again in order to achieve the results we want. Let’s split up the image above into West Gulf of Mexico and East Gulf of Mexico. library(ggplot2) library(grid) library(gridExtra) p1 = ggplot() + geom_raster(data = df , aes(x = x, y = y, fill = Sea.Surface.Temperature), show.legend=FALSE) + scale_fill_gradientn(colors = pal(20), limits = c(25, 35),na.value = &quot;white&quot;) + borders(fill=&quot;white&quot;, xlim = c(-100,-90), ylim=c(16,30),alpha = 0.5) + coord_quickmap(xlim = c(-100,-90), ylim=c(16,30),expand = FALSE) p2 = ggplot() + geom_raster(data = df , aes(x = x, y = y, fill = Sea.Surface.Temperature), show.legend=FALSE) + scale_fill_gradientn(colors = pal(20), limits = c(25, 35),na.value = &quot;white&quot;) + borders(fill=&quot;white&quot;, xlim = c(-90,-80), ylim=c(16,30),alpha = 0.5) + coord_quickmap(xlim = c(-90,-80), ylim=c(16,30),expand = FALSE) # use the grid.arrange function from grid and gridExtra to plot our 2 ggplots in the same window grid.arrange(p1,p2, ncol=2, nrow=1) grid.arrange takes ggplot objects and plots them onto a window with specified rows and columns. Here we wanted these two separate plots side by side. 13.4 Assignment Download treecov.nc from the datasets folder For South America and Africa, plot the tree cover variable using ggplot2. Use a green color theme from RColorBrewer. Add borders. Place each ggplot next to each other in one plot window using grid.arrange. Submit resulting image to UD Canvas. Your final product should look something like… 13.4.1 Extra Credit - 2 Points Using the data above, approximate the average tree cover for South America and Africa (extents don’t have to be exact, just generally) Plot the same domains above but this time color each continent (yes, the whole thing) based on the average tree cover. For example, if one of the continent has an average tree cover of 30%, the entire continent would be red based on the colorscale you choose where 30% is red. Submit plot and code to canvas "],["shapefiles.html", "14 Shapefiles 14.1 Reading in Shapefiles 14.2 Combining Shapefiles with Data 14.3 Visualizing the Shapefiles &amp; Data 14.4 Saving a shapefile 14.5 Assignment", " 14 Shapefiles Shapefiles are polygons containing geolocated data. R is great at handling shapefiles and is often faster and more efficient in visualizing these than other programs like ArcGIS. For shapefiles, we will use a combination of new packages. Most notably, we’ll be using rgdal, a package designed to work with geolocated datasets with any projection, and sp (spatial points), the sister package of sf. These two packages are useful for reading and curating shapefiles. Data derived from these two packages can also work with mulitiple visualization packages including ggplot2. 14.1 Reading in Shapefiles Often times, the spatial data that we want aren’t in a nice, gridded lat/lon. The oblate spheroid that we call home presents some challenges when it comes to displaying spatial datasets. Often times, datasets like these are stored as separate entities - shapefiles &amp; data tables. Fortunately, we have multiple avenues for working with this data. For this tutorial, we’re going to use data from the University of Delaware coastal flooding dashboard - a emergency warning product for the coastal communities of Delaware. The data is located in the datasets folder under the folder titled cfms_shapefiles/. The file we need to read in from this group is cfms_watersheds.shp, however, we need all files from this folder to stick together. In other words, all files must reside in the same folder together on your local machine. In any program, when you read in a shapefile (.shp), the program automatically references the .shx, .prj, .dbf, etc. files as well. #Load packages library(RColorBrewer) library(rgdal) library(sp) library(ggplot2) library(ggmap) ## Google&#39;s Terms of Service: https://cloud.google.com/maps-platform/terms/. ## Please cite ggmap if you use it! See citation(&quot;ggmap&quot;) for details. library(scales) library(viridis) ## Loading required package: viridisLite ## ## Attaching package: &#39;viridis&#39; ## The following object is masked from &#39;package:scales&#39;: ## ## viridis_pal # Use the readOGR function from the rgdal package to open a shapefile &amp; constituents coast.shp &lt;- readOGR(&quot;/Users/james/Documents/Github/geog473-673/datasets/cfms_shapefiles/cfms_watersheds.shp&quot;) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/Users/james/Documents/Github/geog473-673/datasets/cfms_shapefiles/cfms_watersheds.shp&quot;, layer: &quot;cfms_watersheds&quot; ## with 24 features ## It has 15 fields class(coast.shp) ## [1] &quot;SpatialPolygonsDataFrame&quot; ## attr(,&quot;package&quot;) ## [1] &quot;sp&quot; coast.shp@proj4string ## CRS arguments: ## +proj=tmerc +lat_0=38 +lon_0=-75.4166666666667 +k=0.999995 +x_0=200000 ## +y_0=0 +datum=NAD83 +units=m +no_defs We use readOGR from the rgdal package to load in the shapefile. As stated above, even though we only key the loading of the cfms_watersheds.shp file, readOGR is also opening the other files (.shx, .prj, .dbf, etc.) as well. This projection is unique. It’s a transverse mercator with some specific lat_0 and lon_0 starting points. Notice the class of the shapefile - the underlying package controlling it’s translation to R is the sp package. The sp package provides classes and methods for the points, lines, polygons, and grids of this shapefile. Notice the *@* symbol in coast.shp@proj4string. This is how we query metadata associated with shapefiles. 14.2 Combining Shapefiles with Data We have a data file that contains flooding data for the polygons of the shapefile read in above. This data file is water_levels.csv. Let’s read this data in. plot(coast.shp) # Open the dataset that corresponds to the water levels within the shapefile boxes coast.data &lt;- read.csv(&quot;/Users/james/Documents/Github/geog473-673/datasets/cfms_shapefiles/water_levels.csv&quot;) head(coast.data) ## station lon lat mhhwtomsl navd88tomsl mllwtomsl mhhw mllw ## 1 DBOFS001 -75.14990 38.79219 2.488845 0.3779528 -2.1168 2.110892 -2.4948 ## 2 DBOFS001 -75.14990 38.79219 2.488845 0.3779528 -2.1168 2.110892 -2.4948 ## 3 DBOFS001 -75.14990 38.79219 2.488845 0.3779528 -2.1168 2.110892 -2.4948 ## 4 DBOFS002 -75.30239 38.94447 2.752297 0.2916667 -2.3451 2.460630 -2.6368 ## 5 DBOFS003 -75.30879 38.95471 2.771982 0.2877297 -2.3543 2.484252 -2.6421 ## 6 DBOFS003 -75.30879 38.95471 2.771982 0.2877297 -2.3543 2.484252 -2.6421 ## maxpred maxovermhhw ## 1 3.622047 1.510047 ## 2 3.622047 1.510047 ## 3 3.622047 1.510047 ## 4 4.048557 1.587557 ## 5 4.048557 1.564557 ## 6 4.048557 1.564557 The data is loaded, but how to we correspond the data of the csv file with the appropriate polygon within the shp file? We must find a matching key. In this case, the key is the station. Both the coast.shp and coast.data datasets contain the station variable. Now, we must merge together the flooding data with the station within each polygon. # now let&#39;s find the matching key - in this case, the matching key is the &quot;station&quot;. as.vector(coast.shp$station[1:5]) ## [1] &quot;DBOFS015&quot; &quot;DBOFS016&quot; &quot;DBOFS003&quot; &quot;DBOFS012&quot; &quot;DBOFS013&quot; as.vector(coast.data$station[1:5]) ## [1] &quot;DBOFS001&quot; &quot;DBOFS001&quot; &quot;DBOFS001&quot; &quot;DBOFS002&quot; &quot;DBOFS003&quot; station is the matching column, but the stations are not in the same order. We must reorder them appropriately before we merge together the datasets. # notice the difference above - let&#39;s reorder the data from the shapefile and the csv data by the station, otherwise merging will NOT work. coast.shp = coast.shp[order(as.vector(coast.shp$station)),] coast.data = coast.data[order(as.vector(coast.data$station)),] as.vector(coast.shp$station[1:5]) ## [1] &quot;DBOFS001&quot; &quot;DBOFS001&quot; &quot;DBOFS001&quot; &quot;DBOFS002&quot; &quot;DBOFS003&quot; as.vector(coast.data$station[1:5]) ## [1] &quot;DBOFS001&quot; &quot;DBOFS001&quot; &quot;DBOFS001&quot; &quot;DBOFS002&quot; &quot;DBOFS003&quot; Now that the order of the stations are the same, we can merge together the two datasets. We will merge together the datasets using the merge function from sp. Here, we denote the use of this function by sp::merge. Why? There are multiple merge functions in R, and we need to tell R which package contains the specific merge function we wish to use. The function is straight-forward, but one argument that you may notice is the duplicateGeoms = TRUE argument. We need to specify this because we have multiple data values for the same station name. For example, there are multiple DBOFS001 stations above. # merge together the shapefile data and the csv data based on common variable, here called &#39;station&#39; - we MUST use the duplicateGeoms argument because there are multiple values for the same station name merged.coast = sp::merge(coast.shp,coast.data,by=&#39;station&#39;, duplicateGeoms = TRUE) class(merged.coast) ## [1] &quot;SpatialPolygonsDataFrame&quot; ## attr(,&quot;package&quot;) ## [1] &quot;sp&quot; We have merged together the data for each station and thus each polygon now has data associated with it. There’s 24 variables here and station is the only variable we’ve covered so far. The variable that contains the data we’re interested in here is called maxpred, which is the maximum predicted water level in feet above sea level for the next 24 hours for each station. Before we color the polygons based on this maxpred variable, we need to create a color palette using RColorBrewer. # Now let&#39;s use brewer.pal from RColorBrewer to create a blues color pallette mycolours &lt;- brewer.pal(8, &quot;Blues&quot;) 14.3 Visualizing the Shapefiles &amp; Data The color scheme has been prepared with the appropriate breaks that we wanted. Now, we can plot the dataset and color the shapefile polygons. There are multiple ways to plot this data and we’ll begin with a simple spplot from the sp package. # first we&#39;ll plot a simple one to show this function spplot(obj = merged.coast, zcol = &quot;maxpred&quot;) # now let&#39;s add in extra arguments and color our polygons with our colorshcmee spplot(obj = merged.coast, zcol = &quot;maxpred&quot;, par.settings = list(axis.line = list(col =&quot;transparent&quot;)), main = &quot;Projected Water Levels (Feet)&quot;, cuts = 5, col =&quot;transparent&quot;, col.regions = mycolours) Simple and displays the data easily. Now we turn our attention to the tamp package - Thematic Map Visualization. Thematic maps are geographical maps in which spatial data distributions are visualized. This package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps. It is based on the grammar of graphics, and resembles the syntax of ggplot2. library(tmap) tm_shape(merged.coast) + tm_polygons(col=&#39;maxpred&#39;, title = &quot;Projected Water Levels&quot;, palette = &quot;Blues&quot;) + tm_style(&quot;classic&quot;) + tm_scale_bar(position = c(&quot;left&quot;, &quot;bottom&quot;)) ## Warning in sp::proj4string(obj): CRS object has comment, which is lost in output We automatically queried a Blues color palette with automatic breaks with the tm_shape and tm_polygons functions. Notice how similar this functionality is to ggplot2. Let’s turn our attention to ggplot2 at this time. Before we use this package, we will need to convert our SpatialPolygonsDataFrame to a sf dataframe. For this, we can use the st_as_sf function which converts a foreign object to an sf object. library(sf) # convert it to an sf object gg_merged = st_as_sf(merged.coast) class(gg_merged) ## [1] &quot;sf&quot; &quot;data.frame&quot; # now we use geom_sf since our gg_merged is now a simple feature ggplot() + geom_sf(data = gg_merged, aes(fill = maxpred)) Let’s get rid of the default gray background of this ggplot instance. This default background is actually part of the default ggplot theme. There are multiple themes we can use including theme_bw(), theme_dark(), theme_light(), theme_void() and more. Each of these themes are useful under varying circumstances. The void theme is nice when we want to bring more visual focus to the data. However, this map would look better if we added in state polygons. # read in the states shapefile from the course datasets folder states &lt;- readOGR(&quot;/Users/james/Documents/Github/geog473-673/datasets/ne_10m_admin_1_states_provinces/ne_10m_admin_1_states_provinces.shp&quot;) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/Users/james/Documents/Github/geog473-673/datasets/ne_10m_admin_1_states_provinces/ne_10m_admin_1_states_provinces.shp&quot;, layer: &quot;ne_10m_admin_1_states_provinces&quot; ## with 4594 features ## It has 83 fields ## Integer64 fields read as strings: ne_id class(states) ## [1] &quot;SpatialPolygonsDataFrame&quot; ## attr(,&quot;package&quot;) ## [1] &quot;sp&quot; states = st_as_sf(states) class(states) ## [1] &quot;sf&quot; &quot;data.frame&quot; ggplot(data = states) + geom_sf() + theme_void() + geom_sf(data = gg_merged, aes(fill = maxpred)) + coord_sf(xlim = c(-76.1, -74.7), ylim = c(38.3,40), expand = FALSE) + scale_fill_distiller(palette = &quot;Blues&quot;, direction= 1) Looks pretty good even though the shapefile isn’t quite as sharp as we’d like. Besides the shapefile which we added to the plot via geom_sf(), take notice of the theme_void()…this plot will NOT WORK without theme_void(). There is ongoing github discussions with some of the developers at ggplot2 to figure out why, but for now just make sure you use theme_void to add shapefiles to the same plot in R and have them work properly. Also take note of the scale_fill_distiller function. This is an easy way to throw in a RColorbrewer color pallette. In this case we used the Blues colorpallete since we’re dealing with water but you can make it any RColorbrewer palette you want. Also notice the direction=1. This reverses the order of the blues color pallette. Instead of the scale_fill_distiller function, you can also use scale_fill_viridis() function which has some default pallettes such as - “magma”, “plasma”, and “inferno”. The last package we’ll introduce for visualizing shapefiles is the mapview package. This package overlays shapefiles and corresponding data over an HTML based interactive map. This package handles a lot of the dirty work for us. library(mapview) ## GDAL version &gt;= 3.1.0 | setting mapviewOptions(fgb = TRUE) mapview(merged.coast[&#39;maxpred&#39;], col.regions = mycolours) ## Warning: Found less unique colors (8) than unique zcol values (19)! ## Interpolating color vector to match number of zcol values. 14.4 Saving a shapefile So we combined our csv station data with the corresponding shapefile statoin to make these plots. We don’t need to duplicate that hard work again because we can actually save our merged shapefile/csv as an ENVI shapefile using the writeOGR function from rgdal. writeOGR(obj = merged.coast, dsn = &quot;/Users/james/Downloads/coast_files/merged.coast&quot;, layer = &quot;coast-rgdal&quot;, driver = &quot;ESRI Shapefile&quot;) 14.5 Assignment Using the ne_10m_parks_and_protected_lands shapefile dataset, map the names of the protected lands (variable is name) in the Pacific Northwest using the methods listed below. You will need to download all of the contents of the ne_10m_parks_and_protected_lands shapefile folder. Below, a bounding lat/lon box of -127 W, -110 E, 40 S, 50 N was used for tmap plot and ggplot plot. You may use a bounding box of your choice where you see fit. Submit your plots to UD Canvas. tmap ggplot2 mapview Create the 3 plots above and make them look as close as possible to the ones below. ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/Users/james/Documents/Github/geog473-673/datasets/ne_10m_parks_and_protected_lands/ne_10m_parks_and_protected_lands_area.shp&quot;, layer: &quot;ne_10m_parks_and_protected_lands_area&quot; ## with 61 features ## It has 8 fields ## Integer64 fields read as strings: scalerank ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/Users/james/Documents/Github/geog473-673/datasets/ne_10m_admin_1_states_provinces/ne_10m_admin_1_states_provinces.shp&quot;, layer: &quot;ne_10m_admin_1_states_provinces&quot; ## with 4594 features ## It has 83 fields ## Integer64 fields read as strings: ne_id ## [1] &quot;SpatialPolygonsDataFrame&quot; ## attr(,&quot;package&quot;) ## [1] &quot;sp&quot; ## [1] &quot;sf&quot; &quot;data.frame&quot; "],["remote-data-extraction.html", "15 Remote Data Extraction 15.1 THREDDS 15.2 ERDDAP 15.3 Assignment", " 15 Remote Data Extraction Up to this point, we’ve been using locally downloaded datasets derived from the course datasets folder. We downloaded these files in bulk or individually like so… download.file(&quot;https://github.com/jsimkins2/geog473-673/tree/master/datasets/TreeData.csv&quot;, destfile = &quot;/Users/james/Downloads/TreeData.csv&quot; , mode='wb') While Github is excellent for code, it’s not a cloud service for datasets. THREDDS and ERDDAP are the future of environmental data repositories. 15.1 THREDDS THREDDS (Thematic Realtime Environmental Distributed Data Services) is an efficient way to extract specific areas or time periods of a dataset. For example, if you’re studying 2000-2020 water temperatures of the Delaware Bay, you don’t necessarily want a water temperature dataset covering the Atlantic Ocean from 1960-2020. It’s a waste of time to have to download, store, and process all of that data just to sub-select the Delaware Bay from 2000-2020. THREDDS makes it possible to download your desired subset from the get-go, saving you time and hard-drive space. Here are some NASA/NOAA/UD THREDDS servers: https://thredds.jpl.nasa.gov/thredds/catalog.html - NASA Jet Propulsion Labratory https://thredds.daac.ornl.gov/thredds/catalog.html - Oak Ridge National Lab https://pae-paha.pacioos.hawaii.edu/thredds/catalog.html - Pacific Islands Ocean Observing System http://thredds.demac.udel.edu/thredds/catalog.html - UDEL DEMAC http://basin.ceoe.udel.edu/thredds/catalog.html - UDEL Satellite Receiving Station http://www.smast.umassd.edu:8080/thredds/catalog.html - UMASS Thredds If you have a dataset or type of data you’re interested in, google search it with thredds or thredds server after it. Today we’ll use UD’s Satellite Receiving Station THREDDS (#5 on the list). It’s located at this URL - http://basin.ceoe.udel.edu/thredds/catalog.html Here’s what that looks like: If we click on GOES-R SST, we see we have some different avenues for data extraction. OPeNDAP (Open-source Project for a Network Data Access Protocol) is a great way to subselect the data. Opendap offers html files of the data (BAD IDEA, THIS WILL CRASH YOUR BROWSER) or netCDF files of the data (great idea) Now you can use this page to download subset datasets, or we can make this really easy and use R to accomplish that task. This is a high temporal resolution dataset, so let’s say we want Delaware Bay data from July 14 - July 16, 2019. All we need to make this happen is the url of the opendap page - http://basin.ceoe.udel.edu/thredds/dodsC/GOESJPLSST.nc.html - and the ncdf4 package. library(ncdf4) goes.nc = nc_open(&quot;http://basin.ceoe.udel.edu/thredds/dodsC/GOESJPLSST.nc&quot;) goes.nc ## File http://basin.ceoe.udel.edu/thredds/dodsC/GOESJPLSST.nc (NC_FORMAT_CLASSIC): ## ## 8 variables (excluding dimension variables): ## float projection[] ## proj4_string: +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0 ## epsg_code: 4326 ## float sea_surface_temperature[longitude,latitude,time] ## long_name: sea surface sub-skin temperature ## standard_name: sea_surface_subskin_temperature ## comment: SST obtained by regression with buoy measurements, sensitive to skin SST. Further information at (Petrenko et al., JGR, 2014; doi:10.1002/2013JD020637) ## units: kelvin ## float quality_level[longitude,latitude,time] ## long_name: quality level of SST pixel ## flag_values: 1 ## flag_values: 2 ## flag_values: 3 ## flag_values: 4 ## flag_values: 5 ## flag_meanings: no_data bad_data not_used not_used not_used best_quality ## float dt_analysis[longitude,latitude,time] ## long_name: deviation from SST reference ## comment: Deviation from reference SST, i.e., dt_analysis = SST - reference SST ## units: kelvin ## float satellite_zenith_angle[longitude,latitude,time] ## long_name: satellite zenith angle ## comment: satellite zenith angle ## units: degrees ## float sses_bias[longitude,latitude,time] ## long_name: SSES bias estimate ## comment: Bias is derived against Piecewise Regression SST produced by local regressions with buoys. Subtracting sses_bias from sea_surface_temperature produces more accurate estimate of SST at the depth of buoys. Further information at (Petrenko et al., JTECH, 2016; doi:10.1175/JTECH-D-15-0166.1) ## units: kelvin ## float sses_standard_deviation[longitude,latitude,time] ## long_name: SSES standard deviation ## units: kelvin ## comment: Standard deviation of sea_surface_temperature from SST measured by drifting buoys. Further information at (Petrenko et al., JTECH, 2016; doi:10.1175/JTECH-D-15-0166.1) ## float wind_speed[longitude,latitude,time] ## long_name: wind speed ## units: m s-1 ## comment: Typically represents surface winds (10 meters above the sea surface) ## ## 3 dimensions: ## latitude Size:1800 ## standard_name: latitude ## units: degrees_north ## longitude Size:2500 ## standard_name: longitude ## units: degrees_east ## time Size:26399 ## standard_name: time ## long_name: EPOCH Time ## units: seconds since 1970-01-01T00:00:00Z ## ## 9 global attributes: ## _NCProperties: version=2,netcdf=4.7.1,hdf5=1.10.5 ## creator_name: James Simkins ## creator_email: simkins@udel.edu ## institution: University of Delaware Ocean Exploration, Remote Sensing and Biogeography Group (ORB) ## url: http://orb.ceoe.udel.edu/ ## source: NOAA/NESDIS/STAR GHRSST- http://www.star.nesdis.noaa.gov - SST ## groundstation: University of Delaware, Newark, Center for Remote Sensing ## summary: NOAA/NESDIS/STAR GHRSST GOES16 SST product, fit to UDel ORB lab NW Atlantic extent, reprojected to EPSG:4326. ## acknowledgement: These data were provided by Group for High Resolution Sea Surface Temperature (GHRSST) and the National Oceanic and Atmospheric Administration (NOAA) Just with that one line of code, we’ve opened a connection with the GOES-R dataset on the THREDDS server. Printing the netcdf dataset provides some metadata info. Let’s use this metadata and extract the time period / spatial extent that we want. # print out the names of the variables in our dataset names(goes.nc$var) ## [1] &quot;projection&quot; &quot;sea_surface_temperature&quot; ## [3] &quot;quality_level&quot; &quot;dt_analysis&quot; ## [5] &quot;satellite_zenith_angle&quot; &quot;sses_bias&quot; ## [7] &quot;sses_standard_deviation&quot; &quot;wind_speed&quot; # how is the time stored? goes.nc$dim$time$units ## [1] &quot;seconds since 1970-01-01T00:00:00Z&quot; Seconds since 1970-01-01 is referred to as EPOCH time. Basically, this datetime is considered the inception of the internet. Computers are very good at storing information in this format and this is why we use this. Let’s take out the last value - lastVal = length(goes.nc$dim$time$vals) lastVal ## [1] 26399 epoch_val = goes.nc$dim$time$vals[lastVal] There you go, that’s an EPOCH time value. Let’s convert it to a human timestamp… human_time = as.POSIXct(epoch_val, origin=&quot;1970-01-01&quot;) human_time ## [1] &quot;2021-01-25 14:00:00 EST&quot; as.POSIXct is a datetime package in R. It is a gold standard and you’ll see it as you gain more experience in playing with datetime conversions. You can also use anytime package. library(anytime) anytime(epoch_val) ## [1] &quot;2021-01-25 14:00:00 EST&quot; At this point, all we have to do is convert our human dates to EPOCH so we can extract the data. In order to do this all we need to do is convert a datetime object to a numeric. R handles it for us… start_time = &quot;2019-07-14&quot; # year dash month dash day epoch_start_time = as.numeric(as.POSIXct(start_time, format=&quot;%Y-%m-%d&quot;)) # %Y-%m-%d is telling the computer the format of our datestring is year dash month dash day end_time = &quot;2019-07-16&quot; # year dash month dash day epoch_end_time = as.numeric(as.POSIXct(end_time, format=&quot;%Y-%m-%d&quot;)) # %Y-%m-%d is telling the computer the format of our datestring is year dash month dash day We have the time values converted to the format of the dataset, but now we need to find the index - i.e. where that value lies in the dataset. We can find this using this code… which.min(abs(array - value)) Which reads as - which.min() - which value is the minimum of this array abs() - absolute value - we take the absolute value because negative numbers confuse the math index_start_time = which.min(abs(goes.nc$dim$time$vals - epoch_start_time)) index_start_time ## [1] 13384 That’s the index! This is just a big matching game essentially. goes.nc$dim$time$vals[index_start_time] ## [1] 1563076800 epoch_start_time ## [1] 1563076800 Alright, we have our start time index! What about the latitudes and longitudes? We’ll need to find the index of the lat/lon grid we want. Delaware bay is approximately between -77W, -73W, 36N, and 42N. # print out a few longitude values - notice that the entire dataset is on this grid right here. head(goes.nc$dim$lon$vals) ## [1] -99.99 -99.97 -99.95 -99.93 -99.91 -99.89 # notice how we extract those values using indexing - the 100th value goes.nc$dim$lon$vals[100] ## [1] -98.01 The 100th lon value is -98.20815. Aka, a lon index of 100 returns -98.20815. So…which value is the minimum of the absolute value of the array of values minus the specific value? Let’s plug it in… west_lon = -77 index_west_lon = which.min(abs(goes.nc$dim$longitude$vals - west_lon)) index_west_lon ## [1] 1150 goes.nc$dim$longitude$vals[index_west_lon] ## [1] -77.01 So our desired west_lon is -77, and the closest value within our longitude array is -76.99001. Not bad…let’s run the rest. east_lon = -73 index_east_lon = which.min(abs(goes.nc$dim$longitude$vals - east_lon)) north_lat = 42 index_north_lat = which.min(abs(goes.nc$dim$latitude$vals - north_lat)) south_lat = 36 index_south_lat = which.min(abs(goes.nc$dim$latitude$vals - south_lat)) Everything is indexed! Now we can use these indexes to extract the exact data that we want via ncvar_get from the ncdf4 package. We can enter in arguments named start and count. This tells ncvar_get at what space / time to start grabbing values. count tells ncvar_get how long to count in space / time. For example, if the resolution of our data is hourly and we start at 12:00:00 and the count is 4, that means we grab data at 12:00:00 , 13:00:00 , 14:00:00 , 15:00:00 , 16:00:00. The same goes for lat and lon - the count all depends on what resolution your data is in. If you have a spatial dataset at 5 degree resolution, each count will bring in another 5 degree lat/lon value. At this point, we know our start values. At this point, we only have end values, not count values…Let’s figure those out time_count = which.min(abs(goes.nc$dim$time$vals - epoch_end_time)) - which.min(abs(goes.nc$dim$time$vals - epoch_start_time)) time_count ## [1] 48 So, if we count 48 time values from the start_time of 2019-07-14, we’ll arrive at the end_time of 2019-07-16. Let’s do the same for our lat/lons. # latitude counts lat_count = abs(index_north_lat - index_south_lat) lon_count = abs(index_west_lon - index_east_lon) Why do we take the absolute value? Because sometimes data is stored backwards. We don’t care which way it’s stored, we just need a positive number to count up from the starting value. Longitude values are weird because many times they are stored as negative values if they’re west of the meridian. Latitude values are also negative if we go south of the equator. Now we have our count values, we can proceed. Remember, here is what the start argument is looking for…via (?ncvar_get) A vector of indices indicating where to start reading the passed values (beginning at 1). The length of this vector must equal the number of dimensions the variable has. Order is X-Y-Z-T (i.e., the time dimension is last). If not specified, reading starts at the beginning of the file (1,1,1,...). So x is longitude, y is latitude, and t is time. Whichever value is lower between index_west_lon and index_east_lon is our start value, and vice versa for latitudes. index_west_lon ## [1] 1150 index_east_lon ## [1] 1350 index_south_lat ## [1] 800 index_north_lat ## [1] 500 Wait just a second…the southern index is greater than the northern index. Typically, values are stored from least to greatest. However, we’ve seen before that netCDF files can be stored upside-down before. So, instead of counting from our southern latitude to our northern latitude, we need to count from our northern latitude to our southern latitude. Here is what the count argument is looking for… A vector of integers indicating the count of values to read along each dimension (order is X-Y-Z-T). The length of this vector must equal the number of dimensions the variable has. If not specified and the variable does NOT have an unlimited dimension, the entire variable is read. As a special case, the value &quot;-1&quot; indicates that all entries along that dimension should be read. We tell ncvar_get() where to start it’s longitude, latitude, time and count until we have our lat/lon box and time slices selected. We already have our count values so now we plug them in and run - note this will take a few seconds to run and even longer if you have poor internet connection. This is data being extracted through the internet in real time. # cool, let&#39;s grab sea_surface_temperature for Delaware Bay sst.c &lt;- ncvar_get(goes.nc, &quot;sea_surface_temperature&quot;,start = c(index_west_lon,index_north_lat,index_start_time), count = c(lon_count,lat_count,time_count)) dim(sst.c) ## [1] 200 300 48 And there is the data! It returns the raw data in the format we pulled it in - Lon, Lat, Time. We have a 2 dimensional array (lon x lat) with 48 time slices. Let’s convert one to raster and plot it really quickly. library(maptools) # also loads sp package library(ncdf4) library(raster) library(rasterVis) arr.sst = sst.c[,,4] arr.sst[arr.sst &lt; 273] = NA test.sst = raster(x = arr.sst) test.sst ## class : RasterLayer ## dimensions : 200, 300, 60000 (nrow, ncol, ncell) ## resolution : 0.003333333, 0.005 (x, y) ## extent : 0, 1, 0, 1 (xmin, xmax, ymin, ymax) ## crs : NA ## source : memory ## names : layer ## values : 294.54, 302.97 (min, max) What’s missing? Well we just gave it the raw data, we still need to plug in the extents! Also, we will need to plug in the CRS - luckily we know this data is in lat/lon because of how awesome this metadata is! library(ggplot2) # we need to transpose this dataset just like we did in week 4 of the R intro course test.sst = t(test.sst) # define the projection sst.crs = ncatt_get(goes.nc, &quot;projection&quot;) extent(test.sst) = c(west_lon, east_lon, south_lat, north_lat) crs(test.sst) = sst.crs$proj4_string ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;): Discarded datum WGS_1984 in CRS definition, ## but +towgs84= values preserved # convert raster to dataframe for ggplot df &lt;- as.data.frame(test.sst, xy = TRUE) # Throw together the usa spatial polygons data frame ggplot() + geom_raster(data = df , aes(x = x, y = y, fill = layer)) + borders(fill=&quot;white&quot;, xlim = c(-77,-73), ylim=c(36,42),alpha = 0.5) + coord_quickmap(xlim = c(-77,-73), ylim=c(36,42),expand = FALSE) Boom…There’s the data we wanted and we didn’t have to leave R to get it. We also saved a ton of time and hard drive space. 15.2 ERDDAP THREDDS is very fast and great when you’re already familiar with a dataset. ERDDAP, on the other hand, is meant for humans. ERDDAP is a data server that gives you a simple, consistent way to download subsets of gridded and tabular scientific datasets in common file formats and make graphs and maps. ERDDAP can generate maps on the fly so you can check out the data before you proceed to download. Here are some NASA / NOAA / UDEL ERDDAP pages: https://upwell.pfeg.noaa.gov/erddap/index.html - NOAA Global Earth Observation Over 10,000 datasets available here https://coastwatch.pfeg.noaa.gov/erddap/index.html - NOAA Ocean ERDDAP - over 1400 datasets available here https://gliders.ioos.us/erddap/index.html - IOOS Ocean Glider Data - Over 600 datasets here http://www.neracoos.org/erddap/index.html - NERACOOS Ocean/Met - Over 200 Datasets http://basin.ceoe.udel.edu/erddap/index.html - UDEL Satellite Receiving Station ERDDAP Here’s a bigger list - https://github.com/rmendels/awesome-erddap Let’s check out the UDEL Satellite Receiving Station ERDDAP page - here’s what it looks like If we click on GOES-R SST, we see we have some different avenues for data extraction. OPeNDAP (Open-source Project for a Network Data Access Protocol) is a great way to subselect the data. Opendap offers html files of the data (BAD IDEA, THIS WILL CRASH YOUR BROWSER) or netCDF files of the data (great idea) You can use the Data Access Form at the top of the page to select data and download, or we can streamline this and use R to accomplish this easily. In order to do this, we just need the rerddap package. install.packages(&quot;rerddap&quot;) library(&quot;rerddap&quot;) ?rerddap the list of servers rerddap knows about - server() search an ERDDAP server for terms - ed_search(query, page = NULL, page_size = NULL, which = &quot;griddap&quot;, url = eurl(), ...) get a list of datasets on an ERDDAP server - ed_datasets(which = &quot;tabledap&quot;, url = eurl()) obtain information about a dataset - info(datasetid, url = eurl(), ...) extract data from a griddap dataset - griddap(x, ..., fields = &quot;all&quot;, stride = 1, fmt = &quot;nc&quot;, url = eurl(), store = disk(), read = TRUE, callopts = list()) extract data from a tabledap dataset - tabledap(x, ..., fields = NULL, distinct = FALSE, orderby = NULL, orderbymax = NULL, orderbymin = NULL, orderbyminmax = NULL, units = NULL, url = eurl(), store = disk(), callopts = list()) Be careful when using the functions ed_search() and ed_datasets(). The default ERDDAP has over 9,000 datasets, most of which are grids, so that a list of all the gridded datasets can be quite long. A seemly reasonable search: 15.2.1 Finding the Data You Want The first way to find a dataset is to browse the builtin web page for a particular ERDDAP server. A list of some of the public available ERDDAP servers can be obtained from the rerddap command: servers() #&gt; name #&gt; 1 Marine Domain Awareness (MDA) - Italy #&gt; 2 Marine Institute - Ireland #&gt; 3 CoastWatch Caribbean/Gulf of Mexico Node #&gt; 4 CoastWatch West Coast Node #&gt; 5 NOAA IOOS CeNCOOS (Central and Northern California Ocean Observing System) #&gt; 6 NOAA IOOS NERACOOS (Northeastern Regional Association of Coastal and Ocean Observing Systems) #&gt; 7 NOAA IOOS NGDAC (National Glider Data Assembly Center) #&gt; 8 NOAA IOOS PacIOOS (Pacific Islands Ocean Observing System) at the University of Hawaii (UH) #&gt; 9 NOAA IOOS SECOORA (Southeast Coastal Ocean Observing Regional Association) #&gt; 10 NOAA NCEI (National Centers for Environmental Information) / NCDDC #&gt; 11 NOAA OSMC (Observing System Monitoring Center) #&gt; 12 NOAA UAF (Unified Access Framework) #&gt; 13 ONC (Ocean Networks Canada) #&gt; 14 UC Davis BML (University of California at Davis, Bodega Marine Laboratory) #&gt; 15 R.Tech Engineering #&gt; 16 French Research Institute for the Exploitation of the Sea #&gt; url #&gt; 1 https://bluehub.jrc.ec.europa.eu/erddap/ #&gt; 2 http://erddap.marine.ie/erddap/ #&gt; 3 http://cwcgom.aoml.noaa.gov/erddap/ #&gt; 4 https://coastwatch.pfeg.noaa.gov/erddap/ #&gt; 5 http://erddap.axiomalaska.com/erddap/ #&gt; 6 http://www.neracoos.org/erddap/ #&gt; 7 http://data.ioos.us/gliders/erddap/ #&gt; 8 http://oos.soest.hawaii.edu/erddap/ #&gt; 9 http://129.252.139.124/erddap/ #&gt; 10 http://ecowatch.ncddc.noaa.gov/erddap/ #&gt; 11 http://osmc.noaa.gov/erddap/ #&gt; 12 https://upwell.pfeg.noaa.gov/erddap/ #&gt; 13 http://dap.onc.uvic.ca/erddap/ #&gt; 14 http://bmlsc.ucdavis.edu:8080/erddap/ #&gt; 15 http://meteo.rtech.fr/erddap/ #&gt; 16 http://www.ifremer.fr/erddap/index.html The second way to find and obtain the desired data is to use functions in rerddap. The basic steps are: Find the dataset on an ERDDAP server (rerddap::servers(), rerddap::ed_search(), rerddap::ed_datasets() ). Get the needed information about the dataset (rerddap::info() ) Think about what you are going to do. Make the request for the data (rerddap::griddap() or rerddap::tabledap() ). rerddap::tabledap() - Point datasets, like buoy data or weather station. Here is an example of this… whichBUOYS &lt;- rerddap::ed_search(query = &quot;buoy&quot;) ## Registered S3 method overwritten by &#39;hoardr&#39;: ## method from ## print.cache_info httr NOAA’s National Data Buoy Center (NDBC) collects world-wide data from buoys in the ocean. Let’s browse that dataset by entering this into the R console rerddap::browse('cwwcNDBCMet') That’s the ERDDAP site that we’re going to pull data from… library(rerddap) library(ggplot2) library(mapdata) info(&#39;cwwcNDBCMet&#39;) ## &lt;ERDDAP info&gt; cwwcNDBCMet ## Base URL: https://upwell.pfeg.noaa.gov/erddap/ ## Dataset Type: tabledap ## Variables: ## apd: ## Range: 0.0, 95.0 ## Units: s ## atmp: ## Range: -153.4, 50.0 ## Units: degree_C ## bar: ## Range: 800.7, 1198.8 ## Units: hPa ## dewp: ## Range: -99.9, 48.7 ## Units: degree_C ## dpd: ## Range: 0.0, 64.0 ## Units: s ## gst: ## Range: 0.0, 75.5 ## Units: m s-1 ## latitude: ## Range: -55.0, 71.758 ## Units: degrees_north ## longitude: ## Range: -177.75, 179.001 ## Units: degrees_east ## mwd: ## Range: 0, 359 ## Units: degrees_true ## ptdy: ## Range: -14.2, 14.0 ## Units: hPa ## station: ## tide: ## Range: -9.37, 6.13 ## Units: m ## time: ## Range: 4910400.0, 1.6115949E9 ## Units: seconds since 1970-01-01T00:00:00Z ## vis: ## Range: 0.0, 66.7 ## Units: km ## wd: ## Range: 0, 359 ## Units: degrees_true ## wspd: ## Range: 0.0, 96.0 ## Units: m s-1 ## wspu: ## Range: -98.7, 97.5 ## Units: m s-1 ## wspv: ## Range: -98.7, 97.5 ## Units: m s-1 ## wtmp: ## Range: -98.7, 50.0 ## Units: degree_C ## wvht: ## Range: 0.0, 92.39 ## Units: m BuoysInfo &lt;- info(&#39;cwwcNDBCMet&#39;) locationBuoys &lt;- tabledap(BuoysInfo, distinct = TRUE, fields = c(&quot;station&quot;, &quot;longitude&quot;, &quot;latitude&quot;), &quot;longitude&gt;=-76&quot;, &quot;longitude&lt;=-74&quot;, &quot;latitude&gt;=38&quot;, &quot;latitude&lt;=40&quot;) ## info() output passed to x; setting base url to: https://upwell.pfeg.noaa.gov/erddap/ locationBuoys = locationBuoys[1:4,] locationBuoys$latitude &lt;- as.numeric(locationBuoys$latitude) locationBuoys$longitude &lt;- as.numeric(locationBuoys$longitude) xlim &lt;- c(-76, -74) ylim &lt;- c(38, 40) coast &lt;- map_data(&quot;worldHires&quot;, ylim = ylim, xlim = xlim) ggplot() + geom_point(data = locationBuoys, aes(x = longitude , y = latitude, colour = factor(station) )) + geom_polygon(data = coast, aes(x = long, y = lat, group = group), fill = &quot;grey80&quot;) + theme_bw() + ylab(&quot;latitude&quot;) + xlab(&quot;longitude&quot;) + coord_fixed(1.3, xlim = xlim, ylim = ylim) + ggtitle(&quot;Delaware Bay Buoys&quot;) rerddap::griddap() - Gridded datasets, like sea surface temperature from GOES-R. whichSST &lt;- ed_search(query = &quot;SST&quot;) returns about 1000 responses. The more focused query: whichSST &lt;- ed_search(query = &quot;SST MODIS&quot;) still returns 172 responses. If the simple search doesn’t narrow things enough, look at the advanced search function ed_search_adv(). Let’s use this dataset - https://coastwatch.pfeg.noaa.gov/erddap/griddap/jplMURSST41.html info(&#39;jplMURSST41&#39;) ## &lt;ERDDAP info&gt; jplMURSST41 ## Base URL: https://upwell.pfeg.noaa.gov/erddap/ ## Dataset Type: griddap ## Dimensions (range): ## time: (2002-06-01T09:00:00Z, 2021-01-24T09:00:00Z) ## latitude: (-89.99, 89.99) ## longitude: (-179.99, 180.0) ## Variables: ## analysed_sst: ## Units: degree_C ## analysis_error: ## Units: degree_C ## mask: ## sea_ice_fraction: ## Units: 1 Alright, let’s extract some of this data and plot it. For this, we can use ‘last’ for the latest time instead of plugging in a time string. require(&quot;ggplot2&quot;) require(&quot;mapdata&quot;) require(&quot;rerddap&quot;) # grab the latest MUR SST info sstInfo &lt;- info(&#39;jplMURSST41&#39;) # get latest daily sst murSST &lt;- griddap(sstInfo, latitude = c(30, 45), longitude = c(-80., -70), time = c(&#39;last&#39;,&#39;last&#39;), fields = &#39;analysed_sst&#39;) ## info() output passed to x; setting base url to: https://upwell.pfeg.noaa.gov/erddap/ mycolor &lt;- colors$temperature w &lt;- map_data(&quot;worldHires&quot;, ylim = c(30, 45), xlim = c(-80., -70)) ggplot(data = murSST$data, aes(x = lon, y = lat, fill = analysed_sst)) + geom_polygon(data = w, aes(x = long, y = lat, group = group), fill = &quot;grey80&quot;) + geom_raster(interpolate = FALSE) + scale_fill_gradientn(colours = mycolor, na.value = NA) + theme_bw() + ylab(&quot;latitude&quot;) + xlab(&quot;longitude&quot;) + coord_fixed(1.3, xlim = c(-80, -70), ylim = c(30, 45)) + ggtitle(&quot;Latest MUR SST&quot;) What if we want to use a custom URL that’s not pre-loaded into rerddap ? Let’s use Ireland’s Marine Institute ERDDAP. Follow these steps: Go to http://erddap.marine.ie/erddap/ and select datasets Look to the far right column named Dataset ID Locate this ID - IMI_NEATL Select Graph under ‘Make a Graph’ Column for that Dataset ID Look at the variables - see sea_surface_temperature urlBase &lt;- &quot;http://erddap.marine.ie/erddap/&quot; parameter &lt;- &quot;sea_surface_temperature&quot; sstTimes &lt;- c(&quot;last&quot;, &quot;last&quot;) sstLats &lt;- c(48.00625, 57.50625) sstLons &lt;- c(-17.99375, -1.00625) xlim &lt;- c(-17.99375, -1.00625) ylim &lt;- c(48.00625, 57.50625) dataInfo &lt;- rerddap::info(&quot;IMI_NEATL&quot;, url = urlBase) NAtlsst &lt;- griddap(dataInfo, longitude = sstLons, latitude = sstLats, time = sstTimes, fields = parameter, url = urlBase) ## info() output passed to x; setting base url to: http://erddap.marine.ie/erddap/ str(NAtlsst$data) ## &#39;data.frame&#39;: 1034960 obs. of 4 variables: ## $ time : chr &quot;2021-01-25T00:00:00Z&quot; &quot;2021-01-25T00:00:00Z&quot; &quot;2021-01-25T00:00:00Z&quot; &quot;2021-01-25T00:00:00Z&quot; ... ## $ lat : num 48 48 48 48 48 ... ## $ lon : num -18 -18 -18 -18 -17.9 ... ## $ sea_surface_temperature: num 11.9 11.9 11.9 11.9 12.5 ... my.col &lt;- colors$temperature w &lt;- map_data(&quot;worldHires&quot;, ylim = ylim, xlim = xlim) myplot &lt;- ggplot() + geom_raster(data = NAtlsst$data, aes(x = lon, y = lat, fill = sea_surface_temperature), interpolate = FALSE) + geom_polygon(data = w, aes(x = long, y = lat, group = group), fill = &quot;grey80&quot;) + theme_bw() + scale_fill_gradientn(colours = my.col, na.value = NA, limits = c(5,15), name = &quot;Temperature&quot;) + ylab(&quot;latitude&quot;) + xlab(&quot;longitude&quot;) + coord_fixed(1.3, xlim = xlim, ylim = ylim, expand=FALSE) + ggtitle(paste(&quot;Sea Surface Temperature : &quot;, NAtlsst$data$time[1])) myplot 15.3 Assignment Deliverables: - R Code/Script - Images/Plots Select any dataset from the THREDDS server and create a script that loads in the data and plots it. In your script explain why you chose that dataset, detail each step and describe why it’s happening, Turn in your R script and the Image/Plot you’ve made. Select any dataset from the ERDDAP server and create a script that loads in the data and plots it. In your script explain why you chose that dataset, detail each step and describe why it’s happening, Turn in your R script and the Image/Plot you’ve made. 15.3.1 Extra Credit - 2 Points Find a dataset that uses TableDap’s instance of ERDDAP. This is a method for storing point data instead of spatial data. Create a routine that downloads and extracts data using the tabledap() function from rerddap "],["functions-and-code-presentation.html", "16 Functions and Code Presentation 16.1 Functions 16.2 NetCDF Processing Function 16.3 Functions in For Loops 16.4 RMarkdown 16.5 Assignment", " 16 Functions and Code Presentation In this tutorial, we’ll focus on functions and displaying R code &amp; results with style. Functions are important for code speed and documentation. Presenting R code is important if you need to explain how to do something using R (i.e. this tutorial). We’ll start by explaining functions and proceeding through a complex example. Afterwards, we’ll show how RMarkdown works so you can present a complex function of your own. 16.1 Functions We create functions to speed up and organize code. So far, we’ve only been coding things via scripting and occasionally using for loops. While this is fine, it’s slower than writing functions would be. Writing or creating functions is considered much cleaner and more proper. As a recap, whenever we use the phrase library(ncdf4) we are loading the ncdf4 package. Within that package are a number of .R scripts/files that contain functions that someone has been kind enough to write for the world. By using library(ncdf4), R reads in all of those functions and thus knows what we want when we execute a function like nc_open(). Instead of re-writing the pure function of nc_open() over and over again every time we want to open a netcdf file, we are able to just load the function. Before we get further into ncdf4, let’s write some R code to square a sequence of values. # square a sequence of values with start of 5 and stop of 10 arg1 = 1 arg2 = 5 my.seq = seq(arg1, arg2, 1) my.seq.squared = my.seq ** 2 my.seq.squared ## [1] 1 4 9 16 25 This code is straight-forward and light. Let’s now assume, however, that we want to run this code on multiple sets of start and stop values. If we turn the code above into a function, this is streamlined. # create a function called seq_squared seq_squared = function(arg1, arg2){ my.seq = seq(arg1, arg2, 1) my.seq.squared = my.seq ** 2 return(my.seq.squared) } seq_squared(arg1 = 5, arg2 = 10) ## [1] 25 36 49 64 81 100 Above we have a function that takes in start and stop values of a sequence, uses these values to make a sequence, and returns the squares of each of these values. We create a new function in R using the function R call and name it seq_squared. When we write a function of our own, we can add in our own arguments as we see fit. In this case, we create arguments named arg1 and arg2. These arguments are used to create my.seq which is a sequence within our xfunc function. We create a my.seq.squared variable which is a vector of my.seq values squared. Ultimately, my.seq.squared is the output of our function. It’s what we include in our return statement. Every function requires something to be returned, and my.seq.squared is the output of our seq_squared. 16.2 NetCDF Processing Function Loading in all of the weather data from Willow Creek, WI is cumbersome. Let’s write a function that reads in the data and converts everything to a dataframe for us. library(ncdf4) read.wcr = function(fname) { fullname = strsplit(fname, &quot;/&quot;) dataset_str = fullname[[1]][length(fullname[[1]])] datname = strsplit(dataset_str, &quot;_&quot;)[[1]][1] data.year = substr(dataset_str, nchar(dataset_str)-6, nchar(dataset_str)-3) data.date = seq(from=as.POSIXct(paste0(data.year,&quot;-1-1 0:00&quot;, tz=&quot;UTC&quot;)),to=as.POSIXct(paste0(data.year,&quot;-12-31 23:00&quot;, tz=&quot;UTC&quot;)),by=&quot;hour&quot;) vars.info &lt;- data.frame(CF.name = c(&quot;date&quot;, &quot;air_temperature&quot;, &quot;precipitation_flux&quot;, &quot;surface_downwelling_shortwave_flux_in_air&quot;, &quot;specific_humidity&quot;, &quot;surface_downwelling_longwave_flux_in_air&quot;, &quot;air_pressure&quot;, &quot;eastward_wind&quot;, &quot;northward_wind&quot;, &quot;wind_speed&quot;)) df &lt;- list() tem &lt;- ncdf4::nc_open(fname) dim &lt;- tem$dim for (j in seq_along(vars.info$CF.name)) { if (exists(as.character(vars.info$CF.name[j]), tem$var)) { df[[j]] &lt;- ncdf4::ncvar_get(tem, as.character(vars.info$CF.name[j])) } else { df[[j]] = NA } } names(df) &lt;- vars.info$CF.name df &lt;- data.frame(df) nc_close(tem) if(all(is.na(df$date))){ df$date = data.date } return(df) } In the function above, we’ve refactored code that we previously wrote to read in a Willow Creek netCDF file. read.wcr is our function name, the function() command tells R we’re declaring a function. fname is one of the arguments that the function takes. For example, the function what we use for opening netCDF files - nc_open() has the following arguments… nc_open( filename, write=FALSE, readunlim=TRUE, verbose=FALSE, auto_GMT=TRUE, suppress_dimvals=FALSE ) The function we’ve coded above only takes the filename, but if our routine was more complex we could add more arguments in. Once the argument is taken in, we generalize naming schemes that are unimportant. Notice how we use object names like tem for example. This is short for temporary, and is set up to be overwritten if we were to use this function within a for loop to open multiple files. Finally, focus in on the return(df) line - return is what we get out of a function. Now, let’s actually use the read.wcr() function we wrote above. wcr_df = read.wcr(fname = &quot;~/Documents/Github/geog473-673/datasets/WCr_1hr.2010.nc&quot;) summary(wcr_df) ## date air_temperature precipitation_flux ## Min. :2010-01-01 00:00:00 Min. :248.4 Min. :0.000e+00 ## 1st Qu.:2010-04-02 06:45:00 1st Qu.:270.5 1st Qu.:0.000e+00 ## Median :2010-07-02 12:30:00 Median :280.9 Median :0.000e+00 ## Mean :2010-07-02 12:30:00 Mean :279.8 Mean :2.564e-05 ## 3rd Qu.:2010-10-01 18:15:00 3rd Qu.:289.4 3rd Qu.:0.000e+00 ## Max. :2010-12-31 23:00:00 Max. :304.0 Max. :9.102e-03 ## surface_downwelling_shortwave_flux_in_air specific_humidity ## Min. : 0.00 Min. :0.0001864 ## 1st Qu.: 0.00 1st Qu.:0.0025622 ## Median : 0.00 Median :0.0051646 ## Mean : 78.04 Mean :0.0065146 ## 3rd Qu.: 12.62 3rd Qu.:0.0100803 ## Max. :1045.39 Max. :0.0258134 ## surface_downwelling_longwave_flux_in_air air_pressure eastward_wind ## Min. :139.7 Min. :91108 Min. :-10.6414 ## 1st Qu.:263.0 1st Qu.:94818 1st Qu.: -1.7298 ## Median :301.7 Median :95236 Median : -0.2718 ## Mean :304.7 Mean :95191 Mean : -0.2817 ## 3rd Qu.:351.6 3rd Qu.:95650 3rd Qu.: 1.2213 ## Max. :444.1 Max. :96918 Max. : 13.7162 ## northward_wind wind_speed ## Min. :-12.3305 Mode:logical ## 1st Qu.: -1.5300 NA&#39;s:8760 ## Median : -0.2183 ## Mean : -0.1984 ## 3rd Qu.: 1.0876 ## Max. : 6.5341 It’s as simple as that. This is why programming can become a black box and why we don’t always look under the hood. Code and functions like the one above can be messy and as long as we understand the output we don’t always care how it gets done. Remember that we have Willow Creek weather data for 2010, 2011, and 2012. Let’s set up a for loop for this to show how we might read in all 3 years of data. We’re going to use the rbind() function to latch on extra data here. 16.3 Functions in For Loops # create a sequence of years for the years we have data yearSeq = seq(2010,2012) # begin the for loop to open each year of data for (y in yearSeq){ # use our read.wcr() function and paste in the filename that changes for each year of data tem_df = read.wcr(fname = paste0(&quot;~/Documents/Github/geog473-673/datasets/WCr_1hr.&quot;,y,&quot;.nc&quot;)) # != means DOES NOT EQUAL. This statement reads as - if y DOES NOT EQUAL the first year in yearSeq, proceed to use rbind # otherwise, if y is indeed the first year in yearSeq, we need to initialize wcr_df BEFORE we overwrite tem_df if (y != yearSeq[1]){ wcr_df = rbind(wcr_df, tem_df) } else { wcr_df = tem_df } } length(wcr_df$date) ## [1] 26304 summary(wcr_df) ## date air_temperature precipitation_flux ## Min. :2010-01-01 00:00:00 Min. :245.0 Min. :0.000e+00 ## 1st Qu.:2010-10-02 00:45:00 1st Qu.:270.4 1st Qu.:0.000e+00 ## Median :2011-07-03 00:30:00 Median :280.0 Median :0.000e+00 ## Mean :2011-07-03 00:30:00 Mean :279.6 Mean :1.937e-05 ## 3rd Qu.:2012-04-02 00:15:00 3rd Qu.:289.4 3rd Qu.:0.000e+00 ## Max. :2012-12-31 23:00:00 Max. :305.5 Max. :9.102e-03 ## surface_downwelling_shortwave_flux_in_air specific_humidity ## Min. : 0.00 Min. :0.000001 ## 1st Qu.: 0.00 1st Qu.:0.002611 ## Median : 0.00 Median :0.004790 ## Mean : 79.62 Mean :0.006324 ## 3rd Qu.: 13.01 3rd Qu.:0.009615 ## Max. :1045.39 Max. :0.027334 ## surface_downwelling_longwave_flux_in_air air_pressure eastward_wind ## Min. :132.8 Min. :91108 Min. :-12.3726 ## 1st Qu.:266.0 1st Qu.:94838 1st Qu.: -1.7470 ## Median :304.8 Median :95287 Median : -0.3203 ## Mean :305.5 Mean :95222 Mean : -0.2160 ## 3rd Qu.:349.3 3rd Qu.:95676 3rd Qu.: 1.3130 ## Max. :465.8 Max. :97104 Max. : 13.7162 ## northward_wind wind_speed ## Min. :-12.3305 Mode:logical ## 1st Qu.: -1.7196 NA&#39;s:26304 ## Median : -0.3798 ## Mean : -0.4407 ## 3rd Qu.: 0.8609 ## Max. : 8.0738 Remember, != means DOES NOT EQUAL. This statement reads as - if y DOES NOT EQUAL the first year in yearSeq, proceed to use rbind(). Otherwise, if y is indeed the first year in yearSeq, we need to initialize wcr_df BEFORE we overwrite tem_df. Based on the summary we see that we have successfully merged the 3 datasets into one continuous dataset. Since we defined the function above, we don’t need to make a messy for loop with all of the function bits in it. Functions speed up our program and also make for more sophisticated coding. 16.4 RMarkdown Code is often messy and difficult to explain. One R feature that can make explaining code or a function easier is RMarkdown. This is how this textbook has been built. RMarkdown is a combination of HTML like syntax with R. Here is how you make an RMarkdown document. Open a new RMarkdown document - File -&gt; New File -&gt; R Markdown... Enter in details The initial RMarkdown should look like this We tell RMarkdown we want to write and run R code with 3 backticks and {r} - ```{r} We end our R code section with 3 backticks - ``` Thus, for our my.seq.squared function above, it would be coded like this in RMarkdown. ```{r} # create a function called seq_squared seq_squared = function(arg1, arg2){ my.seq = seq(arg1, arg2, 1) my.seq.squared = my.seq ** 2 return(my.seq.squared) } ``` In our example here, R code is keyed in the R Markdown document like so. Notice the cars inside of {r cars}. This names the coding section but has no impact on the code itself. We do not need to name each coding section - {r} alone will suffice. Anything outside of the R code sections will show up as plain text. When we are finished with our R Markdown document, we need to Knit the document. See the blue yarn ball at the top of your script? This will create a new PDF file. Et Voila! A professional looking RMarkdown PDF that makes your R code easier to explain and present. Here’s what the output above should look like 16.5 Assignment Create a function that extracts and plots a spatial dataset given a time range. Follow the steps below: Choose an online dataset like we did last week. Create a function that extracts the data given a time range, creates a data frame and plots the dataset. Include arguments within the function such as time range, spatial extents, etc. Create an RMarkdown PDF that details how the function works. Include an example of the function working to plot the given dataset Submit resulting PDF to Canvas week 7 assignment "],["machine-learning-with-r.html", "17 Machine Learning with R 17.1 Machine Learning Terms 17.2 Classical Machine Learning", " 17 Machine Learning with R Machine Learning is data science technique where we predict results based on incoming data. Machine Learning is behind the recommended videos for you to watch on YouTube, the recommended products for you to purchase on Amazon, and the targeted advertisements you see on search engine results. Huge companies use data from our browsing history or purchase history to evaluate trends and create predictions on what product a consumer will want to buy next - this is machine learning in action.Various tasks for machine learning include data concepts learning, data function learning, or predictive modeling which involves the identification of predictive patterns. Each of these tasks are learned from an initial dataset. Algorithms, or simply R code with a purpose, can be used to observe patterns, trends, experiences, and/or instructions from this initial dataset. The goal of machine learning is to use these initial instructions to create advanced insights and improve algorithms. As humans, we’re constantly undergoing subconscious machine learning. We recognize patterns in our daily lives and use this data to form a more accurate analysis or form a better prediction model for ourselves. Let’s explain machine learning with an example regarding mathematics in school. Students in math classes are often given practice problems to train their mathematic ability. Often times, a math concept is taught to the students and then students use this concept to help solve practice problems. Let’s assume that for this example, the math concept is not taught. Instead, only practice problems and their associated answers are given. Each practice problem encodes pieces of information that a student observes alongside the answer. We can call this learning process the training of an algorithm. After many practice problems, we expect that the students would have identified a pattern they can use to help solve the problem. So a test is given to the student and we compare their results to the answer key. Evaluating the accuracy of the students answers gives us a measure of effectiveness for both the student and the set of practice problems. To summarize, machine learning algorithms are like math students who have been given vast amounts of practice problems and instructed to find methods for solving them by finding patterns between the math problems and their associated answers. Machine Learning aims to understand the concepts or the formulas behind datasets. Computers are much better at crunching numbers than us and this is why we use machine learning. 17.1 Machine Learning Terms Machine Learning is a subject within a broader knowledge category known as artificial intelligence. Machine learning has 3 components associated with it - data, features, and algorithms. These components are present within every sub-category or technique of Machine Learning. Let’s start from the top and describe some important terms as simply as possible. Artificial Intelligence Intelligence demonstrated by machines. This is the name of the knowledge field machine learning lies in, similar to physics in the science knowledge field. Machine Learning Data science technique aimed to predict results based on data. One category of artificial intelligence. Data A collection of observations or informative metrics used to practice machine learning. Many times we will encounter training datasets - used to train an algorithm, and test datasets - datasets used to test an algorithm. In our math example above, training datasets are the practice problems, test datasets are the exam given to the students. Training Data Dataset that is used to train a machine learning algorithm. The algorithm aims to find patterns within this dataset that it can then use to make predictions. In our math example above, the training dataset is the set of practice problems the students are given initially. Test Data Dataset that is used to test an algorithm. In the math example above, the test dataset is the exam given to the students to check their comprehension. Features Variables/columns of a dataset that act as input for a machine learning model or algorithm. Algorithms A process, set of functions, or calculation aimed to solve problems. In our case, we’ll develop machine learning algorithms via R code. Algorithm is a fancy term that gets used as a buzzword but we’ve developed code in this course that are technically algorithms, just not with the purpose of machine learning. Model A mathematical representation of a real-world process that are built with algorithms and training data. Models are the entire process from inputting data, to training an algorithm, to analyzing output. Label/Output The final output of a machine learning process which can be output classes such as students who got an A on a math exam, students who got a B, etc. Regression Machine learning technique used on continuous data such as time series data. This technique involves fitting a line. We’ve fit regression lines to time-series data before with the Willow Creek Air Temperature dataset. In machine learning, regression would be used to predict future temperatures based on the dataset we’ve initially used. Classification Categorizing data into groups or predefined classes. For example, we could classify a particular lat/lon point as land or ocean. Another example would be classifying fruits and vegetables. Generalization The entire point of machine learning. A model is trained to utilize patterns learned from a task to solve a similar task with slight variations. For example, a consumer product model may be used to suggest next purchases for that consumer. Let’s say a consumer buys a basketball so the machine learning model recommends basketball shoes, as consumers who bought basketballs prior have also purchased basketball shoes. Generalization is the act of performing tasks of the same difficulty and nature. This may also be referred to as interpolation, although generalization is a more commonly used and understood term. Extrapolation Extrapolation, on the other hand, is when the model is able to obtain higher-dimensional insights from a lower-dimensional training. Extrapolation is a more difficult task. Weather models use extrapolation to predict the future based on current values. Overfitting A term used when a machine learning model fits the training dataset too well. A model is overfitting if it fits the training data too well and there is a poor generalization of new data. Underfitting A machine learning model that fits a training dataset poorly. Parameter A function argument for algorithms. For example, in a climate change prediction model, a parameter is carbon dioxide emission scenario. Ensemble A member of a collection of varying output data. Ensembles are used to show variance of output within a model. For example, algorithms often have calculations that have some variance associated with them - in other words, we have a range of possibilities. Ensembles are used to show that output can vary despite having the same input data and algorithm. Decision Tree A method for finding answers by asking questions with binary answers and proceeding accordingly. Feature Engineering Feature engineering refers to a process of selecting and transforming variables when creating a predictive model using machine learning such as random forest or regression. 17.2 Classical Machine Learning Classical machine learning has actually been around since the 1950s. These machine learning models were built on algorithms that solved formal math tasks — searching for patterns in numbers, evaluating the proximity of data points, and calculating vectors’ directions. These same algorithms are used today by social media companies (Instagram), e-commerce companies (Amazon), and credit card companies (Chase). Ever wonder how targeted ads on Instagram, Facebook, or Google work? This is classical machine learning in action. Classical machine learning is often divided into two categories – Supervised and Unsupervised Learning. 17.2.1 Supervised Learning In supervised learning, we have clear training data that serves as the “Superviser” or “Teacher” who gives the machine all the answers. In our math example, the students were given the answers with the practice problem (training dataset) by the Superviser. As another example, let’s say we wish to train an algorithm to identify a latitude and longitude point as land or ocean. In order to teach the algorithm, we would provide it with the answer (ocean or land) of a particular point so it can learn. There are two primary types of supervised learning - classification and regression. 17.2.1.1 Classification Categorizes objects based on one of the attributes beforehand. For example, separate music by genre, highlight video by sport, student by grade, etc. We’ve all seen same-day loan services through commercials or billboards before. Some of these loan services today don’t involve human interaction. How can machine learning be used to decide whether someone should be given a loan or not? The loan company has lots of profiles of people who took money before. They have data about age, education, occupation and salary and – most importantly – the fact of paying the money back. Or not. They use this data as input for a machine learning algorithm that decides whether to give an applicant a loan or not. 17.2.1.2 Regression Regression is basically classification where we forecast a number instead of category. Examples are temperature by time of day, traffic by time of the day, housing prices by year, etc. Regression is perfect when something depends on time. In the image above we draw a line through some data points - yes, this is machine learning. 17.2.2 Unsupervised Learning Unsupervised learning is a technique that was created out of bad data. Labeled data with variables/columns is not always available and this is where unsupervised learning can come in and help. Exploratory data analysis can be used to aid unsupervised learning techniques. Two of the more popular unsupervised learning techniques include clustering, dimension reduction, and association. 17.2.2.1 Clustering Divides objects based on unknown features and let the machine choose the best way. Clustering is done with creating a gridded dataset from a point dataset. For example, with weather modeling, clustering is a step that is used to spatially interpolate points to a structured grid. Popular clustering algorithms include K-means clustering, mean-shift, and DBSCAN. 17.2.2.2 Dimension Reduction Assembles specific features into more high-level ones. For example, we can merge all dogs with triangle ears, long noses, and big tails to a nice abstraction — “shepherd”. Yes, we’re losing some information about the specific shepherds, but the new abstraction is much more useful for naming and explaining purposes. As a bonus, such “abstracted” models learn faster, overfit less and use a lower number of features. Principal Component Analysis (PCA) and Singular Value Decomposition (SVD) are often used in Earth Science machine learning applications. 17.2.2.3 Association Look for a pattern based on consumer purchases. Basketball purchases are sometimes followed by basketball shoe purchases. Recommend basketball shoe purchases to the consumer! 17.2.3 Neural Networks Any neural network is basically a collection of neurons and connections between them. Neuron is a function with a bunch of inputs and one output. Its task is to take all numbers from its input, perform a function on them and send the result to the output. Here is an example of a simple but useful in real life neuron: sum up all numbers from the inputs and if that sum is bigger than N — give 1 as a result. Otherwise — zero. Connections are like channels between neurons. They connect outputs of one neuron with the inputs of another so they can send digits to each other. Each connection has only one parameter — weight. It’s like a connection strength for a signal. When the number 10 passes through a connection with a weight 0.5 it turns into 5. These weights tell the neuron to respond more to one input and less to another. Weights are adjusted when training — that’s how the network learns. Basically, that’s all there is to it. 17.2.4 Reinforcement Learning This type of machine learning is used by self driving cars and robot vacuums. Reinforcement learning is used in cases when your problem is not related to data at all, but you have an environment to live in. The algorithm learns based on the reactions of an action within an environment. The images shown in this tutorial were taken from Vasily Zubarev who wrote an excellent machine learning concepts manual. Vasily Zubarev is the best Machine Learning educator I’ve come across and I suggest poking around his website for more Machine Learning content. "],["time-series-forecasting.html", "18 Time Series Forecasting 18.1 Load the Packages and Data 18.2 Aggregate Data 18.3 Prepping the Model 18.4 Running the Model 18.5 Validation and Accuracy 18.6 Future Forecasting 18.7 Assignment", " 18 Time Series Forecasting Time series forecasting is widely used in sciences with the intended purpose of predicting the future based on present data. This type of machine learning technique has been used for weather forecasting, glacier melting outlooks, and carbon dioxide concentration predictions. In this tutorial, we will use the Willow Creek Weather Datasets and build an Air Temperature Forecast using Linear Regression Modeling. The goal of this exercise is to build a linear regression model using existing training data to accurately predict existing testing data. This is an isolated tutorial that does not account for other variables (precipitation, shortwave radiation, etc.) that may have an influence on Air Temperature. While it may not serve as the best predictive model for air temperature, it serves as a first step in linear regression modeling and machine learning. 18.1 Load the Packages and Data Let’s use the read.wcr function that we previously created to load in our dataset. library(ncdf4) read.wcr = function(fname) { fullname = strsplit(fname, &quot;/&quot;) dataset_str = fullname[[1]][length(fullname[[1]])] datname = strsplit(dataset_str, &quot;_&quot;)[[1]][1] data.year = substr(dataset_str, nchar(dataset_str)-6, nchar(dataset_str)-3) data.date = seq(from=as.POSIXct(paste0(data.year,&quot;-1-1 0:00&quot;, tz=&quot;UTC&quot;)),to=as.POSIXct(paste0(data.year,&quot;-12-31 23:00&quot;, tz=&quot;UTC&quot;)),by=&quot;hour&quot;) vars.info &lt;- data.frame(CF.name = c(&quot;date&quot;, &quot;air_temperature&quot;, &quot;precipitation_flux&quot;, &quot;surface_downwelling_shortwave_flux_in_air&quot;, &quot;specific_humidity&quot;, &quot;surface_downwelling_longwave_flux_in_air&quot;, &quot;air_pressure&quot;, &quot;eastward_wind&quot;, &quot;northward_wind&quot;, &quot;wind_speed&quot;)) df &lt;- list() tem &lt;- ncdf4::nc_open(fname) dim &lt;- tem$dim for (j in seq_along(vars.info$CF.name)) { if (exists(as.character(vars.info$CF.name[j]), tem$var)) { df[[j]] &lt;- ncdf4::ncvar_get(tem, as.character(vars.info$CF.name[j])) } else { df[[j]] = NA } } names(df) &lt;- vars.info$CF.name df &lt;- data.frame(df) nc_close(tem) if(all(is.na(df$date))){ df$date = data.date } return(df) } Now, let’s throw the read.wcr function into a loop so we can read in the data for each year. # create a sequence of years for the years we have data yearSeq = seq(2010,2012) # begin the for loop to open each year of data for (y in yearSeq){ # use our read.wcr() function and paste in the filename that changes for each year of data tem_df = read.wcr(fname = paste0(&quot;~/Documents/Github/geog473-673/datasets/WCr_1hr.&quot;,y,&quot;.nc&quot;)) # != means DOES NOT EQUAL. This statement reads as - if y DOES NOT EQUAL the first year in yearSeq, proceed to use rbind # otherwise, if y is indeed the first year in yearSeq, we need to initialize wcr_df BEFORE we overwrite tem_df if (y != yearSeq[1]){ wcr_df = rbind(wcr_df, tem_df) } else { wcr_df = tem_df } } length(wcr_df$date) ## [1] 26304 head(wcr_df) ## date air_temperature precipitation_flux ## 1 2010-01-01 00:00:00 260.1135 0 ## 2 2010-01-01 01:00:00 260.0315 0 ## 3 2010-01-01 02:00:00 259.9805 0 ## 4 2010-01-01 03:00:00 259.9005 0 ## 5 2010-01-01 04:00:00 259.7585 0 ## 6 2010-01-01 05:00:00 259.6825 0 ## surface_downwelling_shortwave_flux_in_air specific_humidity ## 1 0 0.001176792 ## 2 0 0.001170627 ## 3 0 0.001167514 ## 4 0 0.001160727 ## 5 0 0.001142952 ## 6 0 0.001133232 ## surface_downwelling_longwave_flux_in_air air_pressure eastward_wind ## 1 237.6100 95704.5 1.522976 ## 2 241.6900 95735.5 1.624952 ## 3 240.3500 95765.5 1.696362 ## 4 240.9325 95795.5 1.493329 ## 5 247.3875 95824.5 1.921457 ## 6 246.5075 95853.0 1.963118 ## northward_wind wind_speed ## 1 -1.462524 NA ## 2 -1.563508 NA ## 3 -1.553177 NA ## 4 -1.702995 NA ## 5 -1.460475 NA ## 6 -1.667144 NA Our data has been read in. Let’s convert the Air Temperature units from degrees Kelvin to degrees Fahrenheit. Then, let’s plot our time series. library(ggplot2) library(lubridate) # convert the data to Fahrenheit wcr_df$air_temperature = (wcr_df$air_temperature - 273.15) * (9/5) + 32 ggplot(data = wcr_df, aes(x = date, y = air_temperature)) + geom_rect(xmin = as.POSIXct(&quot;2010-01-01&quot;), xmax = as.POSIXct(&quot;2012-06-01&quot;), ymin = -10, ymax = 100, fill = &quot;lightblue&quot;, alpha = 0.1) + annotate(&quot;text&quot;, x = as.POSIXct(&quot;2011-01-01&quot;) , y = 80, color = &quot;blue&quot;, label = &quot;Train Region&quot;) + annotate(&quot;text&quot;, x = as.POSIXct(&quot;2012-12-01&quot;), y = 80, color = &quot;coral&quot;, label = &quot;Test Region&quot;) + geom_point(size=0.5, alpha = 0.5, color = &quot;black&quot;) + labs(title = &quot;Willow Creek Hourly Air Temperature - 2010-2013&quot;,y = &quot;Air Temperature (F)&quot;, x = &quot;&quot;) 18.2 Aggregate Data Let’s aggregate our dataset from an hourly resolution to a daily resolution. This can be done using the xts (a time series library) library which has a convenient function called apply.daily() that averages the default resolution data to a daily resolution. This will enhance speed but also decrease accuracy as we are removing data. In real practice and with proper computer resources we would rather stick with our original hourly resolution. library(xts) library(zoo) # create time series data frame based on our datetime xt = xts(wcr_df, order.by = wcr_df$date) # aggregate to daily resolution daily = xts::apply.daily(xt,mean) # recreate the date variable which has been deemed as the index of our daily dataframe - while it is the index, it&#39;s easier to use if it&#39;s a variable. daily = data.frame(date=index(daily), coredata(daily)) # remove artifact of daily aggregation daily$date.1 = NULL # plot the new daily air temperature ggplot(data = daily, aes(x = date, y = air_temperature)) + geom_rect(xmin = as.POSIXct(&quot;2010-01-01&quot;), xmax = as.POSIXct(&quot;2012-06-01&quot;), ymin = -10, ymax = 85, fill = &quot;lightblue&quot;, alpha = 0.1) + annotate(&quot;text&quot;, x = as.POSIXct(&quot;2011-01-01&quot;) , y = 70, color = &quot;blue&quot;, label = &quot;Train Region&quot;) + annotate(&quot;text&quot;, x = as.POSIXct(&quot;2012-12-01&quot;), y = 70, color = &quot;coral&quot;, label = &quot;Test Region&quot;) + geom_point(alpha = 0.5, color = &quot;black&quot;) + labs(title = &quot;Willow Creek Daily Air Temperature - 2010-2013&quot;,y = &quot;Air Temperature (F)&quot;, x = &quot;&quot;) 18.3 Prepping the Model Now that we have our data in daily resolution, we can split our dataset into 2; a test dataset, and a train dataset. When we build a linear regression model for a time series dataset, we select a training period and a testing period. For our case, we’re going to train the dataset with data from 2010-01-01 to 2012-05-31. In other words, we’re using 5/6ths of our dataset to train the model. Then, we’ll use the remaining 1/6th (2012-06-01 to 2013-01-01) to test the model to see how well it predicts the temperature using linear regression. train = daily[1:which(daily$date == as.POSIXct(&quot;2012-05-31 23:00:00 EDT&quot;)),] test = daily[which(daily$date == as.POSIXct(&quot;2012-06-01 23:00:00 EDT&quot;)):length(daily$date),] train = data.frame(date = train$date, air_temperature = train$air_temperature) test = data.frame(date = test$date, air_temperature = test$air_temperature) # print out the results of our split head(train) ## date air_temperature ## 1 2010-01-01 23:00:00 4.352076 ## 2 2010-01-02 23:00:00 -4.916312 ## 3 2010-01-03 23:00:00 4.264553 ## 4 2010-01-04 23:00:00 9.920745 ## 5 2010-01-05 23:00:00 11.206665 ## 6 2010-01-06 23:00:00 12.708645 head(test) ## date air_temperature ## 1 2012-06-01 23:00:00 54.73873 ## 2 2012-06-02 23:00:00 59.44044 ## 3 2012-06-03 23:00:00 63.61205 ## 4 2012-06-04 23:00:00 64.61727 ## 5 2012-06-05 23:00:00 61.49611 ## 6 2012-06-06 23:00:00 62.09221 We have our test and train datasets, but now we must prepare these datasets for the linear regression model. Remember, we are trying to train the model using the train dataset and use that model to accurately predict the temperature values of the test dataset. We’ll need to provide the model with some information regarding the number of variables that go into the model. library(timetk) library(recipes) # create a recipe for the air temperature variable rec_train = recipe(air_temperature ~ ., data = train) recipe_ts = step_timeseries_signature(recipe = rec_train, &quot;date&quot;) recipe_ts ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 1 ## ## Operations: ## ## Timeseries signature features from &quot;date&quot; We only have 1 predictor (day of year) and 1 outcome variable (air temperature). When we initiate a linear regression model for a time series, the model will add extra columns (second, minute, hour, day of year, week of year, weekday, etc.) to the training dataset. Remember, we’re creating a recipe that holds instructions for the actual model. We can add recipe instructions on whether to evaluate the interaction of these columns and Air Temperature or not. We’ll remove seconds, minutes, hours since we are at a daily resolution. These instructions are used on the training dataframe while it’s training the particular model but does not impact the actual dataframe itself. recipe_spec = recipe_ts # when we run the model, extra dat values will be added automatically - so let&#39;s remove our default date column when teh model is run - note this does not change the dataframe recipe_spec = step_rm(recipe_spec, date) # some other variables will be added in automatically as well, let&#39;s remove the ones that aren&#39;t imoprtant. recipe_spec = step_rm(recipe_spec, contains(&quot;iso&quot;), contains(&quot;second&quot;), contains(&quot;minute&quot;), contains(&quot;hour&quot;), contains(&quot;am.pm&quot;), contains(&quot;xts&quot;)) # create new columns based on the interaction of these variables recipe_spec = step_interact(recipe_spec, ~ date_month.lbl * date_day) recipe_spec = step_interact(recipe_spec, ~ date_month.lbl * date_mweek) recipe_spec = step_interact(recipe_spec, ~ date_month.lbl * date_wday.lbl * date_yday) recipe_spec_final = step_dummy(recipe_spec, contains(&quot;lbl&quot;), one_hot = TRUE) recipe_spec_final ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 1 ## ## Operations: ## ## Timeseries signature features from &quot;date&quot; ## Delete terms date ## Delete terms contains(&quot;iso&quot;), contains(&quot;second&quot;), ... ## Interactions with date_month.lbl * date_day ## Interactions with date_month.lbl * date_mweek ## Interactions with date_month.lbl * date_wday.lbl * date_yday ## Dummy variables from contains(&quot;lbl&quot;) Our instructions have been specified, now let’s use the linear_reg function from the parsnip package to create the instance. We’ll need to set an engine to run the model (there are multiple regression model engines that are beyond the scope of this tutorial) and that engine will be glmnet. We’ll initiate the model, create a workflow for the model where we add in our prewritten recipe above, then we’ll add our model to the workflow. Once it’s ready, we can run the model and it’s workflow using the fit function. Our model has been trained. Now, we can use our trained model to predict the test air temperature based on the test date. 18.4 Running the Model library(workflows) library(parsnip) library(recipes) library(yardstick) library(glmnet) library(tidyverse) # initiate the model with a penalty of 2 and no mixture - these parameters can be changed for more variability model_spec_glmnet &lt;- parsnip::linear_reg(mode = &quot;regression&quot;, penalty = 2, mixture = 0) set_engine(model_spec_glmnet, &quot;glmnet&quot;) ## Linear Regression Model Specification (regression) ## ## Main Arguments: ## penalty = 2 ## mixture = 0 ## ## Computational engine: glmnet # add our recipe to the workflow, then add the model to the workflow workflow_glmnet = add_recipe(workflow(), recipe_spec_final) workflow_glmnet = add_model(workflow_glmnet, model_spec_glmnet) workflow_glmnet ## ══ Workflow ═════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: linear_reg() ## ## ── Preprocessor ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## 7 Recipe Steps ## ## ● step_timeseries_signature() ## ● step_rm() ## ● step_rm() ## ● step_interact() ## ● step_interact() ## ● step_interact() ## ● step_dummy() ## ## ── Model ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Linear Regression Model Specification (regression) ## ## Main Arguments: ## penalty = 2 ## mixture = 0 # tune the model based on the training data workflow_trained &lt;- workflow_glmnet workflow_trained = fit(workflow_glmnet, data = train) # predict the test air temperature based on our model prediction_tbl = predict(workflow_trained, test) prediction_tbl = bind_cols(prediction_tbl, test) prediction_tbl ## # A tibble: 214 x 3 ## .pred date air_temperature ## &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 64.5 2012-06-01 23:00:00 54.7 ## 2 65.8 2012-06-02 23:00:00 59.4 ## 3 67.1 2012-06-03 23:00:00 63.6 ## 4 72.4 2012-06-04 23:00:00 64.6 ## 5 77.6 2012-06-05 23:00:00 61.5 ## 6 69.0 2012-06-06 23:00:00 62.1 ## 7 64.6 2012-06-07 23:00:00 66.3 ## 8 69.1 2012-06-08 23:00:00 68.3 ## 9 69.6 2012-06-09 23:00:00 74.9 ## 10 65.3 2012-06-10 23:00:00 73.8 ## # … with 204 more rows # plot our modeled test air temperature against the actual test air temperature ggplot(data = daily, aes(x = date, y = air_temperature)) + geom_rect(xmin = as.POSIXct(&quot;2010-01-01&quot;), xmax = as.POSIXct(&quot;2012-06-01&quot;), ymin = -10, ymax = 85, fill = &quot;lightblue&quot;, alpha = 0.01) + annotate(&quot;text&quot;, x = as.POSIXct(&quot;2011-01-01&quot;) , y = 75, color = &quot;blue&quot;, label = &quot;Train Region&quot;) + annotate(&quot;text&quot;, x = as.POSIXct(&quot;2012-12-01&quot;), y = 75, color = &quot;coral&quot;, label = &quot;Test Region&quot;) + geom_point(alpha = 0.5, color = &quot;black&quot;) + geom_point(aes(x = date, y = .pred), data = prediction_tbl, alpha = 0.5, color = &quot;darkblue&quot;) + labs(title = &quot;GLM: Out-Of-Sample Forecast&quot;) 18.5 Validation and Accuracy The plot seems to show that the model predicted the test air temperature fairly well, but let’s evaluate the metrics the see how it does based on the statistics. Then, let’s plot the residuals - the difference between the actual test air temperature and the predicted air temperature. # calculate the metrics of test air temperature vs. predicted air temperature metrics(prediction_tbl, air_temperature, .pred) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 11.2 ## 2 rsq standard 0.778 ## 3 mae standard 8.89 # plot the residuals ggplot(data = prediction_tbl, aes(x = date, y = air_temperature - .pred)) + geom_hline(yintercept = 0, color = &quot;black&quot;) + geom_point(color = &quot;blue&quot;, alpha = 0.5) + geom_smooth(span = 0.05, color = &quot;red&quot;) + geom_smooth(span = 1.00, se = FALSE) + labs(title = &quot;GLM Model Residuals, Out-of-Sample&quot;, y = &quot;Air Temperature: Actual - Predicted&quot;, x = &quot;&quot;) + scale_y_continuous(limits = c(-5, 5)) Remember, we don’t want to overfit our model and make it too accurate! Overfitting of our model to the training dataset can create unrealistic outcomes with unrealistic regularity. Temperatures are variable! 18.6 Future Forecasting The residuals show the model performed fairly well. Now, can we forecast the temperature into the future? We’ll follow the same procedure as before but now we’ll create extra time series values and fill those using the model we’ve built # Extract daily index idx &lt;- tk_index(daily) # Get time series summary from index daily_summary &lt;- tk_get_timeseries_summary(idx) # add 180 days to the model idx_future &lt;- tk_make_future_timeseries(idx, n_future = 180) # create a tibble table future_tbl &lt;- tibble(date = idx_future) head(future_tbl) ## # A tibble: 6 x 1 ## date ## &lt;dttm&gt; ## 1 2013-01-01 00:00:00 ## 2 2013-01-02 00:00:00 ## 3 2013-01-03 00:00:00 ## 4 2013-01-04 00:00:00 ## 5 2013-01-05 00:00:00 ## 6 2013-01-06 00:00:00 # run the model using the entire dataset, predict the temperature for the dates of future_tbl, and bind the columns together. future_predictions_tbl = fit(workflow_glmnet, data = daily) future_predictions_tbl = predict(future_predictions_tbl, future_tbl) future_predictions_tbl = bind_cols(future_predictions_tbl, future_tbl) # plot the future forecasted temperature ggplot(data = daily, aes(x = date, y = air_temperature)) + geom_rect(xmin = as.POSIXct(&quot;2010-01-01&quot;), xmax = as.POSIXct(&quot;2012-06-01&quot;), ymin = -10, ymax = 85, fill = &quot;lightblue&quot;, alpha = 0.01) + annotate(&quot;text&quot;, x = as.POSIXct(&quot;2011-01-01&quot;) , y = 75, color = &quot;blue&quot;, label = &quot;Train Region&quot;) + annotate(&quot;text&quot;, x = as.POSIXct(&quot;2012-12-01&quot;), y = 75, color = &quot;coral&quot;, label = &quot;Test Region&quot;) + geom_point(alpha = 0.5, color = &quot;black&quot;) + geom_point(aes(x = as.POSIXct(date), y = .pred), data = future_predictions_tbl, alpha = 0.5, color = &quot;dodgerblue1&quot;) + geom_smooth(aes(x = as.POSIXct(date), y = .pred), data = future_predictions_tbl, method = &#39;loess&#39;, color = &quot;dodgerblue4&quot;) + labs(title = &quot;Willow Creek Air Temperature: 6-Month Forecast&quot;, y = &quot;Air Temperature (F)&quot;, x = &quot;&quot;) Let’s add in some error regions. We’ll calculate the spread of the 95th percentile and the 80th percentile confidence intervals. 95% of the area under a normal curve lies within roughly 1.96 standard deviations of the mean, and due to the central limit theorem, this number is therefore used in the construction of approximate 95% confidence intervals. For 80%, the are under a normal curve lies within roughly 1.28 standard deviations. We’ll use these values to create our prediction error thresholds. # Calculate standard deviation of residuals (actual temp subtracted from residuals) test_resid_sd &lt;- summarize(prediction_tbl, stdev = sd(air_temperature- .pred)) # create lo.95 and other variables for the error areas future_predictions_tbl &lt;- mutate(future_predictions_tbl, lo.95 = .pred - 1.96 * test_resid_sd$stdev, lo.80 = .pred - 1.28 * test_resid_sd$stdev, hi.80 = .pred + 1.28 * test_resid_sd$stdev, hi.95 = .pred + 1.96 * test_resid_sd$stdev) # plot the error areas ggplot(data = daily, aes(x = date, y = air_temperature)) + geom_point(alpha = 0.5, color = &quot;black&quot;) + geom_ribbon(aes(y = .pred, x = as.POSIXct(date),ymin = lo.95, ymax = hi.95), data = future_predictions_tbl, fill = &quot;#D5DBFF&quot;, color = NA, size = 0) + geom_ribbon(aes(y = .pred, x = as.POSIXct(date),ymin = lo.80, ymax = hi.80, fill = key), data = future_predictions_tbl, fill = &quot;#596DD5&quot;, color = NA, size = 0, alpha = 0.8) + geom_point(aes(x = as.POSIXct(date), y = .pred), data = future_predictions_tbl, alpha = 0.5, color = &quot;red&quot;) + geom_smooth(aes(x = as.POSIXct(date), y = .pred), data = future_predictions_tbl, method = &#39;loess&#39;, color = &quot;white&quot;)+ labs(title = &quot;Willow Creek Daily Air Temperature: 6 Month Forecast&quot;,y = &quot;Air Temperature (F)&quot;, x = &quot;&quot;) Remember, no forecast is ever perfect! 18.7 Assignment Perform the same analysis for the Willow Creek Air Temperature (2010,2011, and 2012 datasets) but this time aggregate to a weekly resolution using the apply.weekly() function from xts. Proceed with the same workflow and create the same plots. Add plots to a document (Word, Google Docs, or RMarkdown file, your choice) and answer the following questions; How does the weekly resolution model compare to the daily resolution model? Why do you think it performs better/worse? How could we enhance the accuracy of the model? Upload file (as word doc, PDF, whatever works) to UD Canvas. "],["random-forest-modeling.html", "19 Random Forest Modeling 19.1 Prepare the Data 19.2 Dimensionality Reduction 19.3 Random Forests 19.4 Assignment", " 19 Random Forest Modeling In this tutorial, we’ll explore feature engineering, training and test splitting, and model selecting with random forests. We’ll introduce the caret package, a popular R package with machine learning tools. The dataset we’ll be using is called Bejaia_ForestFires.csv and contains information regarding forest fire conditions in Algeria. We are going to use machine learning practices to extract trends from the dataset and attempt to predict whether a forest fire will occur on a particular day given environmental conditions. Here are the variables within the dataset: Weather Observations Date Day, month (‘june’ to ‘september’) for the year 2012 Temp Max daily temperature in degrees Celsius: 22 to 42 RH Relative Humidity in %: 21 to 90 Ws Wind speed in km/h: 6 to 29 Rain Total daily precip in mm: 0 to 16.8 Fire Weather Indices FFMC Fine Fuel Moisture Code index from the FWI system: 28.6 to 92.5 DMC Duff Moisture Code index from the FWI system: 1.1 to 65.9 DC Drought Code index from the FWI system: 7 to 220.4 ISI Initial Spread Index from the FWI system: 0 to 18.5 BUI Buildup Index from the FWI system: 1.1 to 68 FWI Class Weather Index : 0 to 31.1 Class Component Class : Forest Fire presence (fire) or absence (no fire) 19.1 Prepare the Data The Bejaia_ForestFires.csv dataset is located in the course datasets folder. Let’s open it and plot the Forest Fire presence/absence (the Class variable). library(ggplot2) library(tibble) ff_data = read.csv(&quot;/Users/james/Documents/Github/geog473-673/datasets/Bejaia_ForestFires.csv&quot;) ff_data = as.tibble(ff_data) # response variable used for classification ggplot(ff_data, aes(x = Class, fill = Class)) + geom_bar() Now, let’s use the gather function from the tidyverse to evaluate each variable against the Class (aka Fire or No Fire). Once we have our gathered data frame, let’s show a density plot for each variable. library(tidyverse) gathered = gather(ff_data, x, y, day:FWI) ggplot(data = gathered, aes(x = y, color = Class, fill = Class)) + geom_density(alpha = 0.3) + facet_wrap( ~ x, scales = &quot;free&quot;, ncol = 3) 19.2 Dimensionality Reduction Sometimes datasets contain too many variables and evaluating which are important can be difficult. Fortunately, there are statistical methods we can use in R to reduce the number of dimensions to retain only the high impact variables. 19.2.1 Principal Component Analysis Principal Component Analysis (PCA) is a popular statistical method that uses correlations and covariances between variables to reduce data that contains many variables. PCA projects the data with fewer dimensions (aka the principal components) using linear combinations across the dataset variables. The dimensions (principal components) are ordered from most explained variance to least explained variance (of the original variables). Principal component analysis can also reveal important features of the data such as outliers and departures from a multinormal distribution. First, we’ll need to cut out the Fire Weather Indices of our dataset. These are calculated based on environmental conditions and are used to help predict forest fires. We’re going to remove these and attempt to create new methods for predicting whether there will be a forest fire or not. library(ellipse) # create dataframe with only weather variables - turn Class variable into a factor then turn it into numeric so we can run prcomp cutdata = data.frame(Temperature = ff_data$Temperature, Rain = ff_data$Rain, RH = ff_data$RH, Ws = ff_data$Ws, Class = as.numeric(as.factor(ff_data$Class))) head(cutdata) ## Temperature Rain RH Ws Class ## 1 29 0.0 57 18 2 ## 2 29 1.3 61 13 2 ## 3 26 13.1 82 22 2 ## 4 25 2.5 89 13 2 ## 5 27 0.0 77 16 2 ## 6 31 0.0 67 14 1 # perform pca using prcomp and extract scores pcaOutput &lt;- prcomp(as.matrix(cutdata), scale = TRUE, center = TRUE) pcaOutput ## Standard deviations (1, .., p=5): ## [1] 1.5839583 0.9756049 0.8161873 0.7594599 0.5443622 ## ## Rotation (n x k) = (5 x 5): ## PC1 PC2 PC3 PC4 PC5 ## Temperature 0.5424264 -0.2172267 -0.1829743 0.18263364 -0.76925388 ## Rain -0.4447857 -0.3014978 -0.5223649 -0.61317843 -0.24982364 ## RH -0.4889724 0.1941107 0.6351261 -0.05239005 -0.56311381 ## Ws -0.2986886 -0.8190447 0.1429360 0.45867885 0.09557101 ## Class -0.4238282 0.3916610 -0.5194607 0.61443158 -0.14002013 # create separate dataframe pcaOutput2 &lt;- as.data.frame(pcaOutput$x) # add Class as variable for the groups pcaOutput2$groups &lt;- ff_data$Class # calculate centroids of PC1 and PC2 centroids &lt;- aggregate(cbind(PC1, PC2) ~ groups, pcaOutput2, mean) # calculate 95% confidence ellipsoids so we can plot these polygons over ggplot conf.rgn &lt;- do.call(rbind, lapply(unique(pcaOutput2$groups), function(t) data.frame(groups = as.character(t), ellipse(cov(pcaOutput2[pcaOutput2$groups == t, 1:2]), centre = as.matrix(centroids[centroids$groups == t, 2:3]), level = 0.95), stringsAsFactors = FALSE))) # Plot PC1 and PC2 ggplot(data = pcaOutput2, aes(x = PC1, y = PC2, group = as.factor(groups), color = as.factor(groups))) + geom_polygon(data = conf.rgn, aes(fill = groups), alpha = 0.2) + geom_point(size = 2, alpha = 0.6) + labs(title = &quot;Principal Component Analysis of Bejaia Forest Fires: PC1 and PC2&quot;) 19.2.2 t-SNE Dimensionality Reduction TSNE Dimensionality Reduction removes redundant information and preserves high impact information within a dataset. TSNE is similar to PCA with the main difference being that TSNE retains nonlinear variance. First, we’ll calculate a proximity matrix of our ff_data dataframe using the dist function. This function will calculate the relative distances (euclidean) between the rows of the dataset. Then, we’ll use the tsne function from the tsne package. library(tsne) sne_data = dist(ff_data) sne_data = tsne(sne_data) sne_data = as.data.frame(sne_data) sne_data$Class = ff_data$Class ggplot(data = sne_data, aes(x = V1, y = V2, color = Class)) + geom_point() 19.2.3 Multidimensional Scaling Multidimensional scaling (MDS) is a technique that creates a map displaying the relative positions of a number of objects, given only a table of the distances between them. It flows similarly to TSNE in R but instead of using tsne we’ll be using cmdscale to perform classical multidimensional scaling based on the proximity matrix. # create proximity matrix prox = dist(ff_data) # perform mds m_scale = cmdscale(prox) m_scale = as.data.frame(m_scale) # add the class variable back in m_scale$Class = ff_data$Class # plot ggplot(data = m_scale,aes(x = V1, y = V2, color = Class)) + geom_point() + labs(title = &#39;Multidimensional Scaling of Bejaia Forest Fire Data&#39;) 19.3 Random Forests Random Forest is a machine learning method for classification or regression predictions. These predictions are based on the generation of multiple decision trees trees. Decision trees serve as the building block for random forests. The caret package is a popular machine learning package in R that can be used to create random forest models. Our goal is to create a random forest model that predicts the presence/absence of a forest fire given environmental conditions. 19.3.1 Decision Trees Classification in R can be done using the rpart and rpart.plot packages. These can be used to calculate and show decision trees (aka classification trees) for a given scenario. In our case, we’ll calculate and plot the decision tree for whether or not there is a forest fire. Before we proceed, when we calculate random values going into a model in R, it’s good practice to use the set.seed() function. The set.seed() function sets the starting number used to generate a sequence of random numbers – it ensures that you get the same result if you start with that same seed each time you run the same process. library(rpart) library(rpart.plot) library(caret) # cut the data but htis time preserve &#39;Fire&#39; and &#39;No Fire&#39; characters of the ff_data$Class cutdata = data.frame(Temperature = ff_data$Temperature, Rain = ff_data$Rain, RH = ff_data$RH, Ws = ff_data$Ws, Class = ff_data$Class) cutdata$Class = as.factor(cutdata$Class) # now that it&#39;s a factor, we need to redefine the levels again levels(cutdata$Class) = c(&quot;Fire&quot;, &quot;No Fire&quot;) # set the seed set.seed(42) # calculate the rpart model fit &lt;- rpart(Class ~ ., data = cutdata, method = &quot;class&quot;, control = rpart.control(xval = 10, minbucket = 2, cp = 0), parms = list(split = &quot;information&quot;)) # plot the fit! rpart.plot(fit, extra = 100) 19.3.2 Data Partition The goal of this exercise is to predict the presence/absence of a forest fire given environmental conditions. We’ll need to provide the model with a training dataset to learn from and a test dataset to test itself against. This can be done using createDataPartition from the caret package. Then, let’s calculate a correlation matrix using corrplot. # set the seed set.seed(42) # create test and train datasets index &lt;- createDataPartition(as.factor(cutdata$Class), p = 0.7, list = FALSE) train_data &lt;- cutdata[index, ] test_data &lt;- cutdata[-index, ] library(corrplot) # calculate correlation matrix num_train_data = train_data num_train_data$Class = as.numeric(num_train_data$Class) corMatMy &lt;- cor(num_train_data) corrplot(corMatMy, order = &quot;hclust&quot;) highlyCor &lt;- colnames(train_data)[findCorrelation(corMatMy, cutoff = 0.7, verbose = TRUE)] ## All correlations &lt;= 0.7 19.3.3 Training the Model Now, let’s use our training dataset to train a model using random forest methods. Remember, we’re training our model to predict the Class variable (i.e. Fire vs No Fire) based on the other variables (temperature, relative humidity, etc.) # set the seed set.seed(42) # create the model model_rf &lt;- caret::train(Class ~ ., data = train_data, method = &quot;rf&quot;, preProcess = c(&quot;scale&quot;, &quot;center&quot;), trControl = trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 3, savePredictions = TRUE, verboseIter = FALSE)) # print the random forest model output model_rf ## Random Forest ## ## 87 samples ## 4 predictor ## 2 classes: &#39;Fire&#39;, &#39;No Fire&#39; ## ## Pre-processing: scaled (4), centered (4) ## Resampling: Cross-Validated (5 fold, repeated 3 times) ## Summary of sample sizes: 70, 70, 69, 70, 69, 69, ... ## Resampling results across tuning parameters: ## ## mtry Accuracy Kappa ## 2 0.8082789 0.6159185 ## 3 0.8198257 0.6380856 ## 4 0.8274510 0.6524226 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was mtry = 4. # print out the confusion matrix model_rf$finalModel$confusion ## Fire No Fire class.error ## Fire 34 8 0.1904762 ## No Fire 8 37 0.1777778 # plot variable importance of the model importance &lt;- varImp(model_rf, scale = TRUE) plot(importance) Rain is the most important variable for the presence/absence of forest fires. Relative Humidity and Temperature carry importance but not as much as Rain. Wind Speed doesn’t seem to be all that important. 19.3.4 Testing the Model In order to test our Random Forest model, model_rf, we’ll need to use a confusion matrix. A Confusion matrix is a matrix used for evaluating the performance of a classification model. The matrix compares the actual target values (the testing data class) with those predicted by model_rf. This gives us a holistic view of how well our random forest model is performing and shows what kinds of errors it is making. After we get some statistics from the confusino matrix, well need to run the model using the predict function on the test_data. # Calculate the confusion matrix confusionMatrix(predict(model_rf, test_data), as.factor(test_data$Class)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction Fire No Fire ## Fire 16 3 ## No Fire 1 15 ## ## Accuracy : 0.8857 ## 95% CI : (0.7326, 0.968) ## No Information Rate : 0.5143 ## P-Value [Acc &gt; NIR] : 3.724e-06 ## ## Kappa : 0.772 ## ## Mcnemar&#39;s Test P-Value : 0.6171 ## ## Sensitivity : 0.9412 ## Specificity : 0.8333 ## Pos Pred Value : 0.8421 ## Neg Pred Value : 0.9375 ## Prevalence : 0.4857 ## Detection Rate : 0.4571 ## Detection Prevalence : 0.5429 ## Balanced Accuracy : 0.8873 ## ## &#39;Positive&#39; Class : Fire ## # Predict the model results &lt;- data.frame(actual = test_data$Class, predict(model_rf, test_data, type = &quot;prob&quot;)) # rename the variables/columns names(results) = c(&#39;actual&#39;, &#39;Fire&#39;, &#39;No.Fire&#39;) # declare fire vs no fire based on probabilities results$prediction &lt;- ifelse(results$Fire &gt; 0.5, &quot;Fire&quot;, ifelse(results$No.Fire &gt; 0.5, &quot;No Fire&quot;, NA)) # declare whether the model was right or wrong (true or false) results$correct &lt;- ifelse(results$actual == results$prediction, TRUE, FALSE) # plot the results ggplot(results, aes(x = prediction, fill = correct)) + geom_bar(position = &quot;dodge&quot;) + labs(title = &quot;Random Forest Performance&quot;, xlab = &quot;Prediction&quot;) # plot thhe results using geom_jitter ggplot(results, aes(x = prediction, y = Fire, color = correct, shape = correct)) + geom_jitter(size = 3, alpha = 0.9) + labs(title = &quot;Random Forest Performance&quot;, xlab = &quot;Prediction&quot;) 19.3.5 Hyperparameter Tuning Models like the random forest model we constructed above are built based on the predictor variables. Model parameters are constructed based on these variables, although hyperparameters are not model parameters and they cannot be directly trained from the data. Model parameters are learned during training when we optimize a loss function. Sometimes, the construction of the architecture (i.e. format of a decision tree) may not be optimal. The act of finding the optimal model archicture is called hyperparameter tuning. We can tune our model by applying a tuneGrid argument with the caret::train() function. Our tuneGrid will be defined by mtry : The number of variables randomly sampled as candidates at each split. # plot the original model plot(model_rf) # create hyperparameter grid set.seed(42) grid &lt;- expand.grid(mtry = c(1:10)) model_rf_tune_man &lt;- caret::train(Class ~ ., data = train_data, method = &quot;rf&quot;, preProcess = c(&quot;scale&quot;, &quot;center&quot;), trControl = trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10, savePredictions = TRUE, verboseIter = FALSE), tuneGrid = grid) model_rf_tune_man ## Random Forest ## ## 87 samples ## 4 predictor ## 2 classes: &#39;Fire&#39;, &#39;No Fire&#39; ## ## Pre-processing: scaled (4), centered (4) ## Resampling: Cross-Validated (10 fold, repeated 10 times) ## Summary of sample sizes: 78, 78, 79, 79, 77, 79, ... ## Resampling results across tuning parameters: ## ## mtry Accuracy Kappa ## 1 0.8350278 0.6714680 ## 2 0.8176667 0.6355919 ## 3 0.8154722 0.6311586 ## 4 0.8114167 0.6214337 ## 5 0.8171389 0.6328296 ## 6 0.8147778 0.6282390 ## 7 0.8139167 0.6264313 ## 8 0.8135278 0.6255191 ## 9 0.8161389 0.6308268 ## 10 0.8158889 0.6305575 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was mtry = 1. plot(model_rf_tune_man) Notice hoe many more predictors we get. Our dataset is not the best example for this practice because our dataset only has a handful of predictors. However, The search of the tuneGrid function above is not random by default. Let’s perform a random search of the tuning parmaeters and provide a tuneLength - An integer denoting the amount of granularity in the tuning parameter grid. model_rf_tune_auto &lt;- caret::train(Class ~ ., data = train_data, method = &quot;rf&quot;, preProcess = c(&quot;scale&quot;, &quot;center&quot;), trControl = trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10, savePredictions = TRUE, verboseIter = FALSE, search = &quot;random&quot;), tuneGrid = grid, tuneLength = 15) model_rf_tune_auto ## Random Forest ## ## 87 samples ## 4 predictor ## 2 classes: &#39;Fire&#39;, &#39;No Fire&#39; ## ## Pre-processing: scaled (4), centered (4) ## Resampling: Cross-Validated (10 fold, repeated 10 times) ## Summary of sample sizes: 78, 79, 78, 77, 79, 79, ... ## Resampling results across tuning parameters: ## ## mtry Accuracy Kappa ## 1 0.8441111 0.6903933 ## 2 0.8262778 0.6533746 ## 3 0.8163333 0.6317828 ## 4 0.8173611 0.6326702 ## 5 0.8152500 0.6288156 ## 6 0.8196111 0.6372856 ## 7 0.8127778 0.6232523 ## 8 0.8197222 0.6372499 ## 9 0.8161389 0.6300520 ## 10 0.8137500 0.6254779 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was mtry = 1. plot(model_rf_tune_auto) As we see above, random forest models can quickly become complex due to the fact that there’s uncertainty in any classification prediction. A particular model can require tuning as seen above. Once again, this practice is much more useful for a dataset with more predictors but it’s important to show how it can be done here. 19.4 Assignment Perform the exact same analysis for the SidiBel_ForestFires.csv dataset (this has the same variables as Bejaia_ForestFires.csv). Proceed with the same workflow and create the same plots. Add plots to a document (Word, Google Docs, or RMarkdown file, your choice). Upload file (as word doc, PDF, whatever works) to UD Canvas. "]]
