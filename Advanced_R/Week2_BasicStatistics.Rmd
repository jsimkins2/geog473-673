# Basic Statistics
This week we'll be diving into some basic statistical procedures. R started out as THE statistical programming language. It spread like wildfire for it's performance and efficiency at crunching numbers. It's early success as a statistical programming langugae attracted the early developers who really made R into the do-it-all language we're using today. First, I'll throw in the useful definitions list at the top of this tutorial just for ease of access. Second, we'll dive into a recap of for loops and how they're structured. Then we'll show off some of R's great statistics functions and how to add them to a plot. Finally, we'll have an assignment where we cover some of 

## Statistical Function Dictionary

| Function                 |             Description        |
| -----------| -------------------------------------------- |
| length| returns length of a vector | 
| sum | returns the sum |
| mean | returns the mean | 
| median | returns the median | 
| sd | returns the standard deviation (n − 1 in denominator) | 
| min | returns minimum | 
| max | returns maximum |
| sort | sort a vector (rearranges the vector in order) |
| order | returns indices of vectors that will order them |
| rank | returns rank of each element in vector | 
|lm |multiple linear regression|
|glm |generalized linear regression|
|anova |analysis of variance|
|chisq.test |Pearson’s Chi-squared test for count data|
|summary| shows results of various model fitting functions|
|predict |predicted results from model|
|passing |an lm object will result in adding the predicted line to the plot|
|sample |produces a random sample of the specified values|
|set.seed |sets seed for next random sample (repeat random sample)|
|rnorm |produces a random sample from a normal distribution|
|qnorm |quantiles (percentiles) of normal distribution|
|pnorm |CDF of normal distribution|
|dnorm |PDF of normal distribution|
|rbinom |produces a random sample from a binomial distribution|

## Basic Statistics in R

R is famous for it's easy-to-use statistics features and once you get the hang of it you'll never want to touch Microsoft Excel again. Despite the fact that these stats functions seem pretty fancy to me, I call them basic functions because R has so much to offer to statisticians. We'll be using multiple meteorological / climate model datasets for this tutorial. Let's get started with some of these functions...

### Load the data
```{r}
library(ncdf4)
# open the netcdf file of Willow Creek, Wisconsin meteorology data
nc_file = nc_open("/Users/james/Documents/Github/geog473-673/datasets/WCr_1hr.2010.nc")
# what does the nc file look like 
nc_file
# ok, still a lot of info...let's list the names of the variables
names(nc_file$var)
# alright, now we have some names, so let's put the variables into a new dataframe separate from the nc_file
var_names = names(nc_file$var)
willow_creek_2010 = list()
dim <- nc_file$dim
for (v in seq_along(var_names)){
  willow_creek_2010[[v]] = ncvar_get(nc_file, varid = var_names[v])
}
# convert the list into a dataframe
wcreek_df = data.frame(willow_creek_2010)
# tell the dataframe what the column names are
colnames(wcreek_df) = var_names
# print the column names
names(wcreek_df)

# let's rename the variables to make them shorter - note that these short names MUST be in the same order as the longer names
short_names = c("tair", "tmax", "tmin", "swave", "pres", "lwave", "ewind", "nwind", "shum", "prec")
# rename the column names to our new short name vector
colnames(wcreek_df) = short_names
# print the summary statistics
summary(wcreek_df)
```

Our data has been parsed, except for the time variable. This is a time series dataset of weather variables so we'll need a time variable to keep us organized. Here's how we can create one...

```{r}
### TIME ###
# what are the units of time
dim$time$units
# how are the time values spaced
dim$time$vals[1:10]
# we can back out that this data is hourly data just from knowing the units are seconds and time between
# each recorded value is 3600 (3600 seconds == 1 hour)
#### Add a datetime column ####
date.seq = seq(as.POSIXct("2010-01-01 00:00:00"), as.POSIXct("2010-12-31 23:00:00"), by="hour")
wcreek_df['datetime'] = date.seq
wcreek_df$datetime[1:10]
```

The data is loaded, organized, and ready for statistical analysis. 

### Subsetting Data

In certain cases, our dataset may be too large. For example, our dataset contains 1 year of data. How can we subset our dataset from January 1 to June 30?

#### Manual Subsetting

We can use the `which` function to find out which row a particular datetime is located. In other words, the `which` function returns the dates index position. In order for this to work, **the datetime format we use must match the datetime format of the dataset**. In our case, the datetime is YYYY-mm-dd, so we need to search with that format. After we have our indices, we can subset the `wcreek_df` dataset using `wcreek_df[rows,columns]` subsetting rules. 

```{r}
# use YYYY-mm-dd to return which datettime index has a value of 2010-01-01
start_ind = which(wcreek_df$datetime == "2010-01-01")
# use YYYY-mm-dd to return which datettime index has a value of 2010-06-30
end_ind = which(wcreek_df$datetime == "2010-06-30")
# Index our wcree_df dataframe using the indices gathered above. We want all columns so column section stays blank
jan2jun = wcreek_df[start_ind:end_ind,]
```

#### The subset() Function

There is also a handy `subset()` function that can save us a few lines of code. Let's now use the `subset()` function to subset the data from Jan 1 to Jun 30. 

```{r}
# grab all values of the wcreek df when the datetime is less than 2010-07-01
jan2jun = subset(wcreek_df, datetime < "2010-07-01")
```


Let's take a look at air temperature in relation to season. We're going to look at temperatures during the first half of the year by indexing air temperature and date from the 1st index to half the length of the dataset `[1:length(wcreek_df$datetime) / 2]` . So instead of having 8760 temperature values (there's 8760 hours in a non leap-year), we only have 4380. 

```{r}
plot(wcreek_df$tair[1:length(wcreek_df$tair) / 2] ~ wcreek_df$datetime[1:length(wcreek_df$datetime) / 2], pch = 20, col="blue")

# alright, so yeah we know in the northern hemisphere that temperature increases between winter and summer months...but what is the trend like?
fit <- lm(tair[1:length(wcreek_df$tair) / 2] ~ wcreek_df$datetime[1:length(wcreek_df$datetime) / 2], data = wcreek_df)
summary(fit)
coef(fit)

# set our indexes
start_ind = 1
end_ind = length(wcreek_df$tair) / 2 # calculate the length of the data and divide it by 2 to get the first half of year

plot(tair[start_ind:end_ind] ~ wcreek_df$datetime[start_ind:end_ind], data = wcreek_df,
  main= "tair vs. datetime",
  ylab= "Air Temperature ( Kelvin )",
  xlab= "Datetime",
  pch= 19, col= 'blue')
grid(NA,NULL, lty= 4) # NA first for no y axis grid lines, null second to ignore the default x axis linetype
abline(fit, col= 'black', lty= 4, lwd= 2)
legend("topleft", legend=c("lm(fit)"), col= 2, lty= 4, bg= "gray85", box.lty=0)
```

So that's how you would go about investigating trendlines. Histograms are a good way to check out the distribution of data. Let's do that and add in a basic trendline for the probability distribution.

Let's add this to a histogram of the data
```{r}
hist(wcreek_df$air_temperature[1:length(wcreek_df$air_temperature) / 2], freq = FALSE)
x <- seq(250, 310, length.out=100)
y <- with(wcreek_df, dnorm(x, mean(air_temperature[start_ind:end_ind]), 
          sd(air_temperature[start_ind:end_ind])))
lines(x, y, col = "red")
```

Density plots can be thought of as plots of smoothed histograms. The smoothness is controlled by a bandwidth parameter that is analogous to the histogram binwidth.  Let's plot up the density of all the temperature values within this half of the data.

```{r}
d = density(wcreek_df$air_temperature[start_ind:end_ind], bw = 0.5)
plot(d, xlab = "Air Temperature (Kelvin)", ylab = "Density", main="Air Temperature Density", col="black")
```

For a moderate number of observations a useful addition is a jittered rug plot:

```{r}
plot(density(wcreek_df$air_temperature[start_ind:end_ind], bw = 0.5), main = "jittered rug plot")
rug(jitter(wcreek_df$air_temperature[start_ind:end_ind]))
```

### Correlation Plots

Correlation plots show correlation coefficients across variables. For example, we expect shortwave radiation and temperature to have high a high correlation coefficient because generally speaking when the a lot of sunlight is received at the surface, temperature increases. There's a handy package called `corrplot` that caluclates correlation coefficients quickly and intuitively. We'll need to install this package and run our dataframe through it. One last thing...we'll need to calculate the residual values - that is the distance between actual data and the trendline. It's another way to express error. Let's reload a Willow Creek dataset.

```{r, fig.width=10, fig.height=10}
library(corrplot)
library(lubridate) # might need to install lubridate
library(ncdf4)
y = 2010
nc_file = nc_open(paste0("/Users/james/Documents/Github/geog473-673/datasets/WCr_1hr.", y,".nc"))
# what does the nc file look like 
var_names = names(nc_file$var)
wcr_data = list()
dim <- nc_file$dim
for (v in seq_along(var_names)){
  wcr_data[[v]] = ncvar_get(nc_file, varid = var_names[v])
}
wcreek_df = data.frame(wcr_data)
colnames(wcreek_df) = var_names

date.seq = seq(as.POSIXct(paste0(y,"-01-01 00:00:00")), as.POSIXct(paste0(y,"-12-31 23:00:00")), by="hour")
# everything is the same as before, but HERE is where things change a little...
# instead of plugging in the datetimes into the data frame, we need to plug in a numeric. Since a datetime
# is a "POSIXct" class, the correlation matrix function won't know how to handle it. We need a NUMERIC value for this.
# for now, let's just choose our "date" label to be in the form of day-of-year
days = yday(date.seq) # use lubridates yday function, returns a NUMERIC value
wcreek_df['day_of_year'] = days
summary(wcreek_df)

# awesome, now let's calculate the correlation coefficients
cor_wcreek = cor(wcreek_df)
head(cor_wcreek)
# now let's calculate the residuals of the correlations with a 95% confidence interval
residuals_1 <- cor.mtest(wcreek_df, conf.level = .95)
# now let's plot this up. 
corrplot(cor_wcreek, p.mat = residuals_1$p, method = 'color', number.cex = .7, type = 'lower',
         addCoef.col = "black", # Add coefficient of correlation
         tl.col = "black", tl.srt = 90, # Text label color and rotation
         # Combine with significance
         sig.level = 0.05, insig = "blank")
```



# Week 2 Assignment:


Using the WCr_1hr.2010.nc, WCr_1hr.2011.nc and WCr_1hr.2012.nc found in the datasets folder, complete the following:

1. Make a **3 column plot** (each year gets 1 plot) of `surface_downwelling_shortwave_flux_in_air` (radiation from the sun) showing the data and trendlines for each year for the **second half** of the year (~ July 1 to Dec 31)
- Note that it's alright if the abline goes outside of the plot, I'll show you how to fix this later
2. Using the data and same time range above, make a **3 row plot** showing the jittered rug plot with 0.75 bandwidth for each year of `surface_downwelling_shortwave_flux_in_air`
3. Submit plots to assignment 2 on canvas


Your final plots should look something like this **but with different colors**

```{r, echo=FALSE,fig.width=8, fig.height=6}
library(plotrix)
par(mfrow=c(1,3)) ## create plot array of 1 row x 3columns
year.seq = seq(2010,2012,1)
for (y in year.seq){
  nc_file = nc_open(paste0("/Users/james/Documents/Github/geog473-673/datasets/WCr_1hr.", y,".nc"))
  # what does the nc file look like 
  var_names = names(nc_file$var)
  wcr_data = list()
  dim <- nc_file$dim
  for (v in seq_along(var_names)){
    wcr_data[[v]] = ncvar_get(nc_file, varid = var_names[v])
  }
  wcreek_df = data.frame(wcr_data)
  colnames(wcreek_df) = var_names

  date.seq = seq(as.POSIXct(paste0(y,"-01-01 00:00:00")), as.POSIXct(paste0(y,"-12-31 23:00:00")), by="hour")
  wcreek_df['datetime'] = date.seq
  
  fit <- lm(wcreek_df$surface_downwelling_shortwave_flux_in_air[4380:length(wcreek_df$surface_downwelling_shortwave_flux_in_air)] ~ wcreek_df$datetime[4380:length(wcreek_df$datetime)], data = wcreek_df)
  summary(fit)
  coef(fit)
  
  plot(surface_downwelling_shortwave_flux_in_air[4380:length(wcreek_df$surface_downwelling_shortwave_flux_in_air)] ~ datetime[4380:length(wcreek_df$datetime)], data = wcreek_df,
    main= paste0("shortwave radiation vs. datetime ", y),
    ylab= "Radiation ( W m-2 )",
    xlab= "Datetime",
    pch= 19, col= 'blue')
  grid(NA,NULL, lty= 4) # NA first for no y axis grid lines, null second to ignore the default x axis linetype
  ablineclip(fit,y1=0, col= 'coral', lty= 4, lwd= 6)
  legend("topleft", legend=c("lm(fit)"), col= "dodgerblue", lty= 4, bg= "gray85", box.lty=0)
}
```
```{r, echo=FALSE,fig.width=6, fig.height=10}
par(mfrow=c(3,1)) ## create plot array of 1 row x 3columns
year.seq = seq(2010,2012,1)
for (y in year.seq){
  nc_file = nc_open(paste0("/Users/james/Documents/Github/geog473-673/datasets/WCr_1hr.", y,".nc"))
  # what does the nc file look like 
  var_names = names(nc_file$var)
  wcr_data = list()
  dim <- nc_file$dim
  for (v in seq_along(var_names)){
    wcr_data[[v]] = ncvar_get(nc_file, varid = var_names[v])
  }
  wcreek_df = data.frame(wcr_data)
  colnames(wcreek_df) = var_names

  date.seq = seq(as.POSIXct(paste0(y,"-01-01 00:00:00")), as.POSIXct(paste0(y,"-12-31 23:00:00")), by="hour")
  wcreek_df['datetime'] = date.seq
  
  fit <- lm(wcreek_df$air_pressure[4380:length(wcreek_df$air_pressure)] ~ wcreek_df$datetime[4380:length(wcreek_df$datetime)], data = wcreek_df)
  summary(fit)
  coef(fit)
  
  plot(density(wcreek_df$air_pressure[4380:length(wcreek_df$air_pressure)], bw = 0.25),
       main = paste0("shortwave radiation density jittered rugplot ", y), lwd=2, col="red")
  rug(jitter(wcreek_df$air_pressure[4380:length(wcreek_df$air_pressure)]))
}
```


## Extra Credit - 2 points

Using the WCr_1hr.2010.nc, WCr_1hr.2011.nc and WCr_1hr.2012.nc found in the datasets folder, complete the following

1) Fuse together the 3 datasets into one continuous dataframe.
2) Resample the data from an hourly to a daily resolution
3) Plot Air temperature for your new combined data frame and add a trendline to it.
4) Submit to assignment above labeled 'extra_credit.png'

